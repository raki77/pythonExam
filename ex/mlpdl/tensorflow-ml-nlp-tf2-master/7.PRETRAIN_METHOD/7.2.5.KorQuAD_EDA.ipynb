{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "from random import sample, seed\n",
    "from transformers import BertTokenizer\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "seed(1234)\n",
    "phoneme_tokenizer = Okt()\n",
    "\n",
    "DATA_IN_PATH = './data_in/KOR'\n",
    "save_path = \"bert_ckpt/\"\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", lowercase=False).save_pretrained(save_path)\n",
    "\n",
    "bert_tokenizer = BertWordPieceTokenizer(\"bert_ckpt/vocab.txt\", lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\"\n",
    "train_path = keras.utils.get_file(\"train.json\", train_data_url)\n",
    "eval_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\"\n",
    "eval_path = keras.utils.get_file(\"eval.json\", eval_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open(train_path))\n",
    "dev_data = json.load(open(eval_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(train_data['data'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지문 텍스트 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for d in train_data['data']:\n",
    "    documents += [p['context'] for p in d['paragraphs']]\n",
    "print('전체 텍스트 수: {}'.format(len(documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지문 텍스트 어절 단위 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_docs = []\n",
    "for d in documents:\n",
    "    len_docs.append(len(d.split()))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_docs)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_docs)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_docs)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_docs)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_docs)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_docs, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_docs, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot([len_docs],\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_len_docs = [l for l in len_docs if l < 2000]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "# 박스플롯 생성\n",
    "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
    "# labels: 입력한 데이터에 대한 라벨\n",
    "# showmeans: 평균값을 마크함\n",
    "\n",
    "plt.boxplot(filtered_len_docs,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(filtered_len_docs, bins=150, range=[0,600], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of word count in paragraph\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지문 텍스트 버트 토크나이저 토큰 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_bert_tokenized_docs = []\n",
    "for d in documents:\n",
    "    len_bert_tokenized_docs.append(len(bert_tokenizer.encode(d, add_special_tokens=False).tokens))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_bert_tokenized_docs)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_bert_tokenized_docs)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_bert_tokenized_docs)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_bert_tokenized_docs)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_bert_tokenized_docs)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_docs, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_docs, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_len_bert_tokenized_docs = [l for l in len_bert_tokenized_docs if l < 3000]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot(filtered_len_bert_tokenized_docs,\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(filtered_len_bert_tokenized_docs, bins=150, range=[0,1000], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of Bert Tokenizer token count in paragraph\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지문 텍스트 어휘 빈도 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어절 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for d in documents:\n",
    "    sentences += sent_tokenize(d)\n",
    "\n",
    "print('전체 문장 수: {}'.format(len(sentences)))\n",
    "sampled_docs = sample(sentences, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = os.path.join(DATA_IN_PATH, 'NanumGothic.ttf')\n",
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(sampled_docs))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_extracted_docs = []\n",
    "for d in sampled_docs:\n",
    "    noun_extracted_docs += phoneme_tokenizer.nouns(d)\n",
    "\n",
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(noun_extracted_docs))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질문 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for d in train_data['data']:\n",
    "    qas = [p['qas'] for p in d['paragraphs']]\n",
    "    for c in qas:\n",
    "        questions += [q['question'] for q in c]\n",
    "    \n",
    "print('전체 질문 수: {}'.format(len(questions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 텍스트 어절 토큰 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_qs = []\n",
    "for q in questions:\n",
    "    len_qs.append(len(q.split()))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_qs)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_qs)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_qs)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_qs)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_qs)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_qs, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_qs, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot([len_qs],\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(len_qs, bins=50, range=[0,50], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of word count in sentence\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 텍스트 버트 토크나이저 토큰 길이 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_bert_tokenized_q = []\n",
    "for q in questions:\n",
    "    len_bert_tokenized_q.append(len(bert_tokenizer.encode(q, add_special_tokens=False).tokens))\n",
    "    \n",
    "print('텍스트 최대 길이: {}'.format(np.max(len_bert_tokenized_q)))\n",
    "print('텍스트 최소 길이: {}'.format(np.min(len_bert_tokenized_q)))\n",
    "print('텍스트 평균 길이: {:.2f}'.format(np.mean(len_bert_tokenized_q)))\n",
    "print('텍스트 길이 표준편차: {:.2f}'.format(np.std(len_bert_tokenized_q)))\n",
    "print('텍스트 중간 길이: {}'.format(np.median(len_bert_tokenized_q)))\n",
    "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
    "print('제 1 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_q, 25)))\n",
    "print('제 3 사분위 텍스트 길이: {}'.format(np.percentile(len_bert_tokenized_q, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.boxplot([len_bert_tokenized_q],\n",
    "             labels=['counts'],\n",
    "             showmeans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.hist(len_bert_tokenized_q, bins=50, range=[0,50], facecolor='r', density=True, label='train')\n",
    "plt.title(\"Distribution of Bert Tokenizer token count in sentence\", fontsize=15)\n",
    "plt.legend()\n",
    "plt.xlabel('Number of words', fontsize=15)\n",
    "plt.ylabel('Probability', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 질문 텍스트 어휘 빈도 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어절 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_questions = sample(questions, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(sampled_questions))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 명사 토큰 기준"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_extracted_qs = []\n",
    "for q in sampled_questions:\n",
    "    noun_extracted_qs += phoneme_tokenizer.nouns(q)\n",
    "\n",
    "cloud = WordCloud(font_path = font_path, width=800, height=600).generate(' '.join(noun_extracted_qs))\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
