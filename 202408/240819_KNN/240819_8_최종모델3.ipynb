{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6496</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6497 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3      4     5      6        7     8     9     10  \\\n",
       "0      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "1      7.8  0.88  0.00  2.6  0.098  25.0   67.0  0.99680  3.20  0.68   9.8   \n",
       "2      7.8  0.76  0.04  2.3  0.092  15.0   54.0  0.99700  3.26  0.65   9.8   \n",
       "3     11.2  0.28  0.56  1.9  0.075  17.0   60.0  0.99800  3.16  0.58   9.8   \n",
       "4      7.4  0.70  0.00  1.9  0.076  11.0   34.0  0.99780  3.51  0.56   9.4   \n",
       "...    ...   ...   ...  ...    ...   ...    ...      ...   ...   ...   ...   \n",
       "6492   6.2  0.21  0.29  1.6  0.039  24.0   92.0  0.99114  3.27  0.50  11.2   \n",
       "6493   6.6  0.32  0.36  8.0  0.047  57.0  168.0  0.99490  3.15  0.46   9.6   \n",
       "6494   6.5  0.24  0.19  1.2  0.041  30.0  111.0  0.99254  2.99  0.46   9.4   \n",
       "6495   5.5  0.29  0.30  1.1  0.022  20.0  110.0  0.98869  3.34  0.38  12.8   \n",
       "6496   6.0  0.21  0.38  0.8  0.020  22.0   98.0  0.98941  3.26  0.32  11.8   \n",
       "\n",
       "      11  12  \n",
       "0      5   1  \n",
       "1      5   1  \n",
       "2      5   1  \n",
       "3      6   1  \n",
       "4      5   1  \n",
       "...   ..  ..  \n",
       "6492   6   0  \n",
       "6493   5   0  \n",
       "6494   6   0  \n",
       "6495   7   0  \n",
       "6496   6   0  \n",
       "\n",
       "[6497 rows x 13 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ori = pd.read_csv('./data/wine.csv' , header= None)\n",
    "df = df_ori  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6497, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, :12]\n",
    "y = df.iloc[:, 12]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5197.6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6497*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1299.4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6497*0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :12]\n",
    "y = df.iloc[:, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5197, 12)\n",
      "(1300, 12)\n",
      "(5197,)\n",
      "(1300,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 과적합 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = \"./model/all3/{epoch:02d}-{val_accuracy:.4f}.keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/8 [==>...........................] - ETA: 3s - loss: 6.2960 - accuracy: 0.2300\n",
      "Epoch 1: saving model to ./model/all3\\01-0.5731.keras\n",
      "8/8 [==============================] - 1s 23ms/step - loss: 3.4046 - accuracy: 0.2733 - val_loss: 0.7902 - val_accuracy: 0.5731\n",
      "Epoch 2/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7936 - accuracy: 0.5500\n",
      "Epoch 2: saving model to ./model/all3\\02-0.8700.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.4145 - accuracy: 0.8060 - val_loss: 0.3066 - val_accuracy: 0.8700\n",
      "Epoch 3/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2585 - accuracy: 0.8860\n",
      "Epoch 3: saving model to ./model/all3\\03-0.8592.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3141 - accuracy: 0.8789 - val_loss: 0.3752 - val_accuracy: 0.8592\n",
      "Epoch 4/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2569 - accuracy: 0.8920\n",
      "Epoch 4: saving model to ./model/all3\\04-0.8554.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3352 - accuracy: 0.8722 - val_loss: 0.3732 - val_accuracy: 0.8554\n",
      "Epoch 5/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3236 - accuracy: 0.8720\n",
      "Epoch 5: saving model to ./model/all3\\05-0.8692.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3219 - accuracy: 0.8776 - val_loss: 0.3404 - val_accuracy: 0.8692\n",
      "Epoch 6/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2552 - accuracy: 0.9040\n",
      "Epoch 6: saving model to ./model/all3\\06-0.8792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2866 - accuracy: 0.8891 - val_loss: 0.2928 - val_accuracy: 0.8792\n",
      "Epoch 7/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2689 - accuracy: 0.9020\n",
      "Epoch 7: saving model to ./model/all3\\07-0.8946.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2511 - accuracy: 0.9015 - val_loss: 0.2566 - val_accuracy: 0.8946\n",
      "Epoch 8/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2713 - accuracy: 0.8860\n",
      "Epoch 8: saving model to ./model/all3\\08-0.9015.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2352 - accuracy: 0.9071 - val_loss: 0.2439 - val_accuracy: 0.9015\n",
      "Epoch 9/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2148 - accuracy: 0.9220\n",
      "Epoch 9: saving model to ./model/all3\\09-0.9092.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2311 - accuracy: 0.9166 - val_loss: 0.2381 - val_accuracy: 0.9092\n",
      "Epoch 10/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2383 - accuracy: 0.9200\n",
      "Epoch 10: saving model to ./model/all3\\10-0.9131.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2266 - accuracy: 0.9192 - val_loss: 0.2330 - val_accuracy: 0.9131\n",
      "Epoch 11/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2305 - accuracy: 0.9160\n",
      "Epoch 11: saving model to ./model/all3\\11-0.9138.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2230 - accuracy: 0.9197 - val_loss: 0.2302 - val_accuracy: 0.9138\n",
      "Epoch 12/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2026 - accuracy: 0.9300\n",
      "Epoch 12: saving model to ./model/all3\\12-0.9154.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2206 - accuracy: 0.9202 - val_loss: 0.2260 - val_accuracy: 0.9154\n",
      "Epoch 13/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2519 - accuracy: 0.9120\n",
      "Epoch 13: saving model to ./model/all3\\13-0.9177.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2171 - accuracy: 0.9225 - val_loss: 0.2213 - val_accuracy: 0.9177\n",
      "Epoch 14/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2264 - accuracy: 0.9120\n",
      "Epoch 14: saving model to ./model/all3\\14-0.9185.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2141 - accuracy: 0.9240 - val_loss: 0.2182 - val_accuracy: 0.9185\n",
      "Epoch 15/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1652 - accuracy: 0.9460\n",
      "Epoch 15: saving model to ./model/all3\\15-0.9200.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2121 - accuracy: 0.9246 - val_loss: 0.2156 - val_accuracy: 0.9200\n",
      "Epoch 16/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1897 - accuracy: 0.9260\n",
      "Epoch 16: saving model to ./model/all3\\16-0.9231.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2104 - accuracy: 0.9261 - val_loss: 0.2126 - val_accuracy: 0.9231\n",
      "Epoch 17/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2016 - accuracy: 0.9300\n",
      "Epoch 17: saving model to ./model/all3\\17-0.9223.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2088 - accuracy: 0.9264 - val_loss: 0.2114 - val_accuracy: 0.9223\n",
      "Epoch 18/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2241 - accuracy: 0.9240\n",
      "Epoch 18: saving model to ./model/all3\\18-0.9215.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2074 - accuracy: 0.9269 - val_loss: 0.2100 - val_accuracy: 0.9215\n",
      "Epoch 19/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2109 - accuracy: 0.9260\n",
      "Epoch 19: saving model to ./model/all3\\19-0.9215.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2063 - accuracy: 0.9264 - val_loss: 0.2086 - val_accuracy: 0.9215\n",
      "Epoch 20/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2008 - accuracy: 0.9240\n",
      "Epoch 20: saving model to ./model/all3\\20-0.9208.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2054 - accuracy: 0.9264 - val_loss: 0.2079 - val_accuracy: 0.9208\n",
      "Epoch 21/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2376 - accuracy: 0.9160\n",
      "Epoch 21: saving model to ./model/all3\\21-0.9215.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2043 - accuracy: 0.9261 - val_loss: 0.2072 - val_accuracy: 0.9215\n",
      "Epoch 22/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2381 - accuracy: 0.9080\n",
      "Epoch 22: saving model to ./model/all3\\22-0.9215.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2036 - accuracy: 0.9261 - val_loss: 0.2066 - val_accuracy: 0.9215\n",
      "Epoch 23/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1942 - accuracy: 0.9360\n",
      "Epoch 23: saving model to ./model/all3\\23-0.9262.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2025 - accuracy: 0.9269 - val_loss: 0.2050 - val_accuracy: 0.9262\n",
      "Epoch 24/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2019 - accuracy: 0.9300\n",
      "Epoch 24: saving model to ./model/all3\\24-0.9262.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.2019 - accuracy: 0.9271 - val_loss: 0.2045 - val_accuracy: 0.9262\n",
      "Epoch 25/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1996 - accuracy: 0.9240\n",
      "Epoch 25: saving model to ./model/all3\\25-0.9262.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.2011 - accuracy: 0.9281 - val_loss: 0.2035 - val_accuracy: 0.9262\n",
      "Epoch 26/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1939 - accuracy: 0.9360\n",
      "Epoch 26: saving model to ./model/all3\\26-0.9269.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2003 - accuracy: 0.9299 - val_loss: 0.2030 - val_accuracy: 0.9269\n",
      "Epoch 27/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1636 - accuracy: 0.9420\n",
      "Epoch 27: saving model to ./model/all3\\27-0.9277.keras\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1993 - accuracy: 0.9302 - val_loss: 0.2018 - val_accuracy: 0.9277\n",
      "Epoch 28/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2027 - accuracy: 0.9320\n",
      "Epoch 28: saving model to ./model/all3\\28-0.9285.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1985 - accuracy: 0.9317 - val_loss: 0.2007 - val_accuracy: 0.9285\n",
      "Epoch 29/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1633 - accuracy: 0.9420\n",
      "Epoch 29: saving model to ./model/all3\\29-0.9285.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1979 - accuracy: 0.9315 - val_loss: 0.2005 - val_accuracy: 0.9285\n",
      "Epoch 30/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1769 - accuracy: 0.9420\n",
      "Epoch 30: saving model to ./model/all3\\30-0.9300.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1966 - accuracy: 0.9323 - val_loss: 0.1983 - val_accuracy: 0.9300\n",
      "Epoch 31/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2056 - accuracy: 0.9240\n",
      "Epoch 31: saving model to ./model/all3\\31-0.9315.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1957 - accuracy: 0.9333 - val_loss: 0.1972 - val_accuracy: 0.9315\n",
      "Epoch 32/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2250 - accuracy: 0.9260\n",
      "Epoch 32: saving model to ./model/all3\\32-0.9300.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1942 - accuracy: 0.9328 - val_loss: 0.1974 - val_accuracy: 0.9300\n",
      "Epoch 33/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1770 - accuracy: 0.9360\n",
      "Epoch 33: saving model to ./model/all3\\33-0.9315.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1929 - accuracy: 0.9330 - val_loss: 0.1957 - val_accuracy: 0.9315\n",
      "Epoch 34/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2035 - accuracy: 0.9220\n",
      "Epoch 34: saving model to ./model/all3\\34-0.9323.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1919 - accuracy: 0.9333 - val_loss: 0.1928 - val_accuracy: 0.9323\n",
      "Epoch 35/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2182 - accuracy: 0.9280\n",
      "Epoch 35: saving model to ./model/all3\\35-0.9331.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1915 - accuracy: 0.9353 - val_loss: 0.1933 - val_accuracy: 0.9331\n",
      "Epoch 36/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2161 - accuracy: 0.9320\n",
      "Epoch 36: saving model to ./model/all3\\36-0.9338.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1900 - accuracy: 0.9353 - val_loss: 0.1935 - val_accuracy: 0.9338\n",
      "Epoch 37/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1724 - accuracy: 0.9360\n",
      "Epoch 37: saving model to ./model/all3\\37-0.9338.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1890 - accuracy: 0.9343 - val_loss: 0.1915 - val_accuracy: 0.9338\n",
      "Epoch 38/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1868 - accuracy: 0.9320\n",
      "Epoch 38: saving model to ./model/all3\\38-0.9338.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1877 - accuracy: 0.9361 - val_loss: 0.1899 - val_accuracy: 0.9338\n",
      "Epoch 39/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1905 - accuracy: 0.9380\n",
      "Epoch 39: saving model to ./model/all3\\39-0.9346.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1869 - accuracy: 0.9369 - val_loss: 0.1888 - val_accuracy: 0.9346\n",
      "Epoch 40/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1541 - accuracy: 0.9560\n",
      "Epoch 40: saving model to ./model/all3\\40-0.9362.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1857 - accuracy: 0.9369 - val_loss: 0.1881 - val_accuracy: 0.9362\n",
      "Epoch 41/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2380 - accuracy: 0.9240\n",
      "Epoch 41: saving model to ./model/all3\\41-0.9338.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1852 - accuracy: 0.9364 - val_loss: 0.1883 - val_accuracy: 0.9338\n",
      "Epoch 42/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1785 - accuracy: 0.9440\n",
      "Epoch 42: saving model to ./model/all3\\42-0.9362.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1839 - accuracy: 0.9371 - val_loss: 0.1848 - val_accuracy: 0.9362\n",
      "Epoch 43/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1842 - accuracy: 0.9340\n",
      "Epoch 43: saving model to ./model/all3\\43-0.9369.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1821 - accuracy: 0.9376 - val_loss: 0.1845 - val_accuracy: 0.9369\n",
      "Epoch 44/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1654 - accuracy: 0.9480\n",
      "Epoch 44: saving model to ./model/all3\\44-0.9392.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1809 - accuracy: 0.9382 - val_loss: 0.1831 - val_accuracy: 0.9392\n",
      "Epoch 45/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1685 - accuracy: 0.9420\n",
      "Epoch 45: saving model to ./model/all3\\45-0.9392.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1787 - accuracy: 0.9379 - val_loss: 0.1806 - val_accuracy: 0.9392\n",
      "Epoch 46/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1792 - accuracy: 0.9380\n",
      "Epoch 46: saving model to ./model/all3\\46-0.9408.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1762 - accuracy: 0.9397 - val_loss: 0.1776 - val_accuracy: 0.9408\n",
      "Epoch 47/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1304 - accuracy: 0.9600\n",
      "Epoch 47: saving model to ./model/all3\\47-0.9362.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1759 - accuracy: 0.9402 - val_loss: 0.1788 - val_accuracy: 0.9362\n",
      "Epoch 48/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1681 - accuracy: 0.9440\n",
      "Epoch 48: saving model to ./model/all3\\48-0.9423.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1737 - accuracy: 0.9418 - val_loss: 0.1744 - val_accuracy: 0.9423\n",
      "Epoch 49/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1431 - accuracy: 0.9580\n",
      "Epoch 49: saving model to ./model/all3\\49-0.9438.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1661 - accuracy: 0.9430 - val_loss: 0.1697 - val_accuracy: 0.9438\n",
      "Epoch 50/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1817 - accuracy: 0.9360\n",
      "Epoch 50: saving model to ./model/all3\\50-0.9438.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1627 - accuracy: 0.9453 - val_loss: 0.1704 - val_accuracy: 0.9438\n",
      "Epoch 51/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1669 - accuracy: 0.9360\n",
      "Epoch 51: saving model to ./model/all3\\51-0.9415.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1595 - accuracy: 0.9448 - val_loss: 0.1632 - val_accuracy: 0.9415\n",
      "Epoch 52/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1821 - accuracy: 0.9460\n",
      "Epoch 52: saving model to ./model/all3\\52-0.9431.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1556 - accuracy: 0.9474 - val_loss: 0.1581 - val_accuracy: 0.9431\n",
      "Epoch 53/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1304 - accuracy: 0.9620\n",
      "Epoch 53: saving model to ./model/all3\\53-0.9454.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1532 - accuracy: 0.9489 - val_loss: 0.1540 - val_accuracy: 0.9454\n",
      "Epoch 54/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1311 - accuracy: 0.9560\n",
      "Epoch 54: saving model to ./model/all3\\54-0.9477.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1479 - accuracy: 0.9484 - val_loss: 0.1518 - val_accuracy: 0.9477\n",
      "Epoch 55/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1608 - accuracy: 0.9520\n",
      "Epoch 55: saving model to ./model/all3\\55-0.9492.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1450 - accuracy: 0.9497 - val_loss: 0.1455 - val_accuracy: 0.9492\n",
      "Epoch 56/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1387 - accuracy: 0.9540\n",
      "Epoch 56: saving model to ./model/all3\\56-0.9462.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1396 - accuracy: 0.9556 - val_loss: 0.1460 - val_accuracy: 0.9462\n",
      "Epoch 57/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1503 - accuracy: 0.9460\n",
      "Epoch 57: saving model to ./model/all3\\57-0.9531.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1332 - accuracy: 0.9546 - val_loss: 0.1366 - val_accuracy: 0.9531\n",
      "Epoch 58/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1429 - accuracy: 0.9540\n",
      "Epoch 58: saving model to ./model/all3\\58-0.9554.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1298 - accuracy: 0.9574 - val_loss: 0.1321 - val_accuracy: 0.9554\n",
      "Epoch 59/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1189 - accuracy: 0.9640\n",
      "Epoch 59: saving model to ./model/all3\\59-0.9546.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1262 - accuracy: 0.9595 - val_loss: 0.1273 - val_accuracy: 0.9546\n",
      "Epoch 60/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1183 - accuracy: 0.9600\n",
      "Epoch 60: saving model to ./model/all3\\60-0.9531.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1218 - accuracy: 0.9623 - val_loss: 0.1243 - val_accuracy: 0.9531\n",
      "Epoch 61/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0978 - accuracy: 0.9760\n",
      "Epoch 61: saving model to ./model/all3\\61-0.9523.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1197 - accuracy: 0.9613 - val_loss: 0.1276 - val_accuracy: 0.9523\n",
      "Epoch 62/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1115 - accuracy: 0.9620\n",
      "Epoch 62: saving model to ./model/all3\\62-0.9500.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1185 - accuracy: 0.9643 - val_loss: 0.1286 - val_accuracy: 0.9500\n",
      "Epoch 63/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1319 - accuracy: 0.9460\n",
      "Epoch 63: saving model to ./model/all3\\63-0.9538.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1146 - accuracy: 0.9615 - val_loss: 0.1196 - val_accuracy: 0.9538\n",
      "Epoch 64/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0854 - accuracy: 0.9660\n",
      "Epoch 64: saving model to ./model/all3\\64-0.9638.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1104 - accuracy: 0.9636 - val_loss: 0.1132 - val_accuracy: 0.9638\n",
      "Epoch 65/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0941 - accuracy: 0.9760\n",
      "Epoch 65: saving model to ./model/all3\\65-0.9562.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1101 - accuracy: 0.9648 - val_loss: 0.1144 - val_accuracy: 0.9562\n",
      "Epoch 66/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0970 - accuracy: 0.9640\n",
      "Epoch 66: saving model to ./model/all3\\66-0.9608.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1056 - accuracy: 0.9648 - val_loss: 0.1097 - val_accuracy: 0.9608\n",
      "Epoch 67/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0952 - accuracy: 0.9680\n",
      "Epoch 67: saving model to ./model/all3\\67-0.9615.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1026 - accuracy: 0.9661 - val_loss: 0.1071 - val_accuracy: 0.9615\n",
      "Epoch 68/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0778 - accuracy: 0.9780\n",
      "Epoch 68: saving model to ./model/all3\\68-0.9623.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0998 - accuracy: 0.9684 - val_loss: 0.1065 - val_accuracy: 0.9623\n",
      "Epoch 69/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1233 - accuracy: 0.9680\n",
      "Epoch 69: saving model to ./model/all3\\69-0.9623.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0973 - accuracy: 0.9718 - val_loss: 0.1046 - val_accuracy: 0.9623\n",
      "Epoch 70/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0980 - accuracy: 0.9660\n",
      "Epoch 70: saving model to ./model/all3\\70-0.9646.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0951 - accuracy: 0.9718 - val_loss: 0.1024 - val_accuracy: 0.9646\n",
      "Epoch 71/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0862 - accuracy: 0.9800\n",
      "Epoch 71: saving model to ./model/all3\\71-0.9646.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0935 - accuracy: 0.9728 - val_loss: 0.1019 - val_accuracy: 0.9646\n",
      "Epoch 72/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0842 - accuracy: 0.9680\n",
      "Epoch 72: saving model to ./model/all3\\72-0.9669.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0938 - accuracy: 0.9700 - val_loss: 0.0997 - val_accuracy: 0.9669\n",
      "Epoch 73/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0729 - accuracy: 0.9800\n",
      "Epoch 73: saving model to ./model/all3\\73-0.9615.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0934 - accuracy: 0.9741 - val_loss: 0.1022 - val_accuracy: 0.9615\n",
      "Epoch 74/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0661 - accuracy: 0.9820\n",
      "Epoch 74: saving model to ./model/all3\\74-0.9692.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0926 - accuracy: 0.9715 - val_loss: 0.0976 - val_accuracy: 0.9692\n",
      "Epoch 75/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0737 - accuracy: 0.9800\n",
      "Epoch 75: saving model to ./model/all3\\75-0.9700.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0898 - accuracy: 0.9746 - val_loss: 0.0970 - val_accuracy: 0.9700\n",
      "Epoch 76/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9840\n",
      "Epoch 76: saving model to ./model/all3\\76-0.9700.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0875 - accuracy: 0.9751 - val_loss: 0.0956 - val_accuracy: 0.9700\n",
      "Epoch 77/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0693 - accuracy: 0.9800\n",
      "Epoch 77: saving model to ./model/all3\\77-0.9723.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0873 - accuracy: 0.9766 - val_loss: 0.0968 - val_accuracy: 0.9723\n",
      "Epoch 78/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0760 - accuracy: 0.9840\n",
      "Epoch 78: saving model to ./model/all3\\78-0.9723.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0870 - accuracy: 0.9743 - val_loss: 0.0941 - val_accuracy: 0.9723\n",
      "Epoch 79/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0859 - accuracy: 0.9820\n",
      "Epoch 79: saving model to ./model/all3\\79-0.9654.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0839 - accuracy: 0.9787 - val_loss: 0.0986 - val_accuracy: 0.9654\n",
      "Epoch 80/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0903 - accuracy: 0.9800\n",
      "Epoch 80: saving model to ./model/all3\\80-0.9700.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0833 - accuracy: 0.9756 - val_loss: 0.0925 - val_accuracy: 0.9700\n",
      "Epoch 81/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1047 - accuracy: 0.9740\n",
      "Epoch 81: saving model to ./model/all3\\81-0.9700.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0817 - accuracy: 0.9779 - val_loss: 0.0918 - val_accuracy: 0.9700\n",
      "Epoch 82/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0955 - accuracy: 0.9800\n",
      "Epoch 82: saving model to ./model/all3\\82-0.9700.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0807 - accuracy: 0.9774 - val_loss: 0.0911 - val_accuracy: 0.9700\n",
      "Epoch 83/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0856 - accuracy: 0.9740\n",
      "Epoch 83: saving model to ./model/all3\\83-0.9700.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0795 - accuracy: 0.9802 - val_loss: 0.0907 - val_accuracy: 0.9700\n",
      "Epoch 84/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0742 - accuracy: 0.9840\n",
      "Epoch 84: saving model to ./model/all3\\84-0.9692.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0791 - accuracy: 0.9802 - val_loss: 0.0944 - val_accuracy: 0.9692\n",
      "Epoch 85/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0622 - accuracy: 0.9740\n",
      "Epoch 85: saving model to ./model/all3\\85-0.9731.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0796 - accuracy: 0.9777 - val_loss: 0.0917 - val_accuracy: 0.9731\n",
      "Epoch 86/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0599 - accuracy: 0.9820\n",
      "Epoch 86: saving model to ./model/all3\\86-0.9654.keras\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0775 - accuracy: 0.9792 - val_loss: 0.1008 - val_accuracy: 0.9654\n",
      "Epoch 87/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0908 - accuracy: 0.9740\n",
      "Epoch 87: saving model to ./model/all3\\87-0.9669.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0813 - accuracy: 0.9766 - val_loss: 0.0977 - val_accuracy: 0.9669\n",
      "Epoch 88/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1057 - accuracy: 0.9580\n",
      "Epoch 88: saving model to ./model/all3\\88-0.9631.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0780 - accuracy: 0.9787 - val_loss: 0.1023 - val_accuracy: 0.9631\n",
      "Epoch 89/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0893 - accuracy: 0.9680\n",
      "Epoch 89: saving model to ./model/all3\\89-0.9608.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0807 - accuracy: 0.9761 - val_loss: 0.1107 - val_accuracy: 0.9608\n",
      "Epoch 90/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1066 - accuracy: 0.9600\n",
      "Epoch 90: saving model to ./model/all3\\90-0.9623.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0831 - accuracy: 0.9741 - val_loss: 0.1065 - val_accuracy: 0.9623\n",
      "Epoch 91/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0488 - accuracy: 0.9840\n",
      "Epoch 91: saving model to ./model/all3\\91-0.9646.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0802 - accuracy: 0.9766 - val_loss: 0.1004 - val_accuracy: 0.9646\n",
      "Epoch 92/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0932 - accuracy: 0.9720\n",
      "Epoch 92: saving model to ./model/all3\\92-0.9700.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0757 - accuracy: 0.9805 - val_loss: 0.0924 - val_accuracy: 0.9700\n",
      "Epoch 93/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0667 - accuracy: 0.9720\n",
      "Epoch 93: saving model to ./model/all3\\93-0.9662.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0748 - accuracy: 0.9810 - val_loss: 0.0987 - val_accuracy: 0.9662\n",
      "Epoch 94/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0695 - accuracy: 0.9780\n",
      "Epoch 94: saving model to ./model/all3\\94-0.9723.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0757 - accuracy: 0.9777 - val_loss: 0.0912 - val_accuracy: 0.9723\n",
      "Epoch 95/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1019 - accuracy: 0.9680\n",
      "Epoch 95: saving model to ./model/all3\\95-0.9654.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0752 - accuracy: 0.9795 - val_loss: 0.0993 - val_accuracy: 0.9654\n",
      "Epoch 96/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0940 - accuracy: 0.9720\n",
      "Epoch 96: saving model to ./model/all3\\96-0.9669.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0759 - accuracy: 0.9766 - val_loss: 0.0956 - val_accuracy: 0.9669\n",
      "Epoch 97/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1199 - accuracy: 0.9680\n",
      "Epoch 97: saving model to ./model/all3\\97-0.9677.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0767 - accuracy: 0.9777 - val_loss: 0.0965 - val_accuracy: 0.9677\n",
      "Epoch 98/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9780\n",
      "Epoch 98: saving model to ./model/all3\\98-0.9646.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0746 - accuracy: 0.9784 - val_loss: 0.0962 - val_accuracy: 0.9646\n",
      "Epoch 99/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0732 - accuracy: 0.9740\n",
      "Epoch 99: saving model to ./model/all3\\99-0.9738.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0714 - accuracy: 0.9813 - val_loss: 0.0860 - val_accuracy: 0.9738\n",
      "Epoch 100/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0741 - accuracy: 0.9780\n",
      "Epoch 100: saving model to ./model/all3\\100-0.9731.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0696 - accuracy: 0.9810 - val_loss: 0.0856 - val_accuracy: 0.9731\n",
      "Epoch 101/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0529 - accuracy: 0.9840\n",
      "Epoch 101: saving model to ./model/all3\\101-0.9723.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0691 - accuracy: 0.9836 - val_loss: 0.0877 - val_accuracy: 0.9723\n",
      "Epoch 102/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9920\n",
      "Epoch 102: saving model to ./model/all3\\102-0.9708.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0687 - accuracy: 0.9823 - val_loss: 0.0916 - val_accuracy: 0.9708\n",
      "Epoch 103/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9840\n",
      "Epoch 103: saving model to ./model/all3\\103-0.9654.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0726 - accuracy: 0.9815 - val_loss: 0.0980 - val_accuracy: 0.9654\n",
      "Epoch 104/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0943 - accuracy: 0.9700\n",
      "Epoch 104: saving model to ./model/all3\\104-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0716 - accuracy: 0.9777 - val_loss: 0.0841 - val_accuracy: 0.9754\n",
      "Epoch 105/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0619 - accuracy: 0.9820\n",
      "Epoch 105: saving model to ./model/all3\\105-0.9746.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0687 - accuracy: 0.9800 - val_loss: 0.0846 - val_accuracy: 0.9746\n",
      "Epoch 106/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0688 - accuracy: 0.9760\n",
      "Epoch 106: saving model to ./model/all3\\106-0.9746.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0663 - accuracy: 0.9823 - val_loss: 0.0830 - val_accuracy: 0.9746\n",
      "Epoch 107/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9760\n",
      "Epoch 107: saving model to ./model/all3\\107-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0666 - accuracy: 0.9833 - val_loss: 0.0804 - val_accuracy: 0.9762\n",
      "Epoch 108/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0514 - accuracy: 0.9880\n",
      "Epoch 108: saving model to ./model/all3\\108-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0653 - accuracy: 0.9828 - val_loss: 0.0804 - val_accuracy: 0.9762\n",
      "Epoch 109/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0705 - accuracy: 0.9860\n",
      "Epoch 109: saving model to ./model/all3\\109-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0648 - accuracy: 0.9818 - val_loss: 0.0812 - val_accuracy: 0.9754\n",
      "Epoch 110/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0704 - accuracy: 0.9880\n",
      "Epoch 110: saving model to ./model/all3\\110-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0652 - accuracy: 0.9841 - val_loss: 0.0793 - val_accuracy: 0.9762\n",
      "Epoch 111/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0461 - accuracy: 0.9820\n",
      "Epoch 111: saving model to ./model/all3\\111-0.9769.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0692 - accuracy: 0.9797 - val_loss: 0.0788 - val_accuracy: 0.9769\n",
      "Epoch 112/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9780\n",
      "Epoch 112: saving model to ./model/all3\\112-0.9738.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0700 - accuracy: 0.9790 - val_loss: 0.0783 - val_accuracy: 0.9738\n",
      "Epoch 113/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0557 - accuracy: 0.9840\n",
      "Epoch 113: saving model to ./model/all3\\113-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0675 - accuracy: 0.9823 - val_loss: 0.0790 - val_accuracy: 0.9754\n",
      "Epoch 114/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0611 - accuracy: 0.9840\n",
      "Epoch 114: saving model to ./model/all3\\114-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0683 - accuracy: 0.9808 - val_loss: 0.0779 - val_accuracy: 0.9769\n",
      "Epoch 115/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0757 - accuracy: 0.9720\n",
      "Epoch 115: saving model to ./model/all3\\115-0.9738.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0640 - accuracy: 0.9826 - val_loss: 0.0782 - val_accuracy: 0.9738\n",
      "Epoch 116/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0670 - accuracy: 0.9800\n",
      "Epoch 116: saving model to ./model/all3\\116-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0630 - accuracy: 0.9838 - val_loss: 0.0785 - val_accuracy: 0.9754\n",
      "Epoch 117/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0979 - accuracy: 0.9820\n",
      "Epoch 117: saving model to ./model/all3\\117-0.9762.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0628 - accuracy: 0.9836 - val_loss: 0.0777 - val_accuracy: 0.9762\n",
      "Epoch 118/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0494 - accuracy: 0.9820\n",
      "Epoch 118: saving model to ./model/all3\\118-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0621 - accuracy: 0.9846 - val_loss: 0.0772 - val_accuracy: 0.9769\n",
      "Epoch 119/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0393 - accuracy: 0.9900\n",
      "Epoch 119: saving model to ./model/all3\\119-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0626 - accuracy: 0.9838 - val_loss: 0.0774 - val_accuracy: 0.9762\n",
      "Epoch 120/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0998 - accuracy: 0.9700\n",
      "Epoch 120: saving model to ./model/all3\\120-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0652 - accuracy: 0.9815 - val_loss: 0.0793 - val_accuracy: 0.9762\n",
      "Epoch 121/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0831 - accuracy: 0.9740\n",
      "Epoch 121: saving model to ./model/all3\\121-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0654 - accuracy: 0.9823 - val_loss: 0.0770 - val_accuracy: 0.9769\n",
      "Epoch 122/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0493 - accuracy: 0.9840\n",
      "Epoch 122: saving model to ./model/all3\\122-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0663 - accuracy: 0.9813 - val_loss: 0.0780 - val_accuracy: 0.9754\n",
      "Epoch 123/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9920\n",
      "Epoch 123: saving model to ./model/all3\\123-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0662 - accuracy: 0.9820 - val_loss: 0.0765 - val_accuracy: 0.9762\n",
      "Epoch 124/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0741 - accuracy: 0.9820\n",
      "Epoch 124: saving model to ./model/all3\\124-0.9754.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0619 - accuracy: 0.9831 - val_loss: 0.0810 - val_accuracy: 0.9754\n",
      "Epoch 125/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1057 - accuracy: 0.9740\n",
      "Epoch 125: saving model to ./model/all3\\125-0.9754.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0607 - accuracy: 0.9851 - val_loss: 0.0800 - val_accuracy: 0.9754\n",
      "Epoch 126/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0654 - accuracy: 0.9860\n",
      "Epoch 126: saving model to ./model/all3\\126-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0608 - accuracy: 0.9846 - val_loss: 0.0770 - val_accuracy: 0.9769\n",
      "Epoch 127/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0411 - accuracy: 0.9840\n",
      "Epoch 127: saving model to ./model/all3\\127-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0595 - accuracy: 0.9838 - val_loss: 0.0779 - val_accuracy: 0.9762\n",
      "Epoch 128/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0876 - accuracy: 0.9800\n",
      "Epoch 128: saving model to ./model/all3\\128-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0600 - accuracy: 0.9846 - val_loss: 0.0779 - val_accuracy: 0.9754\n",
      "Epoch 129/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0636 - accuracy: 0.9820\n",
      "Epoch 129: saving model to ./model/all3\\129-0.9754.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0600 - accuracy: 0.9843 - val_loss: 0.0766 - val_accuracy: 0.9754\n",
      "Epoch 130/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0437 - accuracy: 0.9860\n",
      "Epoch 130: saving model to ./model/all3\\130-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.0746 - val_accuracy: 0.9762\n",
      "Epoch 131/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9820\n",
      "Epoch 131: saving model to ./model/all3\\131-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0613 - accuracy: 0.9833 - val_loss: 0.0755 - val_accuracy: 0.9785\n",
      "Epoch 132/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0918 - accuracy: 0.9820\n",
      "Epoch 132: saving model to ./model/all3\\132-0.9762.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0662 - accuracy: 0.9810 - val_loss: 0.0781 - val_accuracy: 0.9762\n",
      "Epoch 133/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0735 - accuracy: 0.9740\n",
      "Epoch 133: saving model to ./model/all3\\133-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0621 - accuracy: 0.9826 - val_loss: 0.0754 - val_accuracy: 0.9769\n",
      "Epoch 134/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0668 - accuracy: 0.9800\n",
      "Epoch 134: saving model to ./model/all3\\134-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0597 - accuracy: 0.9849 - val_loss: 0.0776 - val_accuracy: 0.9762\n",
      "Epoch 135/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0869 - accuracy: 0.9740\n",
      "Epoch 135: saving model to ./model/all3\\135-0.9769.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0607 - accuracy: 0.9826 - val_loss: 0.0758 - val_accuracy: 0.9769\n",
      "Epoch 136/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0769 - accuracy: 0.9860\n",
      "Epoch 136: saving model to ./model/all3\\136-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0808 - val_accuracy: 0.9754\n",
      "Epoch 137/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0732 - accuracy: 0.9680\n",
      "Epoch 137: saving model to ./model/all3\\137-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0593 - accuracy: 0.9841 - val_loss: 0.0790 - val_accuracy: 0.9769\n",
      "Epoch 138/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0819 - accuracy: 0.9840\n",
      "Epoch 138: saving model to ./model/all3\\138-0.9762.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0817 - val_accuracy: 0.9762\n",
      "Epoch 139/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0560 - accuracy: 0.9800\n",
      "Epoch 139: saving model to ./model/all3\\139-0.9754.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0594 - accuracy: 0.9833 - val_loss: 0.0824 - val_accuracy: 0.9754\n",
      "Epoch 140/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0467 - accuracy: 0.9800\n",
      "Epoch 140: saving model to ./model/all3\\140-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0754 - val_accuracy: 0.9769\n",
      "Epoch 141/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0712 - accuracy: 0.9820\n",
      "Epoch 141: saving model to ./model/all3\\141-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0575 - accuracy: 0.9849 - val_loss: 0.0740 - val_accuracy: 0.9769\n",
      "Epoch 142/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0597 - accuracy: 0.9840\n",
      "Epoch 142: saving model to ./model/all3\\142-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0564 - accuracy: 0.9838 - val_loss: 0.0800 - val_accuracy: 0.9777\n",
      "Epoch 143/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0374 - accuracy: 0.9880\n",
      "Epoch 143: saving model to ./model/all3\\143-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0576 - accuracy: 0.9851 - val_loss: 0.0774 - val_accuracy: 0.9769\n",
      "Epoch 144/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0474 - accuracy: 0.9880\n",
      "Epoch 144: saving model to ./model/all3\\144-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0570 - accuracy: 0.9843 - val_loss: 0.0727 - val_accuracy: 0.9769\n",
      "Epoch 145/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0471 - accuracy: 0.9880\n",
      "Epoch 145: saving model to ./model/all3\\145-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0563 - accuracy: 0.9851 - val_loss: 0.0738 - val_accuracy: 0.9762\n",
      "Epoch 146/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0378 - accuracy: 0.9920\n",
      "Epoch 146: saving model to ./model/all3\\146-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0589 - accuracy: 0.9843 - val_loss: 0.0753 - val_accuracy: 0.9777\n",
      "Epoch 147/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9860\n",
      "Epoch 147: saving model to ./model/all3\\147-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0572 - accuracy: 0.9851 - val_loss: 0.0720 - val_accuracy: 0.9785\n",
      "Epoch 148/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0727 - accuracy: 0.9860\n",
      "Epoch 148: saving model to ./model/all3\\148-0.9792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0560 - accuracy: 0.9851 - val_loss: 0.0727 - val_accuracy: 0.9792\n",
      "Epoch 149/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9880\n",
      "Epoch 149: saving model to ./model/all3\\149-0.9785.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0563 - accuracy: 0.9856 - val_loss: 0.0721 - val_accuracy: 0.9785\n",
      "Epoch 150/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0427 - accuracy: 0.9900\n",
      "Epoch 150: saving model to ./model/all3\\150-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0557 - accuracy: 0.9849 - val_loss: 0.0763 - val_accuracy: 0.9762\n",
      "Epoch 151/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9900\n",
      "Epoch 151: saving model to ./model/all3\\151-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0552 - accuracy: 0.9851 - val_loss: 0.0755 - val_accuracy: 0.9777\n",
      "Epoch 152/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0790 - accuracy: 0.9760\n",
      "Epoch 152: saving model to ./model/all3\\152-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0555 - accuracy: 0.9851 - val_loss: 0.0716 - val_accuracy: 0.9777\n",
      "Epoch 153/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0301 - accuracy: 0.9940\n",
      "Epoch 153: saving model to ./model/all3\\153-0.9769.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0575 - accuracy: 0.9856 - val_loss: 0.0787 - val_accuracy: 0.9769\n",
      "Epoch 154/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0653 - accuracy: 0.9800\n",
      "Epoch 154: saving model to ./model/all3\\154-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0606 - accuracy: 0.9828 - val_loss: 0.0753 - val_accuracy: 0.9777\n",
      "Epoch 155/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9920\n",
      "Epoch 155: saving model to ./model/all3\\155-0.9800.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0558 - accuracy: 0.9851 - val_loss: 0.0710 - val_accuracy: 0.9800\n",
      "Epoch 156/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0535 - accuracy: 0.9880\n",
      "Epoch 156: saving model to ./model/all3\\156-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0547 - accuracy: 0.9859 - val_loss: 0.0745 - val_accuracy: 0.9785\n",
      "Epoch 157/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0732 - accuracy: 0.9780\n",
      "Epoch 157: saving model to ./model/all3\\157-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0551 - accuracy: 0.9856 - val_loss: 0.0784 - val_accuracy: 0.9762\n",
      "Epoch 158/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0586 - accuracy: 0.9880\n",
      "Epoch 158: saving model to ./model/all3\\158-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0569 - accuracy: 0.9859 - val_loss: 0.0746 - val_accuracy: 0.9762\n",
      "Epoch 159/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0751 - accuracy: 0.9760\n",
      "Epoch 159: saving model to ./model/all3\\159-0.9754.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0551 - accuracy: 0.9864 - val_loss: 0.0803 - val_accuracy: 0.9754\n",
      "Epoch 160/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0440 - accuracy: 0.9780\n",
      "Epoch 160: saving model to ./model/all3\\160-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.0804 - val_accuracy: 0.9769\n",
      "Epoch 161/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9760\n",
      "Epoch 161: saving model to ./model/all3\\161-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0540 - accuracy: 0.9849 - val_loss: 0.0717 - val_accuracy: 0.9785\n",
      "Epoch 162/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0267 - accuracy: 0.9940\n",
      "Epoch 162: saving model to ./model/all3\\162-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0554 - accuracy: 0.9864 - val_loss: 0.0734 - val_accuracy: 0.9777\n",
      "Epoch 163/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0437 - accuracy: 0.9920\n",
      "Epoch 163: saving model to ./model/all3\\163-0.9777.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0577 - accuracy: 0.9856 - val_loss: 0.0725 - val_accuracy: 0.9777\n",
      "Epoch 164/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0365 - accuracy: 0.9880\n",
      "Epoch 164: saving model to ./model/all3\\164-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0543 - accuracy: 0.9864 - val_loss: 0.0703 - val_accuracy: 0.9785\n",
      "Epoch 165/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0681 - accuracy: 0.9780\n",
      "Epoch 165: saving model to ./model/all3\\165-0.9785.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0531 - accuracy: 0.9867 - val_loss: 0.0710 - val_accuracy: 0.9785\n",
      "Epoch 166/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0725 - accuracy: 0.9840\n",
      "Epoch 166: saving model to ./model/all3\\166-0.9792.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0528 - accuracy: 0.9869 - val_loss: 0.0732 - val_accuracy: 0.9792\n",
      "Epoch 167/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0591 - accuracy: 0.9840\n",
      "Epoch 167: saving model to ./model/all3\\167-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0574 - accuracy: 0.9831 - val_loss: 0.0816 - val_accuracy: 0.9769\n",
      "Epoch 168/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0617 - accuracy: 0.9780\n",
      "Epoch 168: saving model to ./model/all3\\168-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0620 - accuracy: 0.9813 - val_loss: 0.0812 - val_accuracy: 0.9762\n",
      "Epoch 169/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0648 - accuracy: 0.9820\n",
      "Epoch 169: saving model to ./model/all3\\169-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0602 - accuracy: 0.9823 - val_loss: 0.0766 - val_accuracy: 0.9777\n",
      "Epoch 170/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0676 - accuracy: 0.9840\n",
      "Epoch 170: saving model to ./model/all3\\170-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0563 - accuracy: 0.9854 - val_loss: 0.0697 - val_accuracy: 0.9785\n",
      "Epoch 171/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0717 - accuracy: 0.9880\n",
      "Epoch 171: saving model to ./model/all3\\171-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0525 - accuracy: 0.9872 - val_loss: 0.0795 - val_accuracy: 0.9754\n",
      "Epoch 172/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0560 - accuracy: 0.9780\n",
      "Epoch 172: saving model to ./model/all3\\172-0.9792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0533 - accuracy: 0.9856 - val_loss: 0.0700 - val_accuracy: 0.9792\n",
      "Epoch 173/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9840\n",
      "Epoch 173: saving model to ./model/all3\\173-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0518 - accuracy: 0.9867 - val_loss: 0.0699 - val_accuracy: 0.9785\n",
      "Epoch 174/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0649 - accuracy: 0.9820\n",
      "Epoch 174: saving model to ./model/all3\\174-0.9792.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0512 - accuracy: 0.9867 - val_loss: 0.0739 - val_accuracy: 0.9792\n",
      "Epoch 175/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0727 - accuracy: 0.9820\n",
      "Epoch 175: saving model to ./model/all3\\175-0.9746.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0525 - accuracy: 0.9869 - val_loss: 0.0814 - val_accuracy: 0.9746\n",
      "Epoch 176/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0654 - accuracy: 0.9860\n",
      "Epoch 176: saving model to ./model/all3\\176-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0542 - accuracy: 0.9854 - val_loss: 0.0752 - val_accuracy: 0.9769\n",
      "Epoch 177/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0544 - accuracy: 0.9820\n",
      "Epoch 177: saving model to ./model/all3\\177-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0519 - accuracy: 0.9874 - val_loss: 0.0741 - val_accuracy: 0.9785\n",
      "Epoch 178/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9840\n",
      "Epoch 178: saving model to ./model/all3\\178-0.9785.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0520 - accuracy: 0.9856 - val_loss: 0.0708 - val_accuracy: 0.9785\n",
      "Epoch 179/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0535 - accuracy: 0.9860\n",
      "Epoch 179: saving model to ./model/all3\\179-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0508 - accuracy: 0.9869 - val_loss: 0.0697 - val_accuracy: 0.9808\n",
      "Epoch 180/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0776 - accuracy: 0.9860\n",
      "Epoch 180: saving model to ./model/all3\\180-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0510 - accuracy: 0.9869 - val_loss: 0.0698 - val_accuracy: 0.9785\n",
      "Epoch 181/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0639 - accuracy: 0.9820\n",
      "Epoch 181: saving model to ./model/all3\\181-0.9777.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9859 - val_loss: 0.0730 - val_accuracy: 0.9777\n",
      "Epoch 182/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0759 - accuracy: 0.9820\n",
      "Epoch 182: saving model to ./model/all3\\182-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0508 - accuracy: 0.9869 - val_loss: 0.0739 - val_accuracy: 0.9769\n",
      "Epoch 183/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9920\n",
      "Epoch 183: saving model to ./model/all3\\183-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0504 - accuracy: 0.9864 - val_loss: 0.0704 - val_accuracy: 0.9785\n",
      "Epoch 184/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9800\n",
      "Epoch 184: saving model to ./model/all3\\184-0.9777.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0519 - accuracy: 0.9859 - val_loss: 0.0740 - val_accuracy: 0.9777\n",
      "Epoch 185/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0442 - accuracy: 0.9860\n",
      "Epoch 185: saving model to ./model/all3\\185-0.9754.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0532 - accuracy: 0.9872 - val_loss: 0.0774 - val_accuracy: 0.9754\n",
      "Epoch 186/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9880\n",
      "Epoch 186: saving model to ./model/all3\\186-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0536 - accuracy: 0.9854 - val_loss: 0.0740 - val_accuracy: 0.9785\n",
      "Epoch 187/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0498 - accuracy: 0.9800\n",
      "Epoch 187: saving model to ./model/all3\\187-0.9792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0521 - accuracy: 0.9861 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 188/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0389 - accuracy: 0.9920\n",
      "Epoch 188: saving model to ./model/all3\\188-0.9792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9874 - val_loss: 0.0716 - val_accuracy: 0.9792\n",
      "Epoch 189/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0244 - accuracy: 0.9900\n",
      "Epoch 189: saving model to ./model/all3\\189-0.9792.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0499 - accuracy: 0.9882 - val_loss: 0.0723 - val_accuracy: 0.9792\n",
      "Epoch 190/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9860\n",
      "Epoch 190: saving model to ./model/all3\\190-0.9792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0496 - accuracy: 0.9882 - val_loss: 0.0686 - val_accuracy: 0.9792\n",
      "Epoch 191/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0544 - accuracy: 0.9840\n",
      "Epoch 191: saving model to ./model/all3\\191-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0494 - accuracy: 0.9874 - val_loss: 0.0686 - val_accuracy: 0.9808\n",
      "Epoch 192/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0433 - accuracy: 0.9920\n",
      "Epoch 192: saving model to ./model/all3\\192-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0493 - accuracy: 0.9877 - val_loss: 0.0687 - val_accuracy: 0.9777\n",
      "Epoch 193/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0310 - accuracy: 0.9920\n",
      "Epoch 193: saving model to ./model/all3\\193-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0504 - accuracy: 0.9879 - val_loss: 0.0783 - val_accuracy: 0.9754\n",
      "Epoch 194/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0517 - accuracy: 0.9860\n",
      "Epoch 194: saving model to ./model/all3\\194-0.9738.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0533 - accuracy: 0.9856 - val_loss: 0.0801 - val_accuracy: 0.9738\n",
      "Epoch 195/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0436 - accuracy: 0.9800\n",
      "Epoch 195: saving model to ./model/all3\\195-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0541 - accuracy: 0.9856 - val_loss: 0.0870 - val_accuracy: 0.9754\n",
      "Epoch 196/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1391 - accuracy: 0.9660\n",
      "Epoch 196: saving model to ./model/all3\\196-0.9631.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0598 - accuracy: 0.9841 - val_loss: 0.1076 - val_accuracy: 0.9631\n",
      "Epoch 197/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0824 - accuracy: 0.9760\n",
      "Epoch 197: saving model to ./model/all3\\197-0.9623.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0620 - accuracy: 0.9818 - val_loss: 0.1117 - val_accuracy: 0.9623\n",
      "Epoch 198/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0797 - accuracy: 0.9740\n",
      "Epoch 198: saving model to ./model/all3\\198-0.9762.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0677 - accuracy: 0.9805 - val_loss: 0.0730 - val_accuracy: 0.9762\n",
      "Epoch 199/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0466 - accuracy: 0.9900\n",
      "Epoch 199: saving model to ./model/all3\\199-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0537 - accuracy: 0.9859 - val_loss: 0.0669 - val_accuracy: 0.9823\n",
      "Epoch 200/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9860\n",
      "Epoch 200: saving model to ./model/all3\\200-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0497 - accuracy: 0.9872 - val_loss: 0.0683 - val_accuracy: 0.9808\n",
      "Epoch 201/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9780\n",
      "Epoch 201: saving model to ./model/all3\\201-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0506 - accuracy: 0.9872 - val_loss: 0.0699 - val_accuracy: 0.9777\n",
      "Epoch 202/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0738 - accuracy: 0.9780\n",
      "Epoch 202: saving model to ./model/all3\\202-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9872 - val_loss: 0.0747 - val_accuracy: 0.9777\n",
      "Epoch 203/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9900\n",
      "Epoch 203: saving model to ./model/all3\\203-0.9715.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0543 - accuracy: 0.9846 - val_loss: 0.0934 - val_accuracy: 0.9715\n",
      "Epoch 204/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0610 - accuracy: 0.9760\n",
      "Epoch 204: saving model to ./model/all3\\204-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0752 - val_accuracy: 0.9769\n",
      "Epoch 205/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0346 - accuracy: 0.9900\n",
      "Epoch 205: saving model to ./model/all3\\205-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0604 - accuracy: 0.9823 - val_loss: 0.0696 - val_accuracy: 0.9800\n",
      "Epoch 206/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0619 - accuracy: 0.9860\n",
      "Epoch 206: saving model to ./model/all3\\206-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0500 - accuracy: 0.9867 - val_loss: 0.0671 - val_accuracy: 0.9831\n",
      "Epoch 207/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0541 - accuracy: 0.9900\n",
      "Epoch 207: saving model to ./model/all3\\207-0.9792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0511 - accuracy: 0.9864 - val_loss: 0.0688 - val_accuracy: 0.9792\n",
      "Epoch 208/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0632 - accuracy: 0.9800\n",
      "Epoch 208: saving model to ./model/all3\\208-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0521 - accuracy: 0.9869 - val_loss: 0.0682 - val_accuracy: 0.9808\n",
      "Epoch 209/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0515 - accuracy: 0.9820\n",
      "Epoch 209: saving model to ./model/all3\\209-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0537 - accuracy: 0.9859 - val_loss: 0.0700 - val_accuracy: 0.9785\n",
      "Epoch 210/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0478 - accuracy: 0.9860\n",
      "Epoch 210: saving model to ./model/all3\\210-0.9777.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0522 - accuracy: 0.9843 - val_loss: 0.0734 - val_accuracy: 0.9777\n",
      "Epoch 211/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0390 - accuracy: 0.9900\n",
      "Epoch 211: saving model to ./model/all3\\211-0.9792.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0513 - accuracy: 0.9867 - val_loss: 0.0684 - val_accuracy: 0.9792\n",
      "Epoch 212/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9920\n",
      "Epoch 212: saving model to ./model/all3\\212-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0501 - accuracy: 0.9887 - val_loss: 0.0678 - val_accuracy: 0.9808\n",
      "Epoch 213/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0683 - accuracy: 0.9860\n",
      "Epoch 213: saving model to ./model/all3\\213-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0490 - accuracy: 0.9887 - val_loss: 0.0679 - val_accuracy: 0.9815\n",
      "Epoch 214/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0410 - accuracy: 0.9840\n",
      "Epoch 214: saving model to ./model/all3\\214-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0476 - accuracy: 0.9885 - val_loss: 0.0732 - val_accuracy: 0.9785\n",
      "Epoch 215/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0832 - accuracy: 0.9820\n",
      "Epoch 215: saving model to ./model/all3\\215-0.9746.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0488 - accuracy: 0.9890 - val_loss: 0.0818 - val_accuracy: 0.9746\n",
      "Epoch 216/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0330 - accuracy: 0.9860\n",
      "Epoch 216: saving model to ./model/all3\\216-0.9754.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0530 - accuracy: 0.9856 - val_loss: 0.0858 - val_accuracy: 0.9754\n",
      "Epoch 217/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1024 - accuracy: 0.9780\n",
      "Epoch 217: saving model to ./model/all3\\217-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0551 - accuracy: 0.9867 - val_loss: 0.0675 - val_accuracy: 0.9800\n",
      "Epoch 218/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0465 - accuracy: 0.9880\n",
      "Epoch 218: saving model to ./model/all3\\218-0.9800.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0478 - accuracy: 0.9882 - val_loss: 0.0680 - val_accuracy: 0.9800\n",
      "Epoch 219/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0374 - accuracy: 0.9940\n",
      "Epoch 219: saving model to ./model/all3\\219-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0474 - accuracy: 0.9882 - val_loss: 0.0674 - val_accuracy: 0.9815\n",
      "Epoch 220/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0443 - accuracy: 0.9920\n",
      "Epoch 220: saving model to ./model/all3\\220-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0483 - accuracy: 0.9885 - val_loss: 0.0679 - val_accuracy: 0.9831\n",
      "Epoch 221/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0364 - accuracy: 0.9900\n",
      "Epoch 221: saving model to ./model/all3\\221-0.9823.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0469 - accuracy: 0.9885 - val_loss: 0.0673 - val_accuracy: 0.9823\n",
      "Epoch 222/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0438 - accuracy: 0.9920\n",
      "Epoch 222: saving model to ./model/all3\\222-0.9792.keras\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.0460 - accuracy: 0.9877 - val_loss: 0.0712 - val_accuracy: 0.9792\n",
      "Epoch 223/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0559 - accuracy: 0.9880\n",
      "Epoch 223: saving model to ./model/all3\\223-0.9815.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9882 - val_loss: 0.0665 - val_accuracy: 0.9815\n",
      "Epoch 224/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0365 - accuracy: 0.9920\n",
      "Epoch 224: saving model to ./model/all3\\224-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0470 - accuracy: 0.9885 - val_loss: 0.0668 - val_accuracy: 0.9831\n",
      "Epoch 225/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0477 - accuracy: 0.9860\n",
      "Epoch 225: saving model to ./model/all3\\225-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0465 - accuracy: 0.9885 - val_loss: 0.0698 - val_accuracy: 0.9785\n",
      "Epoch 226/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9900\n",
      "Epoch 226: saving model to ./model/all3\\226-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0471 - accuracy: 0.9877 - val_loss: 0.0731 - val_accuracy: 0.9777\n",
      "Epoch 227/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9920\n",
      "Epoch 227: saving model to ./model/all3\\227-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0473 - accuracy: 0.9869 - val_loss: 0.0698 - val_accuracy: 0.9800\n",
      "Epoch 228/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0391 - accuracy: 0.9860\n",
      "Epoch 228: saving model to ./model/all3\\228-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0468 - accuracy: 0.9885 - val_loss: 0.0725 - val_accuracy: 0.9769\n",
      "Epoch 229/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0607 - accuracy: 0.9860\n",
      "Epoch 229: saving model to ./model/all3\\229-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0460 - accuracy: 0.9879 - val_loss: 0.0660 - val_accuracy: 0.9831\n",
      "Epoch 230/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0432 - accuracy: 0.9960\n",
      "Epoch 230: saving model to ./model/all3\\230-0.9831.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0462 - accuracy: 0.9885 - val_loss: 0.0665 - val_accuracy: 0.9831\n",
      "Epoch 231/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0306 - accuracy: 0.9940\n",
      "Epoch 231: saving model to ./model/all3\\231-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0474 - accuracy: 0.9877 - val_loss: 0.0675 - val_accuracy: 0.9808\n",
      "Epoch 232/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0394 - accuracy: 0.9940\n",
      "Epoch 232: saving model to ./model/all3\\232-0.9785.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0465 - accuracy: 0.9887 - val_loss: 0.0688 - val_accuracy: 0.9785\n",
      "Epoch 233/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0374 - accuracy: 0.9920\n",
      "Epoch 233: saving model to ./model/all3\\233-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0474 - accuracy: 0.9885 - val_loss: 0.0662 - val_accuracy: 0.9831\n",
      "Epoch 234/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9900\n",
      "Epoch 234: saving model to ./model/all3\\234-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0489 - accuracy: 0.9892 - val_loss: 0.0678 - val_accuracy: 0.9800\n",
      "Epoch 235/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0569 - accuracy: 0.9800\n",
      "Epoch 235: saving model to ./model/all3\\235-0.9831.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0486 - accuracy: 0.9872 - val_loss: 0.0659 - val_accuracy: 0.9831\n",
      "Epoch 236/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0561 - accuracy: 0.9920\n",
      "Epoch 236: saving model to ./model/all3\\236-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0466 - accuracy: 0.9874 - val_loss: 0.0719 - val_accuracy: 0.9777\n",
      "Epoch 237/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0464 - accuracy: 0.9860\n",
      "Epoch 237: saving model to ./model/all3\\237-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0484 - accuracy: 0.9877 - val_loss: 0.0743 - val_accuracy: 0.9769\n",
      "Epoch 238/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0290 - accuracy: 0.9860\n",
      "Epoch 238: saving model to ./model/all3\\238-0.9792.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0473 - accuracy: 0.9879 - val_loss: 0.0739 - val_accuracy: 0.9792\n",
      "Epoch 239/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0595 - accuracy: 0.9840\n",
      "Epoch 239: saving model to ./model/all3\\239-0.9792.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0480 - accuracy: 0.9882 - val_loss: 0.0699 - val_accuracy: 0.9792\n",
      "Epoch 240/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0342 - accuracy: 0.9920\n",
      "Epoch 240: saving model to ./model/all3\\240-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0451 - accuracy: 0.9879 - val_loss: 0.0681 - val_accuracy: 0.9823\n",
      "Epoch 241/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9920\n",
      "Epoch 241: saving model to ./model/all3\\241-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0457 - accuracy: 0.9892 - val_loss: 0.0675 - val_accuracy: 0.9800\n",
      "Epoch 242/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0817 - accuracy: 0.9800\n",
      "Epoch 242: saving model to ./model/all3\\242-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0447 - accuracy: 0.9887 - val_loss: 0.0659 - val_accuracy: 0.9831\n",
      "Epoch 243/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0500 - accuracy: 0.9840\n",
      "Epoch 243: saving model to ./model/all3\\243-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0446 - accuracy: 0.9890 - val_loss: 0.0667 - val_accuracy: 0.9831\n",
      "Epoch 244/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0638 - accuracy: 0.9840\n",
      "Epoch 244: saving model to ./model/all3\\244-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0435 - accuracy: 0.9900 - val_loss: 0.0714 - val_accuracy: 0.9785\n",
      "Epoch 245/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0778 - accuracy: 0.9820\n",
      "Epoch 245: saving model to ./model/all3\\245-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0457 - accuracy: 0.9877 - val_loss: 0.0682 - val_accuracy: 0.9800\n",
      "Epoch 246/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0467 - accuracy: 0.9880\n",
      "Epoch 246: saving model to ./model/all3\\246-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0453 - accuracy: 0.9887 - val_loss: 0.0676 - val_accuracy: 0.9800\n",
      "Epoch 247/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0687 - accuracy: 0.9820\n",
      "Epoch 247: saving model to ./model/all3\\247-0.9800.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0440 - accuracy: 0.9897 - val_loss: 0.0683 - val_accuracy: 0.9800\n",
      "Epoch 248/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0347 - accuracy: 0.9920\n",
      "Epoch 248: saving model to ./model/all3\\248-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0447 - accuracy: 0.9887 - val_loss: 0.0652 - val_accuracy: 0.9831\n",
      "Epoch 249/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0216 - accuracy: 0.9940\n",
      "Epoch 249: saving model to ./model/all3\\249-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0468 - accuracy: 0.9890 - val_loss: 0.0654 - val_accuracy: 0.9823\n",
      "Epoch 250/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0648 - accuracy: 0.9880\n",
      "Epoch 250: saving model to ./model/all3\\250-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0439 - accuracy: 0.9897 - val_loss: 0.0696 - val_accuracy: 0.9815\n",
      "Epoch 251/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0379 - accuracy: 0.9900\n",
      "Epoch 251: saving model to ./model/all3\\251-0.9831.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0435 - accuracy: 0.9890 - val_loss: 0.0649 - val_accuracy: 0.9831\n",
      "Epoch 252/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9860\n",
      "Epoch 252: saving model to ./model/all3\\252-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0444 - accuracy: 0.9890 - val_loss: 0.0655 - val_accuracy: 0.9838\n",
      "Epoch 253/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0652 - accuracy: 0.9820\n",
      "Epoch 253: saving model to ./model/all3\\253-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0442 - accuracy: 0.9892 - val_loss: 0.0709 - val_accuracy: 0.9785\n",
      "Epoch 254/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0639 - accuracy: 0.9880\n",
      "Epoch 254: saving model to ./model/all3\\254-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0455 - accuracy: 0.9885 - val_loss: 0.0658 - val_accuracy: 0.9823\n",
      "Epoch 255/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0623 - accuracy: 0.9880\n",
      "Epoch 255: saving model to ./model/all3\\255-0.9823.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0438 - accuracy: 0.9897 - val_loss: 0.0665 - val_accuracy: 0.9823\n",
      "Epoch 256/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0245 - accuracy: 0.9940\n",
      "Epoch 256: saving model to ./model/all3\\256-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0445 - accuracy: 0.9882 - val_loss: 0.0658 - val_accuracy: 0.9831\n",
      "Epoch 257/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0240 - accuracy: 0.9900\n",
      "Epoch 257: saving model to ./model/all3\\257-0.9823.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0433 - accuracy: 0.9892 - val_loss: 0.0651 - val_accuracy: 0.9823\n",
      "Epoch 258/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0406 - accuracy: 0.9860\n",
      "Epoch 258: saving model to ./model/all3\\258-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0431 - accuracy: 0.9887 - val_loss: 0.0650 - val_accuracy: 0.9831\n",
      "Epoch 259/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9940\n",
      "Epoch 259: saving model to ./model/all3\\259-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0454 - accuracy: 0.9887 - val_loss: 0.0657 - val_accuracy: 0.9823\n",
      "Epoch 260/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0261 - accuracy: 0.9960\n",
      "Epoch 260: saving model to ./model/all3\\260-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0437 - accuracy: 0.9910 - val_loss: 0.0651 - val_accuracy: 0.9831\n",
      "Epoch 261/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0551 - accuracy: 0.9860\n",
      "Epoch 261: saving model to ./model/all3\\261-0.9831.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0426 - accuracy: 0.9895 - val_loss: 0.0663 - val_accuracy: 0.9831\n",
      "Epoch 262/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0854 - accuracy: 0.9840\n",
      "Epoch 262: saving model to ./model/all3\\262-0.9792.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0432 - accuracy: 0.9887 - val_loss: 0.0679 - val_accuracy: 0.9792\n",
      "Epoch 263/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0422 - accuracy: 0.9860\n",
      "Epoch 263: saving model to ./model/all3\\263-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0438 - accuracy: 0.9879 - val_loss: 0.0661 - val_accuracy: 0.9846\n",
      "Epoch 264/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0448 - accuracy: 0.9920\n",
      "Epoch 264: saving model to ./model/all3\\264-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0447 - accuracy: 0.9892 - val_loss: 0.0665 - val_accuracy: 0.9823\n",
      "Epoch 265/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0300 - accuracy: 0.9940\n",
      "Epoch 265: saving model to ./model/all3\\265-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0432 - accuracy: 0.9892 - val_loss: 0.0648 - val_accuracy: 0.9815\n",
      "Epoch 266/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0494 - accuracy: 0.9900\n",
      "Epoch 266: saving model to ./model/all3\\266-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0443 - accuracy: 0.9890 - val_loss: 0.0643 - val_accuracy: 0.9831\n",
      "Epoch 267/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0630 - accuracy: 0.9860\n",
      "Epoch 267: saving model to ./model/all3\\267-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0438 - accuracy: 0.9892 - val_loss: 0.0643 - val_accuracy: 0.9831\n",
      "Epoch 268/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0437 - accuracy: 0.9960\n",
      "Epoch 268: saving model to ./model/all3\\268-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0427 - accuracy: 0.9897 - val_loss: 0.0645 - val_accuracy: 0.9831\n",
      "Epoch 269/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0230 - accuracy: 0.9960\n",
      "Epoch 269: saving model to ./model/all3\\269-0.9823.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0439 - accuracy: 0.9892 - val_loss: 0.0667 - val_accuracy: 0.9823\n",
      "Epoch 270/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9820\n",
      "Epoch 270: saving model to ./model/all3\\270-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0426 - accuracy: 0.9892 - val_loss: 0.0691 - val_accuracy: 0.9808\n",
      "Epoch 271/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0622 - accuracy: 0.9920\n",
      "Epoch 271: saving model to ./model/all3\\271-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0440 - accuracy: 0.9895 - val_loss: 0.0649 - val_accuracy: 0.9854\n",
      "Epoch 272/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0128 - accuracy: 0.9980\n",
      "Epoch 272: saving model to ./model/all3\\272-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0420 - accuracy: 0.9905 - val_loss: 0.0647 - val_accuracy: 0.9831\n",
      "Epoch 273/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0421 - accuracy: 0.9900\n",
      "Epoch 273: saving model to ./model/all3\\273-0.9831.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0420 - accuracy: 0.9905 - val_loss: 0.0645 - val_accuracy: 0.9831\n",
      "Epoch 274/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 274: saving model to ./model/all3\\274-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0421 - accuracy: 0.9905 - val_loss: 0.0640 - val_accuracy: 0.9838\n",
      "Epoch 275/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0204 - accuracy: 0.9980\n",
      "Epoch 275: saving model to ./model/all3\\275-0.9838.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0441 - accuracy: 0.9890 - val_loss: 0.0639 - val_accuracy: 0.9838\n",
      "Epoch 276/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0232 - accuracy: 0.9960\n",
      "Epoch 276: saving model to ./model/all3\\276-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0428 - accuracy: 0.9902 - val_loss: 0.0661 - val_accuracy: 0.9831\n",
      "Epoch 277/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0182 - accuracy: 0.9980\n",
      "Epoch 277: saving model to ./model/all3\\277-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0415 - accuracy: 0.9902 - val_loss: 0.0640 - val_accuracy: 0.9846\n",
      "Epoch 278/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0385 - accuracy: 0.9860\n",
      "Epoch 278: saving model to ./model/all3\\278-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0422 - accuracy: 0.9897 - val_loss: 0.0643 - val_accuracy: 0.9854\n",
      "Epoch 279/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0321 - accuracy: 0.9920\n",
      "Epoch 279: saving model to ./model/all3\\279-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0417 - accuracy: 0.9895 - val_loss: 0.0636 - val_accuracy: 0.9846\n",
      "Epoch 280/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0509 - accuracy: 0.9860\n",
      "Epoch 280: saving model to ./model/all3\\280-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0410 - accuracy: 0.9900 - val_loss: 0.0650 - val_accuracy: 0.9854\n",
      "Epoch 281/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0221 - accuracy: 0.9980\n",
      "Epoch 281: saving model to ./model/all3\\281-0.9831.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0409 - accuracy: 0.9902 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
      "Epoch 282/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0482 - accuracy: 0.9860\n",
      "Epoch 282: saving model to ./model/all3\\282-0.9815.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.9892 - val_loss: 0.0657 - val_accuracy: 0.9815\n",
      "Epoch 283/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0646 - accuracy: 0.9880\n",
      "Epoch 283: saving model to ./model/all3\\283-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0470 - accuracy: 0.9861 - val_loss: 0.0713 - val_accuracy: 0.9785\n",
      "Epoch 284/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0583 - accuracy: 0.9800\n",
      "Epoch 284: saving model to ./model/all3\\284-0.9808.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0471 - accuracy: 0.9854 - val_loss: 0.0687 - val_accuracy: 0.9808\n",
      "Epoch 285/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0641 - accuracy: 0.9800\n",
      "Epoch 285: saving model to ./model/all3\\285-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0455 - accuracy: 0.9879 - val_loss: 0.0643 - val_accuracy: 0.9838\n",
      "Epoch 286/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0413 - accuracy: 0.9840\n",
      "Epoch 286: saving model to ./model/all3\\286-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0411 - accuracy: 0.9897 - val_loss: 0.0635 - val_accuracy: 0.9846\n",
      "Epoch 287/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9920\n",
      "Epoch 287: saving model to ./model/all3\\287-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0401 - accuracy: 0.9902 - val_loss: 0.0640 - val_accuracy: 0.9854\n",
      "Epoch 288/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0191 - accuracy: 0.9920\n",
      "Epoch 288: saving model to ./model/all3\\288-0.9854.keras\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0405 - accuracy: 0.9902 - val_loss: 0.0637 - val_accuracy: 0.9854\n",
      "Epoch 289/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0217 - accuracy: 0.9940\n",
      "Epoch 289: saving model to ./model/all3\\289-0.9854.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9900 - val_loss: 0.0630 - val_accuracy: 0.9854\n",
      "Epoch 290/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0318 - accuracy: 0.9940\n",
      "Epoch 290: saving model to ./model/all3\\290-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0405 - accuracy: 0.9900 - val_loss: 0.0645 - val_accuracy: 0.9823\n",
      "Epoch 291/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0523 - accuracy: 0.9820\n",
      "Epoch 291: saving model to ./model/all3\\291-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0437 - accuracy: 0.9872 - val_loss: 0.0750 - val_accuracy: 0.9777\n",
      "Epoch 292/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0454 - accuracy: 0.9860\n",
      "Epoch 292: saving model to ./model/all3\\292-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0452 - accuracy: 0.9879 - val_loss: 0.0637 - val_accuracy: 0.9854\n",
      "Epoch 293/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0206 - accuracy: 0.9920\n",
      "Epoch 293: saving model to ./model/all3\\293-0.9838.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0404 - accuracy: 0.9895 - val_loss: 0.0664 - val_accuracy: 0.9838\n",
      "Epoch 294/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0188 - accuracy: 0.9940\n",
      "Epoch 294: saving model to ./model/all3\\294-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0416 - accuracy: 0.9897 - val_loss: 0.0689 - val_accuracy: 0.9831\n",
      "Epoch 295/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0227 - accuracy: 0.9960\n",
      "Epoch 295: saving model to ./model/all3\\295-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0403 - accuracy: 0.9910 - val_loss: 0.0635 - val_accuracy: 0.9854\n",
      "Epoch 296/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0510 - accuracy: 0.9880\n",
      "Epoch 296: saving model to ./model/all3\\296-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0414 - accuracy: 0.9895 - val_loss: 0.0703 - val_accuracy: 0.9815\n",
      "Epoch 297/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0324 - accuracy: 0.9880\n",
      "Epoch 297: saving model to ./model/all3\\297-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0410 - accuracy: 0.9892 - val_loss: 0.0651 - val_accuracy: 0.9838\n",
      "Epoch 298/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0251 - accuracy: 0.9940\n",
      "Epoch 298: saving model to ./model/all3\\298-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0412 - accuracy: 0.9902 - val_loss: 0.0704 - val_accuracy: 0.9815\n",
      "Epoch 299/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0562 - accuracy: 0.9880\n",
      "Epoch 299: saving model to ./model/all3\\299-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0418 - accuracy: 0.9908 - val_loss: 0.0691 - val_accuracy: 0.9815\n",
      "Epoch 300/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0576 - accuracy: 0.9880\n",
      "Epoch 300: saving model to ./model/all3\\300-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0404 - accuracy: 0.9900 - val_loss: 0.0654 - val_accuracy: 0.9854\n",
      "Epoch 301/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0455 - accuracy: 0.9920\n",
      "Epoch 301: saving model to ./model/all3\\301-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0387 - accuracy: 0.9910 - val_loss: 0.0622 - val_accuracy: 0.9846\n",
      "Epoch 302/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9880\n",
      "Epoch 302: saving model to ./model/all3\\302-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0395 - accuracy: 0.9905 - val_loss: 0.0629 - val_accuracy: 0.9862\n",
      "Epoch 303/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0680 - accuracy: 0.9880\n",
      "Epoch 303: saving model to ./model/all3\\303-0.9831.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9900 - val_loss: 0.0658 - val_accuracy: 0.9831\n",
      "Epoch 304/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0674 - accuracy: 0.9840\n",
      "Epoch 304: saving model to ./model/all3\\304-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0399 - accuracy: 0.9910 - val_loss: 0.0761 - val_accuracy: 0.9777\n",
      "Epoch 305/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0241 - accuracy: 0.9940\n",
      "Epoch 305: saving model to ./model/all3\\305-0.9785.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0431 - accuracy: 0.9885 - val_loss: 0.0733 - val_accuracy: 0.9785\n",
      "Epoch 306/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0493 - accuracy: 0.9940\n",
      "Epoch 306: saving model to ./model/all3\\306-0.9777.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0433 - accuracy: 0.9877 - val_loss: 0.0747 - val_accuracy: 0.9777\n",
      "Epoch 307/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0659 - accuracy: 0.9880\n",
      "Epoch 307: saving model to ./model/all3\\307-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0445 - accuracy: 0.9885 - val_loss: 0.0671 - val_accuracy: 0.9838\n",
      "Epoch 308/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0233 - accuracy: 0.9900\n",
      "Epoch 308: saving model to ./model/all3\\308-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0392 - accuracy: 0.9905 - val_loss: 0.0705 - val_accuracy: 0.9815\n",
      "Epoch 309/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0490 - accuracy: 0.9860\n",
      "Epoch 309: saving model to ./model/all3\\309-0.9769.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0430 - accuracy: 0.9887 - val_loss: 0.0868 - val_accuracy: 0.9769\n",
      "Epoch 310/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0748 - accuracy: 0.9800\n",
      "Epoch 310: saving model to ./model/all3\\310-0.9785.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0450 - accuracy: 0.9877 - val_loss: 0.0764 - val_accuracy: 0.9785\n",
      "Epoch 311/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0224 - accuracy: 0.9880\n",
      "Epoch 311: saving model to ./model/all3\\311-0.9800.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0444 - accuracy: 0.9877 - val_loss: 0.0740 - val_accuracy: 0.9800\n",
      "Epoch 312/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9860\n",
      "Epoch 312: saving model to ./model/all3\\312-0.9815.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 0.9892 - val_loss: 0.0684 - val_accuracy: 0.9815\n",
      "Epoch 313/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0284 - accuracy: 0.9920\n",
      "Epoch 313: saving model to ./model/all3\\313-0.9854.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0427 - accuracy: 0.9887 - val_loss: 0.0613 - val_accuracy: 0.9854\n",
      "Epoch 314/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0570 - accuracy: 0.9880\n",
      "Epoch 314: saving model to ./model/all3\\314-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0439 - accuracy: 0.9879 - val_loss: 0.0618 - val_accuracy: 0.9862\n",
      "Epoch 315/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0476 - accuracy: 0.9900\n",
      "Epoch 315: saving model to ./model/all3\\315-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0385 - accuracy: 0.9902 - val_loss: 0.0630 - val_accuracy: 0.9854\n",
      "Epoch 316/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9880\n",
      "Epoch 316: saving model to ./model/all3\\316-0.9862.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0377 - accuracy: 0.9913 - val_loss: 0.0627 - val_accuracy: 0.9862\n",
      "Epoch 317/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0369 - accuracy: 0.9900\n",
      "Epoch 317: saving model to ./model/all3\\317-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0379 - accuracy: 0.9908 - val_loss: 0.0617 - val_accuracy: 0.9854\n",
      "Epoch 318/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0518 - accuracy: 0.9840\n",
      "Epoch 318: saving model to ./model/all3\\318-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0383 - accuracy: 0.9900 - val_loss: 0.0615 - val_accuracy: 0.9854\n",
      "Epoch 319/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0379 - accuracy: 0.9940\n",
      "Epoch 319: saving model to ./model/all3\\319-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0377 - accuracy: 0.9913 - val_loss: 0.0610 - val_accuracy: 0.9869\n",
      "Epoch 320/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9920\n",
      "Epoch 320: saving model to ./model/all3\\320-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0381 - accuracy: 0.9908 - val_loss: 0.0608 - val_accuracy: 0.9846\n",
      "Epoch 321/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0282 - accuracy: 0.9960\n",
      "Epoch 321: saving model to ./model/all3\\321-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0390 - accuracy: 0.9913 - val_loss: 0.0641 - val_accuracy: 0.9831\n",
      "Epoch 322/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0554 - accuracy: 0.9900\n",
      "Epoch 322: saving model to ./model/all3\\322-0.9831.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0434 - accuracy: 0.9874 - val_loss: 0.0644 - val_accuracy: 0.9831\n",
      "Epoch 323/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0659 - accuracy: 0.9820\n",
      "Epoch 323: saving model to ./model/all3\\323-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0410 - accuracy: 0.9897 - val_loss: 0.0654 - val_accuracy: 0.9823\n",
      "Epoch 324/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0655 - accuracy: 0.9860\n",
      "Epoch 324: saving model to ./model/all3\\324-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0414 - accuracy: 0.9897 - val_loss: 0.0629 - val_accuracy: 0.9846\n",
      "Epoch 325/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0484 - accuracy: 0.9900\n",
      "Epoch 325: saving model to ./model/all3\\325-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0400 - accuracy: 0.9908 - val_loss: 0.0612 - val_accuracy: 0.9862\n",
      "Epoch 326/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0417 - accuracy: 0.9940\n",
      "Epoch 326: saving model to ./model/all3\\326-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0380 - accuracy: 0.9905 - val_loss: 0.0605 - val_accuracy: 0.9862\n",
      "Epoch 327/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0461 - accuracy: 0.9860\n",
      "Epoch 327: saving model to ./model/all3\\327-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0377 - accuracy: 0.9913 - val_loss: 0.0616 - val_accuracy: 0.9854\n",
      "Epoch 328/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0324 - accuracy: 0.9940\n",
      "Epoch 328: saving model to ./model/all3\\328-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0374 - accuracy: 0.9908 - val_loss: 0.0624 - val_accuracy: 0.9862\n",
      "Epoch 329/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0157 - accuracy: 0.9960\n",
      "Epoch 329: saving model to ./model/all3\\329-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 0.9918 - val_loss: 0.0672 - val_accuracy: 0.9831\n",
      "Epoch 330/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9940\n",
      "Epoch 330: saving model to ./model/all3\\330-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0379 - accuracy: 0.9915 - val_loss: 0.0633 - val_accuracy: 0.9846\n",
      "Epoch 331/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0551 - accuracy: 0.9880\n",
      "Epoch 331: saving model to ./model/all3\\331-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0369 - accuracy: 0.9910 - val_loss: 0.0619 - val_accuracy: 0.9854\n",
      "Epoch 332/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0378 - accuracy: 0.9880\n",
      "Epoch 332: saving model to ./model/all3\\332-0.9846.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0365 - accuracy: 0.9926 - val_loss: 0.0645 - val_accuracy: 0.9846\n",
      "Epoch 333/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0358 - accuracy: 0.9940\n",
      "Epoch 333: saving model to ./model/all3\\333-0.9777.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0394 - accuracy: 0.9910 - val_loss: 0.0736 - val_accuracy: 0.9777\n",
      "Epoch 334/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0401 - accuracy: 0.9840\n",
      "Epoch 334: saving model to ./model/all3\\334-0.9800.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0411 - accuracy: 0.9895 - val_loss: 0.0729 - val_accuracy: 0.9800\n",
      "Epoch 335/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0527 - accuracy: 0.9880\n",
      "Epoch 335: saving model to ./model/all3\\335-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0392 - accuracy: 0.9910 - val_loss: 0.0691 - val_accuracy: 0.9823\n",
      "Epoch 336/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0131 - accuracy: 0.9980\n",
      "Epoch 336: saving model to ./model/all3\\336-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0400 - accuracy: 0.9890 - val_loss: 0.0690 - val_accuracy: 0.9823\n",
      "Epoch 337/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0364 - accuracy: 0.9880\n",
      "Epoch 337: saving model to ./model/all3\\337-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0380 - accuracy: 0.9902 - val_loss: 0.0607 - val_accuracy: 0.9869\n",
      "Epoch 338/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0386 - accuracy: 0.9900\n",
      "Epoch 338: saving model to ./model/all3\\338-0.9846.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0372 - accuracy: 0.9910 - val_loss: 0.0611 - val_accuracy: 0.9846\n",
      "Epoch 339/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0481 - accuracy: 0.9920\n",
      "Epoch 339: saving model to ./model/all3\\339-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9918 - val_loss: 0.0612 - val_accuracy: 0.9869\n",
      "Epoch 340/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0236 - accuracy: 0.9940\n",
      "Epoch 340: saving model to ./model/all3\\340-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 0.9913 - val_loss: 0.0617 - val_accuracy: 0.9854\n",
      "Epoch 341/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0560 - accuracy: 0.9860\n",
      "Epoch 341: saving model to ./model/all3\\341-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0365 - accuracy: 0.9910 - val_loss: 0.0637 - val_accuracy: 0.9846\n",
      "Epoch 342/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0662 - accuracy: 0.9880\n",
      "Epoch 342: saving model to ./model/all3\\342-0.9846.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9885 - val_loss: 0.0624 - val_accuracy: 0.9846\n",
      "Epoch 343/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0536 - accuracy: 0.9940\n",
      "Epoch 343: saving model to ./model/all3\\343-0.9862.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0418 - accuracy: 0.9872 - val_loss: 0.0613 - val_accuracy: 0.9862\n",
      "Epoch 344/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0348 - accuracy: 0.9900\n",
      "Epoch 344: saving model to ./model/all3\\344-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.9910 - val_loss: 0.0612 - val_accuracy: 0.9854\n",
      "Epoch 345/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0425 - accuracy: 0.9900\n",
      "Epoch 345: saving model to ./model/all3\\345-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0377 - accuracy: 0.9913 - val_loss: 0.0609 - val_accuracy: 0.9862\n",
      "Epoch 346/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0464 - accuracy: 0.9880\n",
      "Epoch 346: saving model to ./model/all3\\346-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.9920 - val_loss: 0.0612 - val_accuracy: 0.9862\n",
      "Epoch 347/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0234 - accuracy: 0.9940\n",
      "Epoch 347: saving model to ./model/all3\\347-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0386 - accuracy: 0.9908 - val_loss: 0.0613 - val_accuracy: 0.9862\n",
      "Epoch 348/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0293 - accuracy: 0.9940\n",
      "Epoch 348: saving model to ./model/all3\\348-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0381 - accuracy: 0.9908 - val_loss: 0.0602 - val_accuracy: 0.9862\n",
      "Epoch 349/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0308 - accuracy: 0.9960\n",
      "Epoch 349: saving model to ./model/all3\\349-0.9846.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9915 - val_loss: 0.0619 - val_accuracy: 0.9846\n",
      "Epoch 350/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0252 - accuracy: 0.9940\n",
      "Epoch 350: saving model to ./model/all3\\350-0.9846.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.9913 - val_loss: 0.0619 - val_accuracy: 0.9846\n",
      "Epoch 351/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0349 - accuracy: 0.9880\n",
      "Epoch 351: saving model to ./model/all3\\351-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0367 - accuracy: 0.9902 - val_loss: 0.0608 - val_accuracy: 0.9862\n",
      "Epoch 352/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0513 - accuracy: 0.9880\n",
      "Epoch 352: saving model to ./model/all3\\352-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.9902 - val_loss: 0.0596 - val_accuracy: 0.9854\n",
      "Epoch 353/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0569 - accuracy: 0.9880\n",
      "Epoch 353: saving model to ./model/all3\\353-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 0.9926 - val_loss: 0.0627 - val_accuracy: 0.9846\n",
      "Epoch 354/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0357 - accuracy: 0.9900\n",
      "Epoch 354: saving model to ./model/all3\\354-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9915 - val_loss: 0.0648 - val_accuracy: 0.9846\n",
      "Epoch 355/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0244 - accuracy: 0.9880\n",
      "Epoch 355: saving model to ./model/all3\\355-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0382 - accuracy: 0.9908 - val_loss: 0.0652 - val_accuracy: 0.9846\n",
      "Epoch 356/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9920\n",
      "Epoch 356: saving model to ./model/all3\\356-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0363 - accuracy: 0.9913 - val_loss: 0.0613 - val_accuracy: 0.9862\n",
      "Epoch 357/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0185 - accuracy: 0.9960\n",
      "Epoch 357: saving model to ./model/all3\\357-0.9846.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0360 - accuracy: 0.9910 - val_loss: 0.0624 - val_accuracy: 0.9846\n",
      "Epoch 358/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0265 - accuracy: 0.9940\n",
      "Epoch 358: saving model to ./model/all3\\358-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0368 - accuracy: 0.9915 - val_loss: 0.0616 - val_accuracy: 0.9862\n",
      "Epoch 359/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0233 - accuracy: 0.9900\n",
      "Epoch 359: saving model to ./model/all3\\359-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9905 - val_loss: 0.0608 - val_accuracy: 0.9862\n",
      "Epoch 360/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0720 - accuracy: 0.9860\n",
      "Epoch 360: saving model to ./model/all3\\360-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0365 - accuracy: 0.9918 - val_loss: 0.0665 - val_accuracy: 0.9831\n",
      "Epoch 361/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0365 - accuracy: 0.9940\n",
      "Epoch 361: saving model to ./model/all3\\361-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0383 - accuracy: 0.9910 - val_loss: 0.0639 - val_accuracy: 0.9862\n",
      "Epoch 362/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0527 - accuracy: 0.9900\n",
      "Epoch 362: saving model to ./model/all3\\362-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0369 - accuracy: 0.9920 - val_loss: 0.0617 - val_accuracy: 0.9854\n",
      "Epoch 363/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0243 - accuracy: 0.9960\n",
      "Epoch 363: saving model to ./model/all3\\363-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0357 - accuracy: 0.9923 - val_loss: 0.0654 - val_accuracy: 0.9831\n",
      "Epoch 364/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0627 - accuracy: 0.9920\n",
      "Epoch 364: saving model to ./model/all3\\364-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0372 - accuracy: 0.9910 - val_loss: 0.0625 - val_accuracy: 0.9854\n",
      "Epoch 365/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0447 - accuracy: 0.9900\n",
      "Epoch 365: saving model to ./model/all3\\365-0.9869.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0367 - accuracy: 0.9920 - val_loss: 0.0591 - val_accuracy: 0.9869\n",
      "Epoch 366/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9940\n",
      "Epoch 366: saving model to ./model/all3\\366-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0371 - accuracy: 0.9915 - val_loss: 0.0595 - val_accuracy: 0.9862\n",
      "Epoch 367/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0483 - accuracy: 0.9920\n",
      "Epoch 367: saving model to ./model/all3\\367-0.9831.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0354 - accuracy: 0.9926 - val_loss: 0.0641 - val_accuracy: 0.9831\n",
      "Epoch 368/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0408 - accuracy: 0.9920\n",
      "Epoch 368: saving model to ./model/all3\\368-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0373 - accuracy: 0.9918 - val_loss: 0.0695 - val_accuracy: 0.9808\n",
      "Epoch 369/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0575 - accuracy: 0.9860\n",
      "Epoch 369: saving model to ./model/all3\\369-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0391 - accuracy: 0.9895 - val_loss: 0.0663 - val_accuracy: 0.9808\n",
      "Epoch 370/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0441 - accuracy: 0.9880\n",
      "Epoch 370: saving model to ./model/all3\\370-0.9831.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9897 - val_loss: 0.0689 - val_accuracy: 0.9831\n",
      "Epoch 371/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0551 - accuracy: 0.9860\n",
      "Epoch 371: saving model to ./model/all3\\371-0.9800.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0384 - accuracy: 0.9892 - val_loss: 0.0713 - val_accuracy: 0.9800\n",
      "Epoch 372/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0622 - accuracy: 0.9780\n",
      "Epoch 372: saving model to ./model/all3\\372-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0384 - accuracy: 0.9908 - val_loss: 0.0628 - val_accuracy: 0.9846\n",
      "Epoch 373/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0572 - accuracy: 0.9880\n",
      "Epoch 373: saving model to ./model/all3\\373-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0367 - accuracy: 0.9908 - val_loss: 0.0658 - val_accuracy: 0.9831\n",
      "Epoch 374/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0427 - accuracy: 0.9880\n",
      "Epoch 374: saving model to ./model/all3\\374-0.9808.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0359 - accuracy: 0.9910 - val_loss: 0.0662 - val_accuracy: 0.9808\n",
      "Epoch 375/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0504 - accuracy: 0.9900\n",
      "Epoch 375: saving model to ./model/all3\\375-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0379 - accuracy: 0.9908 - val_loss: 0.0659 - val_accuracy: 0.9831\n",
      "Epoch 376/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0291 - accuracy: 0.9920\n",
      "Epoch 376: saving model to ./model/all3\\376-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9910 - val_loss: 0.0646 - val_accuracy: 0.9831\n",
      "Epoch 377/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9900\n",
      "Epoch 377: saving model to ./model/all3\\377-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0367 - accuracy: 0.9920 - val_loss: 0.0625 - val_accuracy: 0.9846\n",
      "Epoch 378/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0276 - accuracy: 0.9960\n",
      "Epoch 378: saving model to ./model/all3\\378-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9910 - val_loss: 0.0604 - val_accuracy: 0.9854\n",
      "Epoch 379/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0175 - accuracy: 0.9960\n",
      "Epoch 379: saving model to ./model/all3\\379-0.9838.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0368 - accuracy: 0.9915 - val_loss: 0.0605 - val_accuracy: 0.9838\n",
      "Epoch 380/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0393 - accuracy: 0.9940\n",
      "Epoch 380: saving model to ./model/all3\\380-0.9823.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0389 - accuracy: 0.9902 - val_loss: 0.0640 - val_accuracy: 0.9823\n",
      "Epoch 381/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0287 - accuracy: 0.9900\n",
      "Epoch 381: saving model to ./model/all3\\381-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0377 - accuracy: 0.9910 - val_loss: 0.0600 - val_accuracy: 0.9869\n",
      "Epoch 382/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0366 - accuracy: 0.9920\n",
      "Epoch 382: saving model to ./model/all3\\382-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9915 - val_loss: 0.0595 - val_accuracy: 0.9862\n",
      "Epoch 383/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9940\n",
      "Epoch 383: saving model to ./model/all3\\383-0.9846.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0382 - accuracy: 0.9905 - val_loss: 0.0607 - val_accuracy: 0.9846\n",
      "Epoch 384/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9920\n",
      "Epoch 384: saving model to ./model/all3\\384-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0379 - accuracy: 0.9902 - val_loss: 0.0587 - val_accuracy: 0.9862\n",
      "Epoch 385/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0273 - accuracy: 0.9940\n",
      "Epoch 385: saving model to ./model/all3\\385-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0357 - accuracy: 0.9913 - val_loss: 0.0605 - val_accuracy: 0.9854\n",
      "Epoch 386/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9920\n",
      "Epoch 386: saving model to ./model/all3\\386-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.9913 - val_loss: 0.0602 - val_accuracy: 0.9862\n",
      "Epoch 387/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0340 - accuracy: 0.9920\n",
      "Epoch 387: saving model to ./model/all3\\387-0.9823.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0367 - accuracy: 0.9900 - val_loss: 0.0622 - val_accuracy: 0.9823\n",
      "Epoch 388/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0154 - accuracy: 0.9980\n",
      "Epoch 388: saving model to ./model/all3\\388-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0442 - accuracy: 0.9874 - val_loss: 0.0640 - val_accuracy: 0.9831\n",
      "Epoch 389/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0348 - accuracy: 0.9880\n",
      "Epoch 389: saving model to ./model/all3\\389-0.9831.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.0611 - val_accuracy: 0.9831\n",
      "Epoch 390/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0287 - accuracy: 0.9920\n",
      "Epoch 390: saving model to ./model/all3\\390-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0369 - accuracy: 0.9915 - val_loss: 0.0586 - val_accuracy: 0.9862\n",
      "Epoch 391/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0433 - accuracy: 0.9900\n",
      "Epoch 391: saving model to ./model/all3\\391-0.9838.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0353 - accuracy: 0.9913 - val_loss: 0.0629 - val_accuracy: 0.9838\n",
      "Epoch 392/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0416 - accuracy: 0.9920\n",
      "Epoch 392: saving model to ./model/all3\\392-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0380 - accuracy: 0.9915 - val_loss: 0.0605 - val_accuracy: 0.9838\n",
      "Epoch 393/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0179 - accuracy: 0.9980\n",
      "Epoch 393: saving model to ./model/all3\\393-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0364 - accuracy: 0.9908 - val_loss: 0.0596 - val_accuracy: 0.9862\n",
      "Epoch 394/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0431 - accuracy: 0.9900\n",
      "Epoch 394: saving model to ./model/all3\\394-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 0.9915 - val_loss: 0.0638 - val_accuracy: 0.9838\n",
      "Epoch 395/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0382 - accuracy: 0.9940\n",
      "Epoch 395: saving model to ./model/all3\\395-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0364 - accuracy: 0.9918 - val_loss: 0.0650 - val_accuracy: 0.9838\n",
      "Epoch 396/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0205 - accuracy: 0.9940\n",
      "Epoch 396: saving model to ./model/all3\\396-0.9838.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0351 - accuracy: 0.9920 - val_loss: 0.0638 - val_accuracy: 0.9838\n",
      "Epoch 397/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9860\n",
      "Epoch 397: saving model to ./model/all3\\397-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0355 - accuracy: 0.9920 - val_loss: 0.0625 - val_accuracy: 0.9823\n",
      "Epoch 398/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0350 - accuracy: 0.9880\n",
      "Epoch 398: saving model to ./model/all3\\398-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0386 - accuracy: 0.9890 - val_loss: 0.0609 - val_accuracy: 0.9854\n",
      "Epoch 399/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0381 - accuracy: 0.9920\n",
      "Epoch 399: saving model to ./model/all3\\399-0.9869.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0380 - accuracy: 0.9910 - val_loss: 0.0595 - val_accuracy: 0.9869\n",
      "Epoch 400/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0110 - accuracy: 0.9980\n",
      "Epoch 400: saving model to ./model/all3\\400-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0366 - accuracy: 0.9905 - val_loss: 0.0584 - val_accuracy: 0.9862\n",
      "Epoch 401/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0268 - accuracy: 0.9960\n",
      "Epoch 401: saving model to ./model/all3\\401-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0364 - accuracy: 0.9918 - val_loss: 0.0577 - val_accuracy: 0.9862\n",
      "Epoch 402/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0261 - accuracy: 0.9960\n",
      "Epoch 402: saving model to ./model/all3\\402-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0345 - accuracy: 0.9918 - val_loss: 0.0589 - val_accuracy: 0.9862\n",
      "Epoch 403/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0394 - accuracy: 0.9920\n",
      "Epoch 403: saving model to ./model/all3\\403-0.9846.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0345 - accuracy: 0.9915 - val_loss: 0.0609 - val_accuracy: 0.9846\n",
      "Epoch 404/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0190 - accuracy: 0.9940\n",
      "Epoch 404: saving model to ./model/all3\\404-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0346 - accuracy: 0.9920 - val_loss: 0.0592 - val_accuracy: 0.9862\n",
      "Epoch 405/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0298 - accuracy: 0.9920\n",
      "Epoch 405: saving model to ./model/all3\\405-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0356 - accuracy: 0.9915 - val_loss: 0.0630 - val_accuracy: 0.9838\n",
      "Epoch 406/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0501 - accuracy: 0.9860\n",
      "Epoch 406: saving model to ./model/all3\\406-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0381 - accuracy: 0.9900 - val_loss: 0.0603 - val_accuracy: 0.9854\n",
      "Epoch 407/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0238 - accuracy: 0.9940\n",
      "Epoch 407: saving model to ./model/all3\\407-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0406 - accuracy: 0.9895 - val_loss: 0.0588 - val_accuracy: 0.9862\n",
      "Epoch 408/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9880\n",
      "Epoch 408: saving model to ./model/all3\\408-0.9885.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0413 - accuracy: 0.9885 - val_loss: 0.0589 - val_accuracy: 0.9885\n",
      "Epoch 409/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0496 - accuracy: 0.9920\n",
      "Epoch 409: saving model to ./model/all3\\409-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0381 - accuracy: 0.9908 - val_loss: 0.0595 - val_accuracy: 0.9862\n",
      "Epoch 410/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 410: saving model to ./model/all3\\410-0.9846.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9923 - val_loss: 0.0595 - val_accuracy: 0.9846\n",
      "Epoch 411/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0582 - accuracy: 0.9860\n",
      "Epoch 411: saving model to ./model/all3\\411-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0358 - accuracy: 0.9913 - val_loss: 0.0583 - val_accuracy: 0.9862\n",
      "Epoch 412/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0317 - accuracy: 0.9900\n",
      "Epoch 412: saving model to ./model/all3\\412-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0343 - accuracy: 0.9923 - val_loss: 0.0590 - val_accuracy: 0.9862\n",
      "Epoch 413/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0336 - accuracy: 0.9940\n",
      "Epoch 413: saving model to ./model/all3\\413-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0354 - accuracy: 0.9918 - val_loss: 0.0579 - val_accuracy: 0.9869\n",
      "Epoch 414/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9900\n",
      "Epoch 414: saving model to ./model/all3\\414-0.9862.keras\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0352 - accuracy: 0.9915 - val_loss: 0.0574 - val_accuracy: 0.9862\n",
      "Epoch 415/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0437 - accuracy: 0.9900\n",
      "Epoch 415: saving model to ./model/all3\\415-0.9869.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0347 - accuracy: 0.9918 - val_loss: 0.0582 - val_accuracy: 0.9869\n",
      "Epoch 416/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0145 - accuracy: 0.9980\n",
      "Epoch 416: saving model to ./model/all3\\416-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0340 - accuracy: 0.9931 - val_loss: 0.0589 - val_accuracy: 0.9862\n",
      "Epoch 417/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0553 - accuracy: 0.9880\n",
      "Epoch 417: saving model to ./model/all3\\417-0.9862.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0354 - accuracy: 0.9931 - val_loss: 0.0587 - val_accuracy: 0.9862\n",
      "Epoch 418/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9980\n",
      "Epoch 418: saving model to ./model/all3\\418-0.9869.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0376 - accuracy: 0.9908 - val_loss: 0.0574 - val_accuracy: 0.9869\n",
      "Epoch 419/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0387 - accuracy: 0.9920\n",
      "Epoch 419: saving model to ./model/all3\\419-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0369 - accuracy: 0.9913 - val_loss: 0.0576 - val_accuracy: 0.9869\n",
      "Epoch 420/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9920\n",
      "Epoch 420: saving model to ./model/all3\\420-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0344 - accuracy: 0.9926 - val_loss: 0.0587 - val_accuracy: 0.9862\n",
      "Epoch 421/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0533 - accuracy: 0.9900\n",
      "Epoch 421: saving model to ./model/all3\\421-0.9869.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0350 - accuracy: 0.9913 - val_loss: 0.0582 - val_accuracy: 0.9869\n",
      "Epoch 422/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0296 - accuracy: 0.9940\n",
      "Epoch 422: saving model to ./model/all3\\422-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0346 - accuracy: 0.9923 - val_loss: 0.0579 - val_accuracy: 0.9862\n",
      "Epoch 423/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0218 - accuracy: 0.9960\n",
      "Epoch 423: saving model to ./model/all3\\423-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0341 - accuracy: 0.9933 - val_loss: 0.0626 - val_accuracy: 0.9846\n",
      "Epoch 424/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0147 - accuracy: 0.9980\n",
      "Epoch 424: saving model to ./model/all3\\424-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0343 - accuracy: 0.9926 - val_loss: 0.0573 - val_accuracy: 0.9862\n",
      "Epoch 425/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0468 - accuracy: 0.9900\n",
      "Epoch 425: saving model to ./model/all3\\425-0.9862.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9913 - val_loss: 0.0595 - val_accuracy: 0.9862\n",
      "Epoch 426/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0366 - accuracy: 0.9880\n",
      "Epoch 426: saving model to ./model/all3\\426-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0374 - accuracy: 0.9900 - val_loss: 0.0615 - val_accuracy: 0.9846\n",
      "Epoch 427/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0294 - accuracy: 0.9940\n",
      "Epoch 427: saving model to ./model/all3\\427-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0348 - accuracy: 0.9918 - val_loss: 0.0587 - val_accuracy: 0.9854\n",
      "Epoch 428/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0416 - accuracy: 0.9940\n",
      "Epoch 428: saving model to ./model/all3\\428-0.9831.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 0.9915 - val_loss: 0.0661 - val_accuracy: 0.9831\n",
      "Epoch 429/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9860\n",
      "Epoch 429: saving model to ./model/all3\\429-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9918 - val_loss: 0.0691 - val_accuracy: 0.9800\n",
      "Epoch 430/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0475 - accuracy: 0.9900\n",
      "Epoch 430: saving model to ./model/all3\\430-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 0.9892 - val_loss: 0.0690 - val_accuracy: 0.9800\n",
      "Epoch 431/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9880\n",
      "Epoch 431: saving model to ./model/all3\\431-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0376 - accuracy: 0.9918 - val_loss: 0.0612 - val_accuracy: 0.9854\n",
      "Epoch 432/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0320 - accuracy: 0.9960\n",
      "Epoch 432: saving model to ./model/all3\\432-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0344 - accuracy: 0.9920 - val_loss: 0.0618 - val_accuracy: 0.9846\n",
      "Epoch 433/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0779 - accuracy: 0.9800\n",
      "Epoch 433: saving model to ./model/all3\\433-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0352 - accuracy: 0.9915 - val_loss: 0.0625 - val_accuracy: 0.9823\n",
      "Epoch 434/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0124 - accuracy: 0.9960\n",
      "Epoch 434: saving model to ./model/all3\\434-0.9823.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9915 - val_loss: 0.0638 - val_accuracy: 0.9823\n",
      "Epoch 435/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0261 - accuracy: 0.9920\n",
      "Epoch 435: saving model to ./model/all3\\435-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0359 - accuracy: 0.9910 - val_loss: 0.0628 - val_accuracy: 0.9831\n",
      "Epoch 436/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0164 - accuracy: 0.9960\n",
      "Epoch 436: saving model to ./model/all3\\436-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0344 - accuracy: 0.9928 - val_loss: 0.0604 - val_accuracy: 0.9854\n",
      "Epoch 437/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0203 - accuracy: 0.9980\n",
      "Epoch 437: saving model to ./model/all3\\437-0.9862.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0343 - accuracy: 0.9920 - val_loss: 0.0582 - val_accuracy: 0.9862\n",
      "Epoch 438/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0204 - accuracy: 0.9960\n",
      "Epoch 438: saving model to ./model/all3\\438-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0351 - accuracy: 0.9923 - val_loss: 0.0579 - val_accuracy: 0.9869\n",
      "Epoch 439/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0394 - accuracy: 0.9900\n",
      "Epoch 439: saving model to ./model/all3\\439-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0341 - accuracy: 0.9915 - val_loss: 0.0589 - val_accuracy: 0.9854\n",
      "Epoch 440/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0457 - accuracy: 0.9900\n",
      "Epoch 440: saving model to ./model/all3\\440-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0338 - accuracy: 0.9928 - val_loss: 0.0598 - val_accuracy: 0.9862\n",
      "Epoch 441/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0550 - accuracy: 0.9820\n",
      "Epoch 441: saving model to ./model/all3\\441-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0344 - accuracy: 0.9926 - val_loss: 0.0703 - val_accuracy: 0.9800\n",
      "Epoch 442/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0453 - accuracy: 0.9880\n",
      "Epoch 442: saving model to ./model/all3\\442-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0356 - accuracy: 0.9913 - val_loss: 0.0592 - val_accuracy: 0.9869\n",
      "Epoch 443/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0526 - accuracy: 0.9880\n",
      "Epoch 443: saving model to ./model/all3\\443-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0341 - accuracy: 0.9918 - val_loss: 0.0584 - val_accuracy: 0.9869\n",
      "Epoch 444/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0482 - accuracy: 0.9940\n",
      "Epoch 444: saving model to ./model/all3\\444-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0330 - accuracy: 0.9931 - val_loss: 0.0580 - val_accuracy: 0.9862\n",
      "Epoch 445/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0271 - accuracy: 0.9960\n",
      "Epoch 445: saving model to ./model/all3\\445-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0357 - accuracy: 0.9926 - val_loss: 0.0589 - val_accuracy: 0.9838\n",
      "Epoch 446/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0316 - accuracy: 0.9940\n",
      "Epoch 446: saving model to ./model/all3\\446-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0370 - accuracy: 0.9915 - val_loss: 0.0587 - val_accuracy: 0.9862\n",
      "Epoch 447/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0468 - accuracy: 0.9920\n",
      "Epoch 447: saving model to ./model/all3\\447-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0338 - accuracy: 0.9913 - val_loss: 0.0599 - val_accuracy: 0.9838\n",
      "Epoch 448/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0353 - accuracy: 0.9920\n",
      "Epoch 448: saving model to ./model/all3\\448-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0387 - accuracy: 0.9897 - val_loss: 0.0581 - val_accuracy: 0.9862\n",
      "Epoch 449/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0291 - accuracy: 0.9940\n",
      "Epoch 449: saving model to ./model/all3\\449-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0333 - accuracy: 0.9920 - val_loss: 0.0565 - val_accuracy: 0.9869\n",
      "Epoch 450/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0268 - accuracy: 0.9940\n",
      "Epoch 450: saving model to ./model/all3\\450-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0343 - accuracy: 0.9923 - val_loss: 0.0616 - val_accuracy: 0.9838\n",
      "Epoch 451/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0150 - accuracy: 0.9940\n",
      "Epoch 451: saving model to ./model/all3\\451-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0336 - accuracy: 0.9918 - val_loss: 0.0588 - val_accuracy: 0.9869\n",
      "Epoch 452/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0168 - accuracy: 0.9960\n",
      "Epoch 452: saving model to ./model/all3\\452-0.9862.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 0.9928 - val_loss: 0.0589 - val_accuracy: 0.9862\n",
      "Epoch 453/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0301 - accuracy: 0.9960\n",
      "Epoch 453: saving model to ./model/all3\\453-0.9846.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0342 - accuracy: 0.9926 - val_loss: 0.0595 - val_accuracy: 0.9846\n",
      "Epoch 454/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0630 - accuracy: 0.9900\n",
      "Epoch 454: saving model to ./model/all3\\454-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0350 - accuracy: 0.9923 - val_loss: 0.0570 - val_accuracy: 0.9869\n",
      "Epoch 455/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0499 - accuracy: 0.9880\n",
      "Epoch 455: saving model to ./model/all3\\455-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0351 - accuracy: 0.9918 - val_loss: 0.0591 - val_accuracy: 0.9862\n",
      "Epoch 456/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0184 - accuracy: 0.9960\n",
      "Epoch 456: saving model to ./model/all3\\456-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.9923 - val_loss: 0.0596 - val_accuracy: 0.9854\n",
      "Epoch 457/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0250 - accuracy: 0.9960\n",
      "Epoch 457: saving model to ./model/all3\\457-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0391 - accuracy: 0.9897 - val_loss: 0.0585 - val_accuracy: 0.9869\n",
      "Epoch 458/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0304 - accuracy: 0.9940\n",
      "Epoch 458: saving model to ./model/all3\\458-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0335 - accuracy: 0.9926 - val_loss: 0.0570 - val_accuracy: 0.9854\n",
      "Epoch 459/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0189 - accuracy: 0.9940\n",
      "Epoch 459: saving model to ./model/all3\\459-0.9862.keras\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.0346 - accuracy: 0.9928 - val_loss: 0.0571 - val_accuracy: 0.9862\n",
      "Epoch 460/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0253 - accuracy: 0.9940\n",
      "Epoch 460: saving model to ./model/all3\\460-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0342 - accuracy: 0.9923 - val_loss: 0.0604 - val_accuracy: 0.9831\n",
      "Epoch 461/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0379 - accuracy: 0.9920\n",
      "Epoch 461: saving model to ./model/all3\\461-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0348 - accuracy: 0.9931 - val_loss: 0.0589 - val_accuracy: 0.9854\n",
      "Epoch 462/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0270 - accuracy: 0.9940\n",
      "Epoch 462: saving model to ./model/all3\\462-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9915 - val_loss: 0.0600 - val_accuracy: 0.9838\n",
      "Epoch 463/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0272 - accuracy: 0.9980\n",
      "Epoch 463: saving model to ./model/all3\\463-0.9800.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0359 - accuracy: 0.9923 - val_loss: 0.0715 - val_accuracy: 0.9800\n",
      "Epoch 464/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0634 - accuracy: 0.9860\n",
      "Epoch 464: saving model to ./model/all3\\464-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0428 - accuracy: 0.9882 - val_loss: 0.0580 - val_accuracy: 0.9846\n",
      "Epoch 465/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0351 - accuracy: 0.9900\n",
      "Epoch 465: saving model to ./model/all3\\465-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0427 - accuracy: 0.9877 - val_loss: 0.0574 - val_accuracy: 0.9869\n",
      "Epoch 466/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0480 - accuracy: 0.9880\n",
      "Epoch 466: saving model to ./model/all3\\466-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0357 - accuracy: 0.9902 - val_loss: 0.0581 - val_accuracy: 0.9854\n",
      "Epoch 467/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0335 - accuracy: 0.9900\n",
      "Epoch 467: saving model to ./model/all3\\467-0.9869.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.9913 - val_loss: 0.0583 - val_accuracy: 0.9869\n",
      "Epoch 468/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0133 - accuracy: 0.9960\n",
      "Epoch 468: saving model to ./model/all3\\468-0.9838.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0352 - accuracy: 0.9913 - val_loss: 0.0634 - val_accuracy: 0.9838\n",
      "Epoch 469/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0440 - accuracy: 0.9840\n",
      "Epoch 469: saving model to ./model/all3\\469-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0337 - accuracy: 0.9915 - val_loss: 0.0586 - val_accuracy: 0.9862\n",
      "Epoch 470/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0341 - accuracy: 0.9920\n",
      "Epoch 470: saving model to ./model/all3\\470-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0333 - accuracy: 0.9923 - val_loss: 0.0574 - val_accuracy: 0.9862\n",
      "Epoch 471/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0149 - accuracy: 0.9980\n",
      "Epoch 471: saving model to ./model/all3\\471-0.9815.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0386 - accuracy: 0.9910 - val_loss: 0.0628 - val_accuracy: 0.9815\n",
      "Epoch 472/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0420 - accuracy: 0.9940\n",
      "Epoch 472: saving model to ./model/all3\\472-0.9846.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0340 - accuracy: 0.9931 - val_loss: 0.0602 - val_accuracy: 0.9846\n",
      "Epoch 473/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0696 - accuracy: 0.9800\n",
      "Epoch 473: saving model to ./model/all3\\473-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0331 - accuracy: 0.9923 - val_loss: 0.0577 - val_accuracy: 0.9869\n",
      "Epoch 474/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0558 - accuracy: 0.9900\n",
      "Epoch 474: saving model to ./model/all3\\474-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0328 - accuracy: 0.9920 - val_loss: 0.0579 - val_accuracy: 0.9869\n",
      "Epoch 475/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0298 - accuracy: 0.9920\n",
      "Epoch 475: saving model to ./model/all3\\475-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0331 - accuracy: 0.9928 - val_loss: 0.0569 - val_accuracy: 0.9862\n",
      "Epoch 476/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0418 - accuracy: 0.9900\n",
      "Epoch 476: saving model to ./model/all3\\476-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0350 - accuracy: 0.9910 - val_loss: 0.0571 - val_accuracy: 0.9862\n",
      "Epoch 477/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9900\n",
      "Epoch 477: saving model to ./model/all3\\477-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0341 - accuracy: 0.9923 - val_loss: 0.0567 - val_accuracy: 0.9862\n",
      "Epoch 478/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0729 - accuracy: 0.9860\n",
      "Epoch 478: saving model to ./model/all3\\478-0.9838.keras\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.0345 - accuracy: 0.9908 - val_loss: 0.0626 - val_accuracy: 0.9838\n",
      "Epoch 479/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0395 - accuracy: 0.9840\n",
      "Epoch 479: saving model to ./model/all3\\479-0.9838.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0337 - accuracy: 0.9913 - val_loss: 0.0622 - val_accuracy: 0.9838\n",
      "Epoch 480/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0399 - accuracy: 0.9900\n",
      "Epoch 480: saving model to ./model/all3\\480-0.9808.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0358 - accuracy: 0.9915 - val_loss: 0.0673 - val_accuracy: 0.9808\n",
      "Epoch 481/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0142 - accuracy: 0.9960\n",
      "Epoch 481: saving model to ./model/all3\\481-0.9846.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0336 - accuracy: 0.9918 - val_loss: 0.0610 - val_accuracy: 0.9846\n",
      "Epoch 482/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0661 - accuracy: 0.9860\n",
      "Epoch 482: saving model to ./model/all3\\482-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0334 - accuracy: 0.9920 - val_loss: 0.0579 - val_accuracy: 0.9862\n",
      "Epoch 483/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0292 - accuracy: 0.9920\n",
      "Epoch 483: saving model to ./model/all3\\483-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0325 - accuracy: 0.9933 - val_loss: 0.0565 - val_accuracy: 0.9869\n",
      "Epoch 484/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0361 - accuracy: 0.9900\n",
      "Epoch 484: saving model to ./model/all3\\484-0.9877.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0335 - accuracy: 0.9926 - val_loss: 0.0565 - val_accuracy: 0.9877\n",
      "Epoch 485/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0429 - accuracy: 0.9860\n",
      "Epoch 485: saving model to ./model/all3\\485-0.9869.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0351 - accuracy: 0.9910 - val_loss: 0.0569 - val_accuracy: 0.9869\n",
      "Epoch 486/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0287 - accuracy: 0.9960\n",
      "Epoch 486: saving model to ./model/all3\\486-0.9862.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0336 - accuracy: 0.9915 - val_loss: 0.0577 - val_accuracy: 0.9862\n",
      "Epoch 487/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0396 - accuracy: 0.9900\n",
      "Epoch 487: saving model to ./model/all3\\487-0.9869.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0328 - accuracy: 0.9926 - val_loss: 0.0594 - val_accuracy: 0.9869\n",
      "Epoch 488/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0169 - accuracy: 0.9940\n",
      "Epoch 488: saving model to ./model/all3\\488-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0341 - accuracy: 0.9915 - val_loss: 0.0573 - val_accuracy: 0.9869\n",
      "Epoch 489/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0359 - accuracy: 0.9940\n",
      "Epoch 489: saving model to ./model/all3\\489-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0372 - accuracy: 0.9887 - val_loss: 0.0570 - val_accuracy: 0.9869\n",
      "Epoch 490/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 490: saving model to ./model/all3\\490-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0387 - accuracy: 0.9890 - val_loss: 0.0567 - val_accuracy: 0.9854\n",
      "Epoch 491/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0259 - accuracy: 0.9920\n",
      "Epoch 491: saving model to ./model/all3\\491-0.9854.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0367 - accuracy: 0.9910 - val_loss: 0.0586 - val_accuracy: 0.9854\n",
      "Epoch 492/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0368 - accuracy: 0.9960\n",
      "Epoch 492: saving model to ./model/all3\\492-0.9854.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0338 - accuracy: 0.9915 - val_loss: 0.0601 - val_accuracy: 0.9854\n",
      "Epoch 493/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0250 - accuracy: 0.9960\n",
      "Epoch 493: saving model to ./model/all3\\493-0.9869.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0344 - accuracy: 0.9915 - val_loss: 0.0571 - val_accuracy: 0.9869\n",
      "Epoch 494/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0507 - accuracy: 0.9900\n",
      "Epoch 494: saving model to ./model/all3\\494-0.9831.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0348 - accuracy: 0.9910 - val_loss: 0.0632 - val_accuracy: 0.9831\n",
      "Epoch 495/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0473 - accuracy: 0.9880\n",
      "Epoch 495: saving model to ./model/all3\\495-0.9800.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0370 - accuracy: 0.9897 - val_loss: 0.0752 - val_accuracy: 0.9800\n",
      "Epoch 496/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0609 - accuracy: 0.9780\n",
      "Epoch 496: saving model to ./model/all3\\496-0.9785.keras\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0430 - accuracy: 0.9872 - val_loss: 0.0843 - val_accuracy: 0.9785\n",
      "Epoch 497/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0195 - accuracy: 0.9900\n",
      "Epoch 497: saving model to ./model/all3\\497-0.9815.keras\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.0469 - accuracy: 0.9861 - val_loss: 0.0654 - val_accuracy: 0.9815\n",
      "Epoch 498/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0251 - accuracy: 0.9920\n",
      "Epoch 498: saving model to ./model/all3\\498-0.9808.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0350 - accuracy: 0.9905 - val_loss: 0.0751 - val_accuracy: 0.9808\n",
      "Epoch 499/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0544 - accuracy: 0.9780\n",
      "Epoch 499: saving model to ./model/all3\\499-0.9815.keras\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0394 - accuracy: 0.9887 - val_loss: 0.0700 - val_accuracy: 0.9815\n",
      "Epoch 500/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0522 - accuracy: 0.9900\n",
      "Epoch 500: saving model to ./model/all3\\500-0.9838.keras\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0355 - accuracy: 0.9918 - val_loss: 0.0663 - val_accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add( Dense(36, \n",
    "                input_dim=12,\n",
    "                activation='relu'))\n",
    "model3.add( Dense(12, activation='relu'))\n",
    "model3.add( Dense(8, activation='relu'))\n",
    "model3.add( Dense(1, activation='sigmoid'))\n",
    "model3.compile( \n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history3 = model3.fit( X_train, y_train,\n",
    "                    # epochs=2000,\n",
    "                    epochs=500,\n",
    "                    batch_size=500, \n",
    "                    validation_split=0.25, \n",
    "                    callbacks=[checkpointer] )\n",
    "# validation_split 확인 필요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x193a975ed90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [3.4045913219451904,\n",
       "  0.41447609663009644,\n",
       "  0.31411924958229065,\n",
       "  0.3351871371269226,\n",
       "  0.321920245885849,\n",
       "  0.2865988612174988,\n",
       "  0.25114718079566956,\n",
       "  0.23516404628753662,\n",
       "  0.2310681790113449,\n",
       "  0.22658517956733704,\n",
       "  0.22295325994491577,\n",
       "  0.22058257460594177,\n",
       "  0.21711036562919617,\n",
       "  0.21412970125675201,\n",
       "  0.21209277212619781,\n",
       "  0.21042592823505402,\n",
       "  0.20883481204509735,\n",
       "  0.2074473649263382,\n",
       "  0.20627866685390472,\n",
       "  0.20540854334831238,\n",
       "  0.20434153079986572,\n",
       "  0.20356839895248413,\n",
       "  0.2025451809167862,\n",
       "  0.2019365280866623,\n",
       "  0.2011028379201889,\n",
       "  0.20032751560211182,\n",
       "  0.19931578636169434,\n",
       "  0.19849376380443573,\n",
       "  0.19785642623901367,\n",
       "  0.1965596228837967,\n",
       "  0.19569995999336243,\n",
       "  0.19417865574359894,\n",
       "  0.19290030002593994,\n",
       "  0.19192294776439667,\n",
       "  0.19146795570850372,\n",
       "  0.19002577662467957,\n",
       "  0.18895895779132843,\n",
       "  0.18772029876708984,\n",
       "  0.186946302652359,\n",
       "  0.1856929361820221,\n",
       "  0.1851791888475418,\n",
       "  0.18392284214496613,\n",
       "  0.18213678896427155,\n",
       "  0.1808900386095047,\n",
       "  0.17869184911251068,\n",
       "  0.1762181967496872,\n",
       "  0.17591536045074463,\n",
       "  0.17372360825538635,\n",
       "  0.16609874367713928,\n",
       "  0.16267749667167664,\n",
       "  0.15953069925308228,\n",
       "  0.15560658276081085,\n",
       "  0.15315130352973938,\n",
       "  0.14787520468235016,\n",
       "  0.14503274857997894,\n",
       "  0.13955530524253845,\n",
       "  0.13323360681533813,\n",
       "  0.12984047830104828,\n",
       "  0.12616176903247833,\n",
       "  0.1218014657497406,\n",
       "  0.11967534571886063,\n",
       "  0.11854375153779984,\n",
       "  0.11463697254657745,\n",
       "  0.1103750616312027,\n",
       "  0.1100757047533989,\n",
       "  0.10560037195682526,\n",
       "  0.10264204442501068,\n",
       "  0.09981387853622437,\n",
       "  0.09727056324481964,\n",
       "  0.0951245129108429,\n",
       "  0.09349417686462402,\n",
       "  0.09384119510650635,\n",
       "  0.09344659745693207,\n",
       "  0.0925709679722786,\n",
       "  0.08982810378074646,\n",
       "  0.08746767789125443,\n",
       "  0.08734102547168732,\n",
       "  0.08701419085264206,\n",
       "  0.0839044526219368,\n",
       "  0.08330358564853668,\n",
       "  0.08165717869997025,\n",
       "  0.08070359379053116,\n",
       "  0.07947023957967758,\n",
       "  0.07912907004356384,\n",
       "  0.07960333675146103,\n",
       "  0.07745684683322906,\n",
       "  0.08134328573942184,\n",
       "  0.0780196264386177,\n",
       "  0.08070946484804153,\n",
       "  0.08311821520328522,\n",
       "  0.08021736890077591,\n",
       "  0.07569565623998642,\n",
       "  0.07484710961580276,\n",
       "  0.0756523609161377,\n",
       "  0.07519697397947311,\n",
       "  0.0758984163403511,\n",
       "  0.07671255618333817,\n",
       "  0.07461146265268326,\n",
       "  0.07141861319541931,\n",
       "  0.0696326196193695,\n",
       "  0.06909022480249405,\n",
       "  0.0687020942568779,\n",
       "  0.07258641719818115,\n",
       "  0.07164856791496277,\n",
       "  0.06871020793914795,\n",
       "  0.06633315235376358,\n",
       "  0.06656234711408615,\n",
       "  0.06534431874752045,\n",
       "  0.06479042768478394,\n",
       "  0.06523158401250839,\n",
       "  0.0692102387547493,\n",
       "  0.07002685219049454,\n",
       "  0.06746484339237213,\n",
       "  0.06834033876657486,\n",
       "  0.06398994475603104,\n",
       "  0.06300096958875656,\n",
       "  0.06278176605701447,\n",
       "  0.06212865188717842,\n",
       "  0.0625750795006752,\n",
       "  0.06521984189748764,\n",
       "  0.06536867469549179,\n",
       "  0.066307432949543,\n",
       "  0.06624128669500351,\n",
       "  0.061865177005529404,\n",
       "  0.06072996184229851,\n",
       "  0.06078016012907028,\n",
       "  0.05949028208851814,\n",
       "  0.05997253209352493,\n",
       "  0.05998080223798752,\n",
       "  0.059466756880283356,\n",
       "  0.06130165979266167,\n",
       "  0.06621813774108887,\n",
       "  0.06206871196627617,\n",
       "  0.05968904495239258,\n",
       "  0.060739513486623764,\n",
       "  0.057932477444410324,\n",
       "  0.05930711328983307,\n",
       "  0.05974885821342468,\n",
       "  0.05937200039625168,\n",
       "  0.05885976180434227,\n",
       "  0.05746651440858841,\n",
       "  0.056351978331804276,\n",
       "  0.057636115700006485,\n",
       "  0.05696176365017891,\n",
       "  0.05625762790441513,\n",
       "  0.05885732173919678,\n",
       "  0.05723792314529419,\n",
       "  0.055999644100666046,\n",
       "  0.056340016424655914,\n",
       "  0.0557132251560688,\n",
       "  0.05523688718676567,\n",
       "  0.055466681718826294,\n",
       "  0.05745493620634079,\n",
       "  0.060594405978918076,\n",
       "  0.05581677332520485,\n",
       "  0.054693810641765594,\n",
       "  0.05506176874041557,\n",
       "  0.056862715631723404,\n",
       "  0.055132050067186356,\n",
       "  0.05508283153176308,\n",
       "  0.053960949182510376,\n",
       "  0.05538676679134369,\n",
       "  0.0577366016805172,\n",
       "  0.05430382490158081,\n",
       "  0.05312064662575722,\n",
       "  0.052788905799388885,\n",
       "  0.05740974843502045,\n",
       "  0.0619887039065361,\n",
       "  0.06020219624042511,\n",
       "  0.05625441297888756,\n",
       "  0.05247710645198822,\n",
       "  0.05325929820537567,\n",
       "  0.05183510109782219,\n",
       "  0.051214948296546936,\n",
       "  0.05249620974063873,\n",
       "  0.05424775183200836,\n",
       "  0.05187276378273964,\n",
       "  0.05195431783795357,\n",
       "  0.05083862319588661,\n",
       "  0.05099556967616081,\n",
       "  0.05119509622454643,\n",
       "  0.05083402991294861,\n",
       "  0.05040919780731201,\n",
       "  0.05185474082827568,\n",
       "  0.05316497012972832,\n",
       "  0.05356927961111069,\n",
       "  0.052072323858737946,\n",
       "  0.049827512353658676,\n",
       "  0.04990050196647644,\n",
       "  0.04962516203522682,\n",
       "  0.04940250515937805,\n",
       "  0.04933159053325653,\n",
       "  0.050406280905008316,\n",
       "  0.05326124653220177,\n",
       "  0.054062824696302414,\n",
       "  0.05976962298154831,\n",
       "  0.06195107474923134,\n",
       "  0.06768818944692612,\n",
       "  0.05371273308992386,\n",
       "  0.04966384917497635,\n",
       "  0.05062491074204445,\n",
       "  0.0495486855506897,\n",
       "  0.054326485842466354,\n",
       "  0.0589340478181839,\n",
       "  0.06035466864705086,\n",
       "  0.04997025802731514,\n",
       "  0.05114990472793579,\n",
       "  0.052076175808906555,\n",
       "  0.053722452372312546,\n",
       "  0.052216485142707825,\n",
       "  0.051302749663591385,\n",
       "  0.050105515867471695,\n",
       "  0.04899388924241066,\n",
       "  0.04757389426231384,\n",
       "  0.048811737447977066,\n",
       "  0.053005632013082504,\n",
       "  0.0551111102104187,\n",
       "  0.04784838855266571,\n",
       "  0.04740320146083832,\n",
       "  0.048331934958696365,\n",
       "  0.04693606123328209,\n",
       "  0.045990239828825,\n",
       "  0.047181013971567154,\n",
       "  0.04696391150355339,\n",
       "  0.046496596187353134,\n",
       "  0.047093894332647324,\n",
       "  0.047328442335128784,\n",
       "  0.046821024268865585,\n",
       "  0.0460473969578743,\n",
       "  0.04621061310172081,\n",
       "  0.047421857714653015,\n",
       "  0.046518586575984955,\n",
       "  0.04736858978867531,\n",
       "  0.048867709934711456,\n",
       "  0.04863804578781128,\n",
       "  0.04661111161112785,\n",
       "  0.048431988805532455,\n",
       "  0.04729350656270981,\n",
       "  0.04801241308450699,\n",
       "  0.04512625187635422,\n",
       "  0.0456877239048481,\n",
       "  0.0447002537548542,\n",
       "  0.04459010064601898,\n",
       "  0.043457385152578354,\n",
       "  0.04569750651717186,\n",
       "  0.04526025056838989,\n",
       "  0.04401234909892082,\n",
       "  0.04474536329507828,\n",
       "  0.046787604689598083,\n",
       "  0.04388629272580147,\n",
       "  0.043461620807647705,\n",
       "  0.04439570754766464,\n",
       "  0.04417908191680908,\n",
       "  0.04552854225039482,\n",
       "  0.043752800673246384,\n",
       "  0.04450990632176399,\n",
       "  0.043260376900434494,\n",
       "  0.04307425022125244,\n",
       "  0.04541832581162453,\n",
       "  0.04367208480834961,\n",
       "  0.04256920516490936,\n",
       "  0.043163951486349106,\n",
       "  0.043779660016298294,\n",
       "  0.04465729370713234,\n",
       "  0.043232277035713196,\n",
       "  0.044285375624895096,\n",
       "  0.043845925480127335,\n",
       "  0.04274924099445343,\n",
       "  0.043864667415618896,\n",
       "  0.042611464858055115,\n",
       "  0.043996840715408325,\n",
       "  0.041968781501054764,\n",
       "  0.04202207550406456,\n",
       "  0.04206163436174393,\n",
       "  0.04407202824950218,\n",
       "  0.04281100258231163,\n",
       "  0.04148976877331734,\n",
       "  0.04221240431070328,\n",
       "  0.04172604903578758,\n",
       "  0.041033800691366196,\n",
       "  0.040884703397750854,\n",
       "  0.04191437363624573,\n",
       "  0.04698682948946953,\n",
       "  0.04711630567908287,\n",
       "  0.04553956165909767,\n",
       "  0.04108023643493652,\n",
       "  0.04007487744092941,\n",
       "  0.04047267138957977,\n",
       "  0.04002219811081886,\n",
       "  0.04045725613832474,\n",
       "  0.043688125908374786,\n",
       "  0.045185867697000504,\n",
       "  0.04041328281164169,\n",
       "  0.04157058522105217,\n",
       "  0.04031948372721672,\n",
       "  0.04135714843869209,\n",
       "  0.041007354855537415,\n",
       "  0.04115421697497368,\n",
       "  0.041751593351364136,\n",
       "  0.04036221653223038,\n",
       "  0.03870077058672905,\n",
       "  0.039477504789829254,\n",
       "  0.03885785862803459,\n",
       "  0.03985406085848808,\n",
       "  0.04314812645316124,\n",
       "  0.043319717049598694,\n",
       "  0.04452548548579216,\n",
       "  0.039173636585474014,\n",
       "  0.043018992990255356,\n",
       "  0.04502089321613312,\n",
       "  0.04437239095568657,\n",
       "  0.04088498279452324,\n",
       "  0.042660657316446304,\n",
       "  0.04389069229364395,\n",
       "  0.03847372531890869,\n",
       "  0.037662770599126816,\n",
       "  0.037865862250328064,\n",
       "  0.03829808905720711,\n",
       "  0.03769354522228241,\n",
       "  0.0381399467587471,\n",
       "  0.0389937199652195,\n",
       "  0.04341442137956619,\n",
       "  0.04098378121852875,\n",
       "  0.04136384278535843,\n",
       "  0.03998817130923271,\n",
       "  0.03799077868461609,\n",
       "  0.03773801028728485,\n",
       "  0.03736626356840134,\n",
       "  0.03845839202404022,\n",
       "  0.037938740104436874,\n",
       "  0.0369197279214859,\n",
       "  0.03652655705809593,\n",
       "  0.03935869410634041,\n",
       "  0.04113081097602844,\n",
       "  0.03915601223707199,\n",
       "  0.03999144211411476,\n",
       "  0.03804570436477661,\n",
       "  0.037203941494226456,\n",
       "  0.037039317190647125,\n",
       "  0.03664223477244377,\n",
       "  0.03649391233921051,\n",
       "  0.042015280574560165,\n",
       "  0.041792720556259155,\n",
       "  0.037647977471351624,\n",
       "  0.037656936794519424,\n",
       "  0.03620420768857002,\n",
       "  0.0385868139564991,\n",
       "  0.03811375051736832,\n",
       "  0.03729588910937309,\n",
       "  0.03631403297185898,\n",
       "  0.036679185926914215,\n",
       "  0.03620009124279022,\n",
       "  0.036629486829042435,\n",
       "  0.03631262853741646,\n",
       "  0.03818834573030472,\n",
       "  0.036257460713386536,\n",
       "  0.03600151836872101,\n",
       "  0.03683723136782646,\n",
       "  0.036274123936891556,\n",
       "  0.03650669753551483,\n",
       "  0.038344305008649826,\n",
       "  0.036854058504104614,\n",
       "  0.03570655733346939,\n",
       "  0.03721492365002632,\n",
       "  0.036746397614479065,\n",
       "  0.037063442170619965,\n",
       "  0.03535286709666252,\n",
       "  0.03729291632771492,\n",
       "  0.0391167588531971,\n",
       "  0.03888076916337013,\n",
       "  0.038384243845939636,\n",
       "  0.03835037350654602,\n",
       "  0.036704570055007935,\n",
       "  0.03590540587902069,\n",
       "  0.03785337507724762,\n",
       "  0.03695204108953476,\n",
       "  0.0367262065410614,\n",
       "  0.03630669787526131,\n",
       "  0.03684160113334656,\n",
       "  0.03885436803102493,\n",
       "  0.03768023103475571,\n",
       "  0.03704169765114784,\n",
       "  0.03815241530537605,\n",
       "  0.03789319470524788,\n",
       "  0.03568088263273239,\n",
       "  0.03617637977004051,\n",
       "  0.03665566444396973,\n",
       "  0.04424738883972168,\n",
       "  0.04099199175834656,\n",
       "  0.03688110411167145,\n",
       "  0.03525808826088905,\n",
       "  0.03804650530219078,\n",
       "  0.03635143116116524,\n",
       "  0.03660891205072403,\n",
       "  0.03641483187675476,\n",
       "  0.03509681671857834,\n",
       "  0.03554210439324379,\n",
       "  0.03864333778619766,\n",
       "  0.038009244948625565,\n",
       "  0.03664303570985794,\n",
       "  0.03640877082943916,\n",
       "  0.0344853475689888,\n",
       "  0.03445727005600929,\n",
       "  0.03464803099632263,\n",
       "  0.03563884273171425,\n",
       "  0.03809968754649162,\n",
       "  0.040647368878126144,\n",
       "  0.041305430233478546,\n",
       "  0.038145337253808975,\n",
       "  0.0348970927298069,\n",
       "  0.035765212029218674,\n",
       "  0.034346096217632294,\n",
       "  0.03537081554532051,\n",
       "  0.03515865281224251,\n",
       "  0.03468666970729828,\n",
       "  0.033970512449741364,\n",
       "  0.03543175011873245,\n",
       "  0.03759172558784485,\n",
       "  0.036917924880981445,\n",
       "  0.034413810819387436,\n",
       "  0.0349857397377491,\n",
       "  0.03458870202302933,\n",
       "  0.03409436345100403,\n",
       "  0.034255340695381165,\n",
       "  0.03640776127576828,\n",
       "  0.037432171404361725,\n",
       "  0.0348081961274147,\n",
       "  0.036757148802280426,\n",
       "  0.0369715616106987,\n",
       "  0.040886908769607544,\n",
       "  0.0376349501311779,\n",
       "  0.03439611569046974,\n",
       "  0.035150960087776184,\n",
       "  0.03703688457608223,\n",
       "  0.03589314967393875,\n",
       "  0.03438902646303177,\n",
       "  0.03433627635240555,\n",
       "  0.03514159098267555,\n",
       "  0.034081023186445236,\n",
       "  0.03382483124732971,\n",
       "  0.03436508774757385,\n",
       "  0.03558427467942238,\n",
       "  0.03407803550362587,\n",
       "  0.03297458589076996,\n",
       "  0.03570500388741493,\n",
       "  0.03698566555976868,\n",
       "  0.03384830057621002,\n",
       "  0.038657061755657196,\n",
       "  0.03333019092679024,\n",
       "  0.03427163511514664,\n",
       "  0.03361494466662407,\n",
       "  0.033213354647159576,\n",
       "  0.03420814871788025,\n",
       "  0.035030584782361984,\n",
       "  0.03506911173462868,\n",
       "  0.036150846630334854,\n",
       "  0.03909601643681526,\n",
       "  0.033460114151239395,\n",
       "  0.03463420271873474,\n",
       "  0.03423994407057762,\n",
       "  0.03480134159326553,\n",
       "  0.03630896657705307,\n",
       "  0.03586515039205551,\n",
       "  0.042783647775650024,\n",
       "  0.04266548156738281,\n",
       "  0.0356697142124176,\n",
       "  0.03686891496181488,\n",
       "  0.03516262024641037,\n",
       "  0.03372922167181969,\n",
       "  0.033306267112493515,\n",
       "  0.038605451583862305,\n",
       "  0.03396110236644745,\n",
       "  0.03307599201798439,\n",
       "  0.03283588960766792,\n",
       "  0.033098187297582626,\n",
       "  0.03499830141663551,\n",
       "  0.03405999019742012,\n",
       "  0.03447629138827324,\n",
       "  0.03368386626243591,\n",
       "  0.0357985682785511,\n",
       "  0.03359018266201019,\n",
       "  0.03344426676630974,\n",
       "  0.032474104315042496,\n",
       "  0.03348162770271301,\n",
       "  0.03505902364850044,\n",
       "  0.0336231105029583,\n",
       "  0.03279714658856392,\n",
       "  0.03413962572813034,\n",
       "  0.03721656650304794,\n",
       "  0.03871535882353783,\n",
       "  0.03672894462943077,\n",
       "  0.0338420495390892,\n",
       "  0.03443355858325958,\n",
       "  0.034772444516420364,\n",
       "  0.03695285692811012,\n",
       "  0.04299740493297577,\n",
       "  0.04688558727502823,\n",
       "  0.03501856327056885,\n",
       "  0.039430372416973114,\n",
       "  0.035491183400154114],\n",
       " 'accuracy': [0.2732871472835541,\n",
       "  0.8060046434402466,\n",
       "  0.8788812160491943,\n",
       "  0.872209370136261,\n",
       "  0.8775981664657593,\n",
       "  0.8891454935073853,\n",
       "  0.9014626741409302,\n",
       "  0.9071080088615417,\n",
       "  0.9166024923324585,\n",
       "  0.9191685914993286,\n",
       "  0.9196817874908447,\n",
       "  0.9201950430870056,\n",
       "  0.9225044846534729,\n",
       "  0.924044132232666,\n",
       "  0.9245573282241821,\n",
       "  0.9260969758033752,\n",
       "  0.9263536334037781,\n",
       "  0.9268668293952942,\n",
       "  0.9263536334037781,\n",
       "  0.9263536334037781,\n",
       "  0.9260969758033752,\n",
       "  0.9260969758033752,\n",
       "  0.9268668293952942,\n",
       "  0.9271234273910522,\n",
       "  0.9281498789787292,\n",
       "  0.9299461245536804,\n",
       "  0.9302027225494385,\n",
       "  0.9317423701286316,\n",
       "  0.9314857721328735,\n",
       "  0.9322555661201477,\n",
       "  0.9332820177078247,\n",
       "  0.9327688217163086,\n",
       "  0.9330254197120667,\n",
       "  0.9332820177078247,\n",
       "  0.9353348612785339,\n",
       "  0.9353348612785339,\n",
       "  0.9343084692955017,\n",
       "  0.9361047148704529,\n",
       "  0.936874508857727,\n",
       "  0.936874508857727,\n",
       "  0.9363613128662109,\n",
       "  0.9371311068534851,\n",
       "  0.937644362449646,\n",
       "  0.9381575584411621,\n",
       "  0.937900960445404,\n",
       "  0.9396972060203552,\n",
       "  0.9402104020118713,\n",
       "  0.9417500495910645,\n",
       "  0.9430330991744995,\n",
       "  0.9453426003456116,\n",
       "  0.9448293447494507,\n",
       "  0.9473954439163208,\n",
       "  0.9489350914955139,\n",
       "  0.948421835899353,\n",
       "  0.9497048854827881,\n",
       "  0.9556068778038025,\n",
       "  0.9545804262161255,\n",
       "  0.9574031233787537,\n",
       "  0.9594559669494629,\n",
       "  0.9622786641120911,\n",
       "  0.9612522721290588,\n",
       "  0.9643315076828003,\n",
       "  0.9615088701248169,\n",
       "  0.9635617136955261,\n",
       "  0.9648447632789612,\n",
       "  0.9648447632789612,\n",
       "  0.9661278128623962,\n",
       "  0.9684372544288635,\n",
       "  0.9717731475830078,\n",
       "  0.9717731475830078,\n",
       "  0.9727995991706848,\n",
       "  0.9699769020080566,\n",
       "  0.9740826487541199,\n",
       "  0.9715165495872498,\n",
       "  0.974595844745636,\n",
       "  0.9751090407371521,\n",
       "  0.9766486883163452,\n",
       "  0.9743392467498779,\n",
       "  0.9787015914916992,\n",
       "  0.975622296333313,\n",
       "  0.9779317378997803,\n",
       "  0.9774185419082642,\n",
       "  0.9802412390708923,\n",
       "  0.9802412390708923,\n",
       "  0.9776751399040222,\n",
       "  0.9792147874832153,\n",
       "  0.9766486883163452,\n",
       "  0.9787015914916992,\n",
       "  0.9761354923248291,\n",
       "  0.9740826487541199,\n",
       "  0.9766486883163452,\n",
       "  0.9804978370666504,\n",
       "  0.9810110330581665,\n",
       "  0.9776751399040222,\n",
       "  0.9794713854789734,\n",
       "  0.9766486883163452,\n",
       "  0.9776751399040222,\n",
       "  0.9784449338912964,\n",
       "  0.9812676310539246,\n",
       "  0.9810110330581665,\n",
       "  0.9835771322250366,\n",
       "  0.9822940826416016,\n",
       "  0.9815242290496826,\n",
       "  0.9776751399040222,\n",
       "  0.9799845814704895,\n",
       "  0.9822940826416016,\n",
       "  0.9833204746246338,\n",
       "  0.9828072786331177,\n",
       "  0.9817808866500854,\n",
       "  0.9840903282165527,\n",
       "  0.9797279834747314,\n",
       "  0.9789581894874573,\n",
       "  0.9822940826416016,\n",
       "  0.9807544350624084,\n",
       "  0.9825506806373596,\n",
       "  0.9838337302207947,\n",
       "  0.9835771322250366,\n",
       "  0.9846035242080688,\n",
       "  0.9838337302207947,\n",
       "  0.9815242290496826,\n",
       "  0.9822940826416016,\n",
       "  0.9812676310539246,\n",
       "  0.9820374846458435,\n",
       "  0.9830638766288757,\n",
       "  0.9851167798042297,\n",
       "  0.9846035242080688,\n",
       "  0.9838337302207947,\n",
       "  0.9846035242080688,\n",
       "  0.9843469262123108,\n",
       "  0.9835771322250366,\n",
       "  0.9833204746246338,\n",
       "  0.9810110330581665,\n",
       "  0.9825506806373596,\n",
       "  0.9848601222038269,\n",
       "  0.9825506806373596,\n",
       "  0.9833204746246338,\n",
       "  0.9840903282165527,\n",
       "  0.9833204746246338,\n",
       "  0.9833204746246338,\n",
       "  0.9833204746246338,\n",
       "  0.9848601222038269,\n",
       "  0.9838337302207947,\n",
       "  0.9851167798042297,\n",
       "  0.9843469262123108,\n",
       "  0.9851167798042297,\n",
       "  0.9843469262123108,\n",
       "  0.9851167798042297,\n",
       "  0.9851167798042297,\n",
       "  0.9856299757957458,\n",
       "  0.9848601222038269,\n",
       "  0.9851167798042297,\n",
       "  0.9851167798042297,\n",
       "  0.9856299757957458,\n",
       "  0.9828072786331177,\n",
       "  0.9851167798042297,\n",
       "  0.9858865737915039,\n",
       "  0.9856299757957458,\n",
       "  0.9858865737915039,\n",
       "  0.98639976978302,\n",
       "  0.9846035242080688,\n",
       "  0.9848601222038269,\n",
       "  0.98639976978302,\n",
       "  0.9856299757957458,\n",
       "  0.98639976978302,\n",
       "  0.9866564273834229,\n",
       "  0.9869130253791809,\n",
       "  0.9830638766288757,\n",
       "  0.9812676310539246,\n",
       "  0.9822940826416016,\n",
       "  0.9853733777999878,\n",
       "  0.987169623374939,\n",
       "  0.9856299757957458,\n",
       "  0.9866564273834229,\n",
       "  0.9866564273834229,\n",
       "  0.9869130253791809,\n",
       "  0.9853733777999878,\n",
       "  0.987426221370697,\n",
       "  0.9856299757957458,\n",
       "  0.9869130253791809,\n",
       "  0.9869130253791809,\n",
       "  0.9858865737915039,\n",
       "  0.9869130253791809,\n",
       "  0.98639976978302,\n",
       "  0.9858865737915039,\n",
       "  0.987169623374939,\n",
       "  0.9853733777999878,\n",
       "  0.986143171787262,\n",
       "  0.987426221370697,\n",
       "  0.988196074962616,\n",
       "  0.988196074962616,\n",
       "  0.987426221370697,\n",
       "  0.9876828193664551,\n",
       "  0.9879394173622131,\n",
       "  0.9856299757957458,\n",
       "  0.9856299757957458,\n",
       "  0.9840903282165527,\n",
       "  0.9817808866500854,\n",
       "  0.9804978370666504,\n",
       "  0.9858865737915039,\n",
       "  0.987169623374939,\n",
       "  0.987169623374939,\n",
       "  0.987169623374939,\n",
       "  0.9846035242080688,\n",
       "  0.9833204746246338,\n",
       "  0.9822940826416016,\n",
       "  0.9866564273834229,\n",
       "  0.98639976978302,\n",
       "  0.9869130253791809,\n",
       "  0.9858865737915039,\n",
       "  0.9843469262123108,\n",
       "  0.9866564273834229,\n",
       "  0.9887092709541321,\n",
       "  0.9887092709541321,\n",
       "  0.988452672958374,\n",
       "  0.9889658689498901,\n",
       "  0.9856299757957458,\n",
       "  0.9866564273834229,\n",
       "  0.988196074962616,\n",
       "  0.988196074962616,\n",
       "  0.988452672958374,\n",
       "  0.988452672958374,\n",
       "  0.9876828193664551,\n",
       "  0.988196074962616,\n",
       "  0.988452672958374,\n",
       "  0.988452672958374,\n",
       "  0.9876828193664551,\n",
       "  0.9869130253791809,\n",
       "  0.988452672958374,\n",
       "  0.9879394173622131,\n",
       "  0.988452672958374,\n",
       "  0.9876828193664551,\n",
       "  0.9887092709541321,\n",
       "  0.988452672958374,\n",
       "  0.9892224669456482,\n",
       "  0.987169623374939,\n",
       "  0.987426221370697,\n",
       "  0.9876828193664551,\n",
       "  0.9879394173622131,\n",
       "  0.988196074962616,\n",
       "  0.9879394173622131,\n",
       "  0.9892224669456482,\n",
       "  0.9887092709541321,\n",
       "  0.9889658689498901,\n",
       "  0.9899923205375671,\n",
       "  0.9876828193664551,\n",
       "  0.9887092709541321,\n",
       "  0.9897357225418091,\n",
       "  0.9887092709541321,\n",
       "  0.9889658689498901,\n",
       "  0.9897357225418091,\n",
       "  0.9889658689498901,\n",
       "  0.9889658689498901,\n",
       "  0.9892224669456482,\n",
       "  0.988452672958374,\n",
       "  0.9897357225418091,\n",
       "  0.988196074962616,\n",
       "  0.9892224669456482,\n",
       "  0.9887092709541321,\n",
       "  0.9887092709541321,\n",
       "  0.9910187125205994,\n",
       "  0.9894790649414062,\n",
       "  0.9887092709541321,\n",
       "  0.9879394173622131,\n",
       "  0.9892224669456482,\n",
       "  0.9892224669456482,\n",
       "  0.9889658689498901,\n",
       "  0.9892224669456482,\n",
       "  0.9897357225418091,\n",
       "  0.9892224669456482,\n",
       "  0.9892224669456482,\n",
       "  0.9894790649414062,\n",
       "  0.9905055165290833,\n",
       "  0.9905055165290833,\n",
       "  0.9905055165290833,\n",
       "  0.9889658689498901,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.9897357225418091,\n",
       "  0.9894790649414062,\n",
       "  0.9899923205375671,\n",
       "  0.9902489185333252,\n",
       "  0.9892224669456482,\n",
       "  0.986143171787262,\n",
       "  0.9853733777999878,\n",
       "  0.9879394173622131,\n",
       "  0.9897357225418091,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.9899923205375671,\n",
       "  0.9899923205375671,\n",
       "  0.987169623374939,\n",
       "  0.9879394173622131,\n",
       "  0.9894790649414062,\n",
       "  0.9897357225418091,\n",
       "  0.9910187125205994,\n",
       "  0.9894790649414062,\n",
       "  0.9892224669456482,\n",
       "  0.9902489185333252,\n",
       "  0.9907621145248413,\n",
       "  0.9899923205375671,\n",
       "  0.9910187125205994,\n",
       "  0.9905055165290833,\n",
       "  0.9899923205375671,\n",
       "  0.9910187125205994,\n",
       "  0.988452672958374,\n",
       "  0.9876828193664551,\n",
       "  0.988452672958374,\n",
       "  0.9905055165290833,\n",
       "  0.9887092709541321,\n",
       "  0.9876828193664551,\n",
       "  0.9876828193664551,\n",
       "  0.9892224669456482,\n",
       "  0.9887092709541321,\n",
       "  0.9879394173622131,\n",
       "  0.9902489185333252,\n",
       "  0.9912753105163574,\n",
       "  0.9907621145248413,\n",
       "  0.9899923205375671,\n",
       "  0.9912753105163574,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.987426221370697,\n",
       "  0.9897357225418091,\n",
       "  0.9897357225418091,\n",
       "  0.9907621145248413,\n",
       "  0.9905055165290833,\n",
       "  0.9912753105163574,\n",
       "  0.9907621145248413,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9910187125205994,\n",
       "  0.9925583600997925,\n",
       "  0.9910187125205994,\n",
       "  0.9894790649414062,\n",
       "  0.9910187125205994,\n",
       "  0.9889658689498901,\n",
       "  0.9902489185333252,\n",
       "  0.9910187125205994,\n",
       "  0.9917885661125183,\n",
       "  0.9912753105163574,\n",
       "  0.9910187125205994,\n",
       "  0.988452672958374,\n",
       "  0.987169623374939,\n",
       "  0.9910187125205994,\n",
       "  0.9912753105163574,\n",
       "  0.9920451641082764,\n",
       "  0.9907621145248413,\n",
       "  0.9907621145248413,\n",
       "  0.9915319681167603,\n",
       "  0.9912753105163574,\n",
       "  0.9902489185333252,\n",
       "  0.9902489185333252,\n",
       "  0.9925583600997925,\n",
       "  0.9915319681167603,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9910187125205994,\n",
       "  0.9915319681167603,\n",
       "  0.9905055165290833,\n",
       "  0.9917885661125183,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9915319681167603,\n",
       "  0.9925583600997925,\n",
       "  0.9917885661125183,\n",
       "  0.9894790649414062,\n",
       "  0.9897357225418091,\n",
       "  0.9892224669456482,\n",
       "  0.9907621145248413,\n",
       "  0.9907621145248413,\n",
       "  0.9910187125205994,\n",
       "  0.9907621145248413,\n",
       "  0.9910187125205994,\n",
       "  0.9920451641082764,\n",
       "  0.9910187125205994,\n",
       "  0.9915319681167603,\n",
       "  0.9902489185333252,\n",
       "  0.9910187125205994,\n",
       "  0.9915319681167603,\n",
       "  0.9905055165290833,\n",
       "  0.9902489185333252,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9899923205375671,\n",
       "  0.987426221370697,\n",
       "  0.9876828193664551,\n",
       "  0.9915319681167603,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9907621145248413,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9920451641082764,\n",
       "  0.9889658689498901,\n",
       "  0.9910187125205994,\n",
       "  0.9905055165290833,\n",
       "  0.9917885661125183,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9920451641082764,\n",
       "  0.9915319681167603,\n",
       "  0.9899923205375671,\n",
       "  0.9894790649414062,\n",
       "  0.988452672958374,\n",
       "  0.9907621145248413,\n",
       "  0.9923017621040344,\n",
       "  0.9912753105163574,\n",
       "  0.9923017621040344,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9930716156959534,\n",
       "  0.9930716156959534,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9925583600997925,\n",
       "  0.9912753105163574,\n",
       "  0.9923017621040344,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9912753105163574,\n",
       "  0.9899923205375671,\n",
       "  0.9917885661125183,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9892224669456482,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9915319681167603,\n",
       "  0.9915319681167603,\n",
       "  0.9910187125205994,\n",
       "  0.9928149580955505,\n",
       "  0.9920451641082764,\n",
       "  0.9923017621040344,\n",
       "  0.9915319681167603,\n",
       "  0.9928149580955505,\n",
       "  0.9925583600997925,\n",
       "  0.9912753105163574,\n",
       "  0.9917885661125183,\n",
       "  0.9930716156959534,\n",
       "  0.9925583600997925,\n",
       "  0.9915319681167603,\n",
       "  0.9912753105163574,\n",
       "  0.9897357225418091,\n",
       "  0.9920451641082764,\n",
       "  0.9923017621040344,\n",
       "  0.9917885661125183,\n",
       "  0.9928149580955505,\n",
       "  0.9925583600997925,\n",
       "  0.9923017621040344,\n",
       "  0.9917885661125183,\n",
       "  0.9923017621040344,\n",
       "  0.9897357225418091,\n",
       "  0.9925583600997925,\n",
       "  0.9928149580955505,\n",
       "  0.9923017621040344,\n",
       "  0.9930716156959534,\n",
       "  0.9915319681167603,\n",
       "  0.9923017621040344,\n",
       "  0.988196074962616,\n",
       "  0.9876828193664551,\n",
       "  0.9902489185333252,\n",
       "  0.9912753105163574,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9923017621040344,\n",
       "  0.9910187125205994,\n",
       "  0.9930716156959534,\n",
       "  0.9923017621040344,\n",
       "  0.9920451641082764,\n",
       "  0.9928149580955505,\n",
       "  0.9910187125205994,\n",
       "  0.9923017621040344,\n",
       "  0.9907621145248413,\n",
       "  0.9912753105163574,\n",
       "  0.9915319681167603,\n",
       "  0.9917885661125183,\n",
       "  0.9920451641082764,\n",
       "  0.9933282136917114,\n",
       "  0.9925583600997925,\n",
       "  0.9910187125205994,\n",
       "  0.9915319681167603,\n",
       "  0.9925583600997925,\n",
       "  0.9915319681167603,\n",
       "  0.9887092709541321,\n",
       "  0.9889658689498901,\n",
       "  0.9910187125205994,\n",
       "  0.9915319681167603,\n",
       "  0.9915319681167603,\n",
       "  0.9910187125205994,\n",
       "  0.9897357225418091,\n",
       "  0.987169623374939,\n",
       "  0.986143171787262,\n",
       "  0.9905055165290833,\n",
       "  0.9887092709541321,\n",
       "  0.9917885661125183],\n",
       " 'val_loss': [0.7902368307113647,\n",
       "  0.30655035376548767,\n",
       "  0.3751881718635559,\n",
       "  0.37316790223121643,\n",
       "  0.340437114238739,\n",
       "  0.29283103346824646,\n",
       "  0.25656571984291077,\n",
       "  0.24387840926647186,\n",
       "  0.23805645108222961,\n",
       "  0.23304574191570282,\n",
       "  0.230227991938591,\n",
       "  0.22596940398216248,\n",
       "  0.2212792932987213,\n",
       "  0.21815785765647888,\n",
       "  0.21555458009243011,\n",
       "  0.21262489259243011,\n",
       "  0.211403027176857,\n",
       "  0.20997309684753418,\n",
       "  0.2085513025522232,\n",
       "  0.20793834328651428,\n",
       "  0.20724017918109894,\n",
       "  0.20659050345420837,\n",
       "  0.20500504970550537,\n",
       "  0.20448824763298035,\n",
       "  0.2034837007522583,\n",
       "  0.20295315980911255,\n",
       "  0.2017691731452942,\n",
       "  0.20068807899951935,\n",
       "  0.20053264498710632,\n",
       "  0.19834648072719574,\n",
       "  0.19719980657100677,\n",
       "  0.19742527604103088,\n",
       "  0.19571082293987274,\n",
       "  0.19278183579444885,\n",
       "  0.1932532638311386,\n",
       "  0.19346767663955688,\n",
       "  0.19147521257400513,\n",
       "  0.18985490500926971,\n",
       "  0.188773512840271,\n",
       "  0.18810313940048218,\n",
       "  0.1882842779159546,\n",
       "  0.184822678565979,\n",
       "  0.18448486924171448,\n",
       "  0.1831003576517105,\n",
       "  0.1806480586528778,\n",
       "  0.17757648229599,\n",
       "  0.17879515886306763,\n",
       "  0.17436298727989197,\n",
       "  0.16966402530670166,\n",
       "  0.1703772246837616,\n",
       "  0.16318625211715698,\n",
       "  0.1581151932477951,\n",
       "  0.15401864051818848,\n",
       "  0.15175502002239227,\n",
       "  0.14549191296100616,\n",
       "  0.1459827870130539,\n",
       "  0.1365605592727661,\n",
       "  0.1320829689502716,\n",
       "  0.1273498833179474,\n",
       "  0.12425954639911652,\n",
       "  0.12759529054164886,\n",
       "  0.12860046327114105,\n",
       "  0.11957342177629471,\n",
       "  0.11321509629487991,\n",
       "  0.11438293755054474,\n",
       "  0.10968384146690369,\n",
       "  0.10711412876844406,\n",
       "  0.10653423517942429,\n",
       "  0.10463045537471771,\n",
       "  0.10238306224346161,\n",
       "  0.10191765427589417,\n",
       "  0.09971235692501068,\n",
       "  0.1021885797381401,\n",
       "  0.09762527048587799,\n",
       "  0.09702662378549576,\n",
       "  0.09561534225940704,\n",
       "  0.09678851813077927,\n",
       "  0.09410197287797928,\n",
       "  0.09860872477293015,\n",
       "  0.0925377681851387,\n",
       "  0.09181895107030869,\n",
       "  0.09105609357357025,\n",
       "  0.090738445520401,\n",
       "  0.09439258277416229,\n",
       "  0.0917327031493187,\n",
       "  0.10084056854248047,\n",
       "  0.0976838544011116,\n",
       "  0.1023046150803566,\n",
       "  0.11067041009664536,\n",
       "  0.10646869242191315,\n",
       "  0.10035665333271027,\n",
       "  0.09241247922182083,\n",
       "  0.09867295622825623,\n",
       "  0.09115201234817505,\n",
       "  0.0992993637919426,\n",
       "  0.09559117257595062,\n",
       "  0.09648150950670242,\n",
       "  0.096169613301754,\n",
       "  0.08602099865674973,\n",
       "  0.08555752784013748,\n",
       "  0.0877360925078392,\n",
       "  0.09161396324634552,\n",
       "  0.09800742566585541,\n",
       "  0.08405036479234695,\n",
       "  0.08460783213376999,\n",
       "  0.08304944634437561,\n",
       "  0.08038745820522308,\n",
       "  0.08035213500261307,\n",
       "  0.08117671310901642,\n",
       "  0.07930649071931839,\n",
       "  0.07878033816814423,\n",
       "  0.07828177511692047,\n",
       "  0.07904507219791412,\n",
       "  0.07794882357120514,\n",
       "  0.07822591066360474,\n",
       "  0.07845199853181839,\n",
       "  0.07772219181060791,\n",
       "  0.07716603577136993,\n",
       "  0.0773705244064331,\n",
       "  0.0793318971991539,\n",
       "  0.07696288824081421,\n",
       "  0.07796827703714371,\n",
       "  0.07649349421262741,\n",
       "  0.0810055285692215,\n",
       "  0.08002477139234543,\n",
       "  0.07698293030261993,\n",
       "  0.07794903963804245,\n",
       "  0.07789342105388641,\n",
       "  0.0765664353966713,\n",
       "  0.07463850826025009,\n",
       "  0.0754634365439415,\n",
       "  0.07806382328271866,\n",
       "  0.07539216428995132,\n",
       "  0.0775880515575409,\n",
       "  0.07582314312458038,\n",
       "  0.08083599805831909,\n",
       "  0.07898714393377304,\n",
       "  0.08172475546598434,\n",
       "  0.08235514163970947,\n",
       "  0.07541846483945847,\n",
       "  0.07397598773241043,\n",
       "  0.07999490201473236,\n",
       "  0.07738642394542694,\n",
       "  0.07266175746917725,\n",
       "  0.07383757084608078,\n",
       "  0.07525406777858734,\n",
       "  0.07199869304895401,\n",
       "  0.07273100316524506,\n",
       "  0.07208915799856186,\n",
       "  0.07634671777486801,\n",
       "  0.07554923743009567,\n",
       "  0.07158137112855911,\n",
       "  0.07868807017803192,\n",
       "  0.07530608773231506,\n",
       "  0.07104189693927765,\n",
       "  0.07451651990413666,\n",
       "  0.07842723280191422,\n",
       "  0.07457011938095093,\n",
       "  0.08030196279287338,\n",
       "  0.08043252676725388,\n",
       "  0.07172230631113052,\n",
       "  0.07336118817329407,\n",
       "  0.07251878082752228,\n",
       "  0.07027056813240051,\n",
       "  0.07100199908018112,\n",
       "  0.07316289097070694,\n",
       "  0.08155903965234756,\n",
       "  0.0811668187379837,\n",
       "  0.07662981003522873,\n",
       "  0.06967528164386749,\n",
       "  0.07946258783340454,\n",
       "  0.06998645514249802,\n",
       "  0.06992393732070923,\n",
       "  0.07391095161437988,\n",
       "  0.081411212682724,\n",
       "  0.07518887519836426,\n",
       "  0.0740860104560852,\n",
       "  0.07080261409282684,\n",
       "  0.06970604509115219,\n",
       "  0.06979826837778091,\n",
       "  0.07300973683595657,\n",
       "  0.07390007376670837,\n",
       "  0.07040530443191528,\n",
       "  0.07400130480527878,\n",
       "  0.0773622989654541,\n",
       "  0.0739755928516388,\n",
       "  0.0686112642288208,\n",
       "  0.07160904258489609,\n",
       "  0.07228559255599976,\n",
       "  0.06860904395580292,\n",
       "  0.0686187893152237,\n",
       "  0.06865079700946808,\n",
       "  0.07829564064741135,\n",
       "  0.0801159143447876,\n",
       "  0.08695266395807266,\n",
       "  0.10755506902933121,\n",
       "  0.11171649396419525,\n",
       "  0.07300986349582672,\n",
       "  0.0668887048959732,\n",
       "  0.06828761100769043,\n",
       "  0.0698670744895935,\n",
       "  0.07471892982721329,\n",
       "  0.09344010800123215,\n",
       "  0.07519100606441498,\n",
       "  0.06962025910615921,\n",
       "  0.06713730096817017,\n",
       "  0.06876382976770401,\n",
       "  0.06820216029882431,\n",
       "  0.07002350687980652,\n",
       "  0.07341178506612778,\n",
       "  0.06842642277479172,\n",
       "  0.06782827526330948,\n",
       "  0.06790103018283844,\n",
       "  0.07316002994775772,\n",
       "  0.081812284886837,\n",
       "  0.08577381074428558,\n",
       "  0.0674501284956932,\n",
       "  0.06801735609769821,\n",
       "  0.06741078197956085,\n",
       "  0.06788776069879532,\n",
       "  0.06728065013885498,\n",
       "  0.07123320549726486,\n",
       "  0.06653585284948349,\n",
       "  0.06682998687028885,\n",
       "  0.06975354999303818,\n",
       "  0.07306050509214401,\n",
       "  0.06977466493844986,\n",
       "  0.07250671833753586,\n",
       "  0.06600502878427505,\n",
       "  0.06648056954145432,\n",
       "  0.06748601794242859,\n",
       "  0.06876441091299057,\n",
       "  0.06621918082237244,\n",
       "  0.06775105744600296,\n",
       "  0.06585494428873062,\n",
       "  0.0719391256570816,\n",
       "  0.07432565838098526,\n",
       "  0.07388297468423843,\n",
       "  0.06986924260854721,\n",
       "  0.0680684745311737,\n",
       "  0.06745883077383041,\n",
       "  0.06590786576271057,\n",
       "  0.06665486097335815,\n",
       "  0.07138712704181671,\n",
       "  0.06820093840360641,\n",
       "  0.06758418679237366,\n",
       "  0.0683014914393425,\n",
       "  0.06521663814783096,\n",
       "  0.06540009379386902,\n",
       "  0.06955868005752563,\n",
       "  0.06494326889514923,\n",
       "  0.06554057449102402,\n",
       "  0.07091659307479858,\n",
       "  0.06581473350524902,\n",
       "  0.06649774312973022,\n",
       "  0.06575868278741837,\n",
       "  0.06513041257858276,\n",
       "  0.06496763974428177,\n",
       "  0.06573557108640671,\n",
       "  0.065131776034832,\n",
       "  0.06634406745433807,\n",
       "  0.06786756962537766,\n",
       "  0.0661194697022438,\n",
       "  0.06647606194019318,\n",
       "  0.06480131298303604,\n",
       "  0.0642915666103363,\n",
       "  0.06429790705442429,\n",
       "  0.06452594697475433,\n",
       "  0.0666835755109787,\n",
       "  0.06909648329019547,\n",
       "  0.06494996696710587,\n",
       "  0.06468435376882553,\n",
       "  0.06454510986804962,\n",
       "  0.06397619098424911,\n",
       "  0.06389725208282471,\n",
       "  0.06611677259206772,\n",
       "  0.06401072442531586,\n",
       "  0.0642700344324112,\n",
       "  0.06357521563768387,\n",
       "  0.06496552377939224,\n",
       "  0.06539323925971985,\n",
       "  0.06567002832889557,\n",
       "  0.07131912559270859,\n",
       "  0.06871633976697922,\n",
       "  0.06434269994497299,\n",
       "  0.06353405863046646,\n",
       "  0.0639868974685669,\n",
       "  0.06371273100376129,\n",
       "  0.06298694014549255,\n",
       "  0.06454482674598694,\n",
       "  0.07498911023139954,\n",
       "  0.06373028457164764,\n",
       "  0.06639749556779861,\n",
       "  0.06893431395292282,\n",
       "  0.06346789747476578,\n",
       "  0.07034727185964584,\n",
       "  0.06507112085819244,\n",
       "  0.07037830352783203,\n",
       "  0.06910112500190735,\n",
       "  0.0654032900929451,\n",
       "  0.062210563570261,\n",
       "  0.06291814893484116,\n",
       "  0.06581845134496689,\n",
       "  0.07607920467853546,\n",
       "  0.07329604774713516,\n",
       "  0.07467623054981232,\n",
       "  0.06710503250360489,\n",
       "  0.07050758600234985,\n",
       "  0.08676765859127045,\n",
       "  0.07642252743244171,\n",
       "  0.07403314113616943,\n",
       "  0.06835594028234482,\n",
       "  0.06132376939058304,\n",
       "  0.06184038519859314,\n",
       "  0.06295327097177505,\n",
       "  0.06267399340867996,\n",
       "  0.06166788563132286,\n",
       "  0.061506692320108414,\n",
       "  0.061017490923404694,\n",
       "  0.06083831936120987,\n",
       "  0.06411001086235046,\n",
       "  0.06435683369636536,\n",
       "  0.06544169783592224,\n",
       "  0.06289080530405045,\n",
       "  0.06122409552335739,\n",
       "  0.06052538380026817,\n",
       "  0.06161294877529144,\n",
       "  0.06236579269170761,\n",
       "  0.06718344986438751,\n",
       "  0.06326206773519516,\n",
       "  0.06192934885621071,\n",
       "  0.06445827335119247,\n",
       "  0.07358856499195099,\n",
       "  0.07285274565219879,\n",
       "  0.06906968355178833,\n",
       "  0.06899362057447433,\n",
       "  0.06065908446907997,\n",
       "  0.061080463230609894,\n",
       "  0.06117657572031021,\n",
       "  0.061658188700675964,\n",
       "  0.0636768564581871,\n",
       "  0.062351807951927185,\n",
       "  0.06134600192308426,\n",
       "  0.06118106469511986,\n",
       "  0.06094961240887642,\n",
       "  0.06115074083209038,\n",
       "  0.06133194640278816,\n",
       "  0.06017613410949707,\n",
       "  0.061918873339891434,\n",
       "  0.06194475665688515,\n",
       "  0.060847777873277664,\n",
       "  0.0596022866666317,\n",
       "  0.06268911063671112,\n",
       "  0.0648469552397728,\n",
       "  0.06519290804862976,\n",
       "  0.061271149665117264,\n",
       "  0.06244737654924393,\n",
       "  0.06162063777446747,\n",
       "  0.0607900395989418,\n",
       "  0.06645649671554565,\n",
       "  0.06386691331863403,\n",
       "  0.06166892498731613,\n",
       "  0.06537805497646332,\n",
       "  0.06254842132329941,\n",
       "  0.05908150225877762,\n",
       "  0.059456922113895416,\n",
       "  0.06411123275756836,\n",
       "  0.06947167962789536,\n",
       "  0.0662793293595314,\n",
       "  0.06892084330320358,\n",
       "  0.07130330055952072,\n",
       "  0.06280286610126495,\n",
       "  0.06578269600868225,\n",
       "  0.06623806059360504,\n",
       "  0.0658864676952362,\n",
       "  0.06458300352096558,\n",
       "  0.06250622123479843,\n",
       "  0.060360852628946304,\n",
       "  0.060498692095279694,\n",
       "  0.06402984261512756,\n",
       "  0.06000341475009918,\n",
       "  0.05948873609304428,\n",
       "  0.06066953390836716,\n",
       "  0.05869678035378456,\n",
       "  0.06054141744971275,\n",
       "  0.06024942919611931,\n",
       "  0.0622497983276844,\n",
       "  0.06395025551319122,\n",
       "  0.06114239618182182,\n",
       "  0.0586111843585968,\n",
       "  0.06291169673204422,\n",
       "  0.06047183275222778,\n",
       "  0.05957351624965668,\n",
       "  0.06383967399597168,\n",
       "  0.06496955454349518,\n",
       "  0.06381198763847351,\n",
       "  0.06251551955938339,\n",
       "  0.060881100594997406,\n",
       "  0.05952344462275505,\n",
       "  0.05836046114563942,\n",
       "  0.05765429511666298,\n",
       "  0.058947864919900894,\n",
       "  0.060939159244298935,\n",
       "  0.059246908873319626,\n",
       "  0.06302953511476517,\n",
       "  0.060255266726017,\n",
       "  0.05884451046586037,\n",
       "  0.05885491147637367,\n",
       "  0.059530872851610184,\n",
       "  0.05947752296924591,\n",
       "  0.058343466371297836,\n",
       "  0.058978043496608734,\n",
       "  0.0579131543636322,\n",
       "  0.05737576261162758,\n",
       "  0.05819559097290039,\n",
       "  0.05894535779953003,\n",
       "  0.05867072939872742,\n",
       "  0.057436440140008926,\n",
       "  0.057562537491321564,\n",
       "  0.05872120335698128,\n",
       "  0.05815057456493378,\n",
       "  0.057931095361709595,\n",
       "  0.06258106231689453,\n",
       "  0.057348813861608505,\n",
       "  0.05947478115558624,\n",
       "  0.06153557449579239,\n",
       "  0.058738596737384796,\n",
       "  0.06611580401659012,\n",
       "  0.06909608095884323,\n",
       "  0.06901156902313232,\n",
       "  0.061244890093803406,\n",
       "  0.06178271397948265,\n",
       "  0.06252583116292953,\n",
       "  0.06376542896032333,\n",
       "  0.06277038156986237,\n",
       "  0.06037316098809242,\n",
       "  0.0582476407289505,\n",
       "  0.05787267908453941,\n",
       "  0.05891561508178711,\n",
       "  0.05978432297706604,\n",
       "  0.0702517181634903,\n",
       "  0.05916029214859009,\n",
       "  0.05841866880655289,\n",
       "  0.05799182504415512,\n",
       "  0.05890578776597977,\n",
       "  0.05865471065044403,\n",
       "  0.05987946689128876,\n",
       "  0.058054644614458084,\n",
       "  0.056522369384765625,\n",
       "  0.06164710596203804,\n",
       "  0.058823928236961365,\n",
       "  0.05893922224640846,\n",
       "  0.0595073401927948,\n",
       "  0.057040024548769,\n",
       "  0.05913250893354416,\n",
       "  0.05956552177667618,\n",
       "  0.05847518891096115,\n",
       "  0.05697161331772804,\n",
       "  0.05708802491426468,\n",
       "  0.060424529016017914,\n",
       "  0.05886507406830788,\n",
       "  0.06004795804619789,\n",
       "  0.07145840674638748,\n",
       "  0.05799432471394539,\n",
       "  0.05744257941842079,\n",
       "  0.05812916159629822,\n",
       "  0.05833561345934868,\n",
       "  0.06342192739248276,\n",
       "  0.05862927809357643,\n",
       "  0.057392388582229614,\n",
       "  0.06277068704366684,\n",
       "  0.06015866622328758,\n",
       "  0.05770324543118477,\n",
       "  0.0579342357814312,\n",
       "  0.05691330134868622,\n",
       "  0.05711408704519272,\n",
       "  0.05673893913626671,\n",
       "  0.06260154396295547,\n",
       "  0.06218712031841278,\n",
       "  0.0673183724284172,\n",
       "  0.061018019914627075,\n",
       "  0.05787200480699539,\n",
       "  0.05650165677070618,\n",
       "  0.05653674155473709,\n",
       "  0.056909091770648956,\n",
       "  0.057706814259290695,\n",
       "  0.05940740555524826,\n",
       "  0.057298824191093445,\n",
       "  0.056996919214725494,\n",
       "  0.0566982664167881,\n",
       "  0.05860746651887894,\n",
       "  0.06005685031414032,\n",
       "  0.05706081539392471,\n",
       "  0.06322918832302094,\n",
       "  0.07522932440042496,\n",
       "  0.08427836000919342,\n",
       "  0.06536687910556793,\n",
       "  0.07506498694419861,\n",
       "  0.07001084834337234,\n",
       "  0.06625328958034515],\n",
       " 'val_accuracy': [0.5730769038200378,\n",
       "  0.8700000047683716,\n",
       "  0.8592307567596436,\n",
       "  0.8553845882415771,\n",
       "  0.8692307472229004,\n",
       "  0.879230797290802,\n",
       "  0.8946154117584229,\n",
       "  0.9015384912490845,\n",
       "  0.9092307686805725,\n",
       "  0.9130769371986389,\n",
       "  0.9138461351394653,\n",
       "  0.9153845906257629,\n",
       "  0.9176923036575317,\n",
       "  0.9184615612030029,\n",
       "  0.9200000166893005,\n",
       "  0.9230769276618958,\n",
       "  0.9223076701164246,\n",
       "  0.9215384721755981,\n",
       "  0.9215384721755981,\n",
       "  0.920769214630127,\n",
       "  0.9215384721755981,\n",
       "  0.9215384721755981,\n",
       "  0.926153838634491,\n",
       "  0.926153838634491,\n",
       "  0.926153838634491,\n",
       "  0.9269230961799622,\n",
       "  0.9276922941207886,\n",
       "  0.9284615516662598,\n",
       "  0.9284615516662598,\n",
       "  0.9300000071525574,\n",
       "  0.931538462638855,\n",
       "  0.9300000071525574,\n",
       "  0.931538462638855,\n",
       "  0.9323077201843262,\n",
       "  0.9330769181251526,\n",
       "  0.9338461756706238,\n",
       "  0.9338461756706238,\n",
       "  0.9338461756706238,\n",
       "  0.9346153736114502,\n",
       "  0.9361538290977478,\n",
       "  0.9338461756706238,\n",
       "  0.9361538290977478,\n",
       "  0.936923086643219,\n",
       "  0.939230740070343,\n",
       "  0.939230740070343,\n",
       "  0.9407692551612854,\n",
       "  0.9361538290977478,\n",
       "  0.942307710647583,\n",
       "  0.9438461661338806,\n",
       "  0.9438461661338806,\n",
       "  0.9415384531021118,\n",
       "  0.9430769085884094,\n",
       "  0.9453846216201782,\n",
       "  0.947692334651947,\n",
       "  0.9492307901382446,\n",
       "  0.9461538195610046,\n",
       "  0.9530768990516663,\n",
       "  0.9553846120834351,\n",
       "  0.9546154141426086,\n",
       "  0.9530768990516663,\n",
       "  0.9523077011108398,\n",
       "  0.949999988079071,\n",
       "  0.9538461565971375,\n",
       "  0.9638461470603943,\n",
       "  0.9561538696289062,\n",
       "  0.9607692360877991,\n",
       "  0.9615384340286255,\n",
       "  0.9623076915740967,\n",
       "  0.9623076915740967,\n",
       "  0.9646154046058655,\n",
       "  0.9646154046058655,\n",
       "  0.9669230580329895,\n",
       "  0.9615384340286255,\n",
       "  0.9692307710647583,\n",
       "  0.9700000286102295,\n",
       "  0.9700000286102295,\n",
       "  0.9723076820373535,\n",
       "  0.9723076820373535,\n",
       "  0.9653846025466919,\n",
       "  0.9700000286102295,\n",
       "  0.9700000286102295,\n",
       "  0.9700000286102295,\n",
       "  0.9700000286102295,\n",
       "  0.9692307710647583,\n",
       "  0.9730769395828247,\n",
       "  0.9653846025466919,\n",
       "  0.9669230580329895,\n",
       "  0.9630769491195679,\n",
       "  0.9607692360877991,\n",
       "  0.9623076915740967,\n",
       "  0.9646154046058655,\n",
       "  0.9700000286102295,\n",
       "  0.9661538600921631,\n",
       "  0.9723076820373535,\n",
       "  0.9653846025466919,\n",
       "  0.9669230580329895,\n",
       "  0.9676923155784607,\n",
       "  0.9646154046058655,\n",
       "  0.9738461375236511,\n",
       "  0.9730769395828247,\n",
       "  0.9723076820373535,\n",
       "  0.9707692265510559,\n",
       "  0.9653846025466919,\n",
       "  0.9753845930099487,\n",
       "  0.9746153950691223,\n",
       "  0.9746153950691223,\n",
       "  0.9761538505554199,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9769230484962463,\n",
       "  0.9738461375236511,\n",
       "  0.9753845930099487,\n",
       "  0.9769230484962463,\n",
       "  0.9738461375236511,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9769230484962463,\n",
       "  0.9761538505554199,\n",
       "  0.9761538505554199,\n",
       "  0.9769230484962463,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9769230484962463,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9753845930099487,\n",
       "  0.9761538505554199,\n",
       "  0.9784615635871887,\n",
       "  0.9761538505554199,\n",
       "  0.9769230484962463,\n",
       "  0.9761538505554199,\n",
       "  0.9769230484962463,\n",
       "  0.9753845930099487,\n",
       "  0.9769230484962463,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9769230484962463,\n",
       "  0.9761538505554199,\n",
       "  0.9776923060417175,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9784615635871887,\n",
       "  0.9761538505554199,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9776923060417175,\n",
       "  0.9800000190734863,\n",
       "  0.9784615635871887,\n",
       "  0.9761538505554199,\n",
       "  0.9761538505554199,\n",
       "  0.9753845930099487,\n",
       "  0.9769230484962463,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9784615635871887,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9769230484962463,\n",
       "  0.9761538505554199,\n",
       "  0.9776923060417175,\n",
       "  0.9784615635871887,\n",
       "  0.9753845930099487,\n",
       "  0.9792307615280151,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9746153950691223,\n",
       "  0.9769230484962463,\n",
       "  0.9784615635871887,\n",
       "  0.9784615635871887,\n",
       "  0.9807692170143127,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.9753845930099487,\n",
       "  0.9784615635871887,\n",
       "  0.9792307615280151,\n",
       "  0.9792307615280151,\n",
       "  0.9792307615280151,\n",
       "  0.9792307615280151,\n",
       "  0.9807692170143127,\n",
       "  0.9776923060417175,\n",
       "  0.9753845930099487,\n",
       "  0.9738461375236511,\n",
       "  0.9753845930099487,\n",
       "  0.9630769491195679,\n",
       "  0.9623076915740967,\n",
       "  0.9761538505554199,\n",
       "  0.9823076725006104,\n",
       "  0.9807692170143127,\n",
       "  0.9776923060417175,\n",
       "  0.9776923060417175,\n",
       "  0.9715384840965271,\n",
       "  0.9769230484962463,\n",
       "  0.9800000190734863,\n",
       "  0.9830769300460815,\n",
       "  0.9792307615280151,\n",
       "  0.9807692170143127,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.9792307615280151,\n",
       "  0.9807692170143127,\n",
       "  0.9815384745597839,\n",
       "  0.9784615635871887,\n",
       "  0.9746153950691223,\n",
       "  0.9753845930099487,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9792307615280151,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.9800000190734863,\n",
       "  0.9769230484962463,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.9784615635871887,\n",
       "  0.9830769300460815,\n",
       "  0.9800000190734863,\n",
       "  0.9830769300460815,\n",
       "  0.9776923060417175,\n",
       "  0.9769230484962463,\n",
       "  0.9792307615280151,\n",
       "  0.9792307615280151,\n",
       "  0.9823076725006104,\n",
       "  0.9800000190734863,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9784615635871887,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.9784615635871887,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9792307615280151,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9815384745597839,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9807692170143127,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9815384745597839,\n",
       "  0.9784615635871887,\n",
       "  0.9807692170143127,\n",
       "  0.983846127986908,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9823076725006104,\n",
       "  0.9776923060417175,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9815384745597839,\n",
       "  0.983846127986908,\n",
       "  0.9815384745597839,\n",
       "  0.9815384745597839,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9776923060417175,\n",
       "  0.9784615635871887,\n",
       "  0.9776923060417175,\n",
       "  0.983846127986908,\n",
       "  0.9815384745597839,\n",
       "  0.9769230484962463,\n",
       "  0.9784615635871887,\n",
       "  0.9800000190734863,\n",
       "  0.9815384745597839,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9823076725006104,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9776923060417175,\n",
       "  0.9800000190734863,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.986923098564148,\n",
       "  0.9846153855323792,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.9807692170143127,\n",
       "  0.9830769300460815,\n",
       "  0.9800000190734863,\n",
       "  0.9846153855323792,\n",
       "  0.9830769300460815,\n",
       "  0.9807692170143127,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9830769300460815,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9823076725006104,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9884615540504456,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.9853846430778503,\n",
       "  0.9830769300460815,\n",
       "  0.9800000190734863,\n",
       "  0.9800000190734863,\n",
       "  0.9853846430778503,\n",
       "  0.9846153855323792,\n",
       "  0.9823076725006104,\n",
       "  0.9823076725006104,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9800000190734863,\n",
       "  0.986923098564148,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.983846127986908,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9846153855323792,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.9861538410186768,\n",
       "  0.9830769300460815,\n",
       "  0.9853846430778503,\n",
       "  0.983846127986908,\n",
       "  0.9800000190734863,\n",
       "  0.9846153855323792,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.983846127986908,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9815384745597839,\n",
       "  0.9846153855323792,\n",
       "  0.986923098564148,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.9861538410186768,\n",
       "  0.983846127986908,\n",
       "  0.983846127986908,\n",
       "  0.9807692170143127,\n",
       "  0.9846153855323792,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.9876922965049744,\n",
       "  0.986923098564148,\n",
       "  0.9861538410186768,\n",
       "  0.986923098564148,\n",
       "  0.986923098564148,\n",
       "  0.986923098564148,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.9853846430778503,\n",
       "  0.986923098564148,\n",
       "  0.9830769300460815,\n",
       "  0.9800000190734863,\n",
       "  0.9784615635871887,\n",
       "  0.9815384745597839,\n",
       "  0.9807692170143127,\n",
       "  0.9815384745597839,\n",
       "  0.983846127986908]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3.history\n",
    "# 키가 있어서 df 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3_df = pd.DataFrame(data=history3.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.404591</td>\n",
       "      <td>0.273287</td>\n",
       "      <td>0.790237</td>\n",
       "      <td>0.573077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.414476</td>\n",
       "      <td>0.806005</td>\n",
       "      <td>0.306550</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.314119</td>\n",
       "      <td>0.878881</td>\n",
       "      <td>0.375188</td>\n",
       "      <td>0.859231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.335187</td>\n",
       "      <td>0.872209</td>\n",
       "      <td>0.373168</td>\n",
       "      <td>0.855385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.321920</td>\n",
       "      <td>0.877598</td>\n",
       "      <td>0.340437</td>\n",
       "      <td>0.869231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy\n",
       "0  3.404591  0.273287  0.790237      0.573077\n",
       "1  0.414476  0.806005  0.306550      0.870000\n",
       "2  0.314119  0.878881  0.375188      0.859231\n",
       "3  0.335187  0.872209  0.373168      0.855385\n",
       "4  0.321920  0.877598  0.340437      0.869231"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history3_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loss = history3_df['loss']\n",
    "y_vloss = history3_df['val_loss']\n",
    "# save series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_len = np.arange(len(y_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2 , label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2 , label='Trainset_loss')\n",
    "plt.legend(loc=1)\n",
    "# plt.xlim(0, 10)\n",
    "plt.ylim(0, 0.2)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "modelpath = \"./model/all4/{epoch:02d}-{val_accuracy:.4f}.keras\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patience=20 검증셋의 오차가 20번 이상 낮아지지 않을 경우 학습종료 하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stoping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, verbose=1,  save_best_only=True,  save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "1/8 [==>...........................] - ETA: 4s - loss: 0.0339 - accuracy: 0.9880\n",
      "Epoch 1: val_loss improved from inf to 0.06086, saving model to ./model/all4\\01-0.0609.h5\n",
      "8/8 [==============================] - 1s 17ms/step - loss: 0.0900 - accuracy: 0.9733 - val_loss: 0.0609 - val_accuracy: 0.9815\n",
      "Epoch 2/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0812 - accuracy: 0.9820\n",
      "Epoch 2: val_loss did not improve from 0.06086\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0684 - accuracy: 0.9800 - val_loss: 0.0723 - val_accuracy: 0.9731\n",
      "Epoch 3/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0369 - accuracy: 0.9860\n",
      "Epoch 3: val_loss did not improve from 0.06086\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0681 - accuracy: 0.9772 - val_loss: 0.0650 - val_accuracy: 0.9792\n",
      "Epoch 4/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0474 - accuracy: 0.9880\n",
      "Epoch 4: val_loss improved from 0.06086 to 0.05682, saving model to ./model/all4\\04-0.0568.h5\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0605 - accuracy: 0.9820 - val_loss: 0.0568 - val_accuracy: 0.9815\n",
      "Epoch 5/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0505 - accuracy: 0.9880\n",
      "Epoch 5: val_loss improved from 0.05682 to 0.05667, saving model to ./model/all4\\05-0.0567.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\mldltest1\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0580 - accuracy: 0.9843 - val_loss: 0.0567 - val_accuracy: 0.9815\n",
      "Epoch 6/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0587 - accuracy: 0.9840\n",
      "Epoch 6: val_loss did not improve from 0.05667\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9828 - val_loss: 0.0579 - val_accuracy: 0.9823\n",
      "Epoch 7/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0709 - accuracy: 0.9780\n",
      "Epoch 7: val_loss did not improve from 0.05667\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0569 - accuracy: 0.9841 - val_loss: 0.0587 - val_accuracy: 0.9808\n",
      "Epoch 8/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0527 - accuracy: 0.9860\n",
      "Epoch 8: val_loss improved from 0.05667 to 0.05581, saving model to ./model/all4\\08-0.0558.h5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0591 - accuracy: 0.9854 - val_loss: 0.0558 - val_accuracy: 0.9808\n",
      "Epoch 9/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0540 - accuracy: 0.9820\n",
      "Epoch 9: val_loss did not improve from 0.05581\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0610 - val_accuracy: 0.9800\n",
      "Epoch 10/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0694 - accuracy: 0.9800\n",
      "Epoch 10: val_loss did not improve from 0.05581\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0588 - accuracy: 0.9843 - val_loss: 0.0560 - val_accuracy: 0.9823\n",
      "Epoch 11/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0616 - accuracy: 0.9860\n",
      "Epoch 11: val_loss did not improve from 0.05581\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0571 - accuracy: 0.9849 - val_loss: 0.0565 - val_accuracy: 0.9815\n",
      "Epoch 12/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0536 - accuracy: 0.9840\n",
      "Epoch 12: val_loss improved from 0.05581 to 0.05580, saving model to ./model/all4\\12-0.0558.h5\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0575 - accuracy: 0.9846 - val_loss: 0.0558 - val_accuracy: 0.9815\n",
      "Epoch 13/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1009 - accuracy: 0.9800\n",
      "Epoch 13: val_loss did not improve from 0.05580\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0580 - accuracy: 0.9849 - val_loss: 0.0592 - val_accuracy: 0.9815\n",
      "Epoch 14/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0773 - accuracy: 0.9720\n",
      "Epoch 14: val_loss did not improve from 0.05580\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0605 - accuracy: 0.9820 - val_loss: 0.0654 - val_accuracy: 0.9785\n",
      "Epoch 15/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0741 - accuracy: 0.9800\n",
      "Epoch 15: val_loss did not improve from 0.05580\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9828 - val_loss: 0.0562 - val_accuracy: 0.9823\n",
      "Epoch 16/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0515 - accuracy: 0.9820\n",
      "Epoch 16: val_loss did not improve from 0.05580\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0619 - accuracy: 0.9828 - val_loss: 0.0630 - val_accuracy: 0.9800\n",
      "Epoch 17/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0369 - accuracy: 0.9880\n",
      "Epoch 17: val_loss did not improve from 0.05580\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0581 - accuracy: 0.9851 - val_loss: 0.0566 - val_accuracy: 0.9831\n",
      "Epoch 18/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0307 - accuracy: 0.9900\n",
      "Epoch 18: val_loss improved from 0.05580 to 0.05523, saving model to ./model/all4\\18-0.0552.h5\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.0565 - accuracy: 0.9854 - val_loss: 0.0552 - val_accuracy: 0.9823\n",
      "Epoch 19/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9920\n",
      "Epoch 19: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0569 - accuracy: 0.9843 - val_loss: 0.0553 - val_accuracy: 0.9823\n",
      "Epoch 20/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0722 - accuracy: 0.9840\n",
      "Epoch 20: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.9851 - val_loss: 0.0558 - val_accuracy: 0.9838\n",
      "Epoch 21/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0456 - accuracy: 0.9860\n",
      "Epoch 21: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0560 - accuracy: 0.9856 - val_loss: 0.0565 - val_accuracy: 0.9815\n",
      "Epoch 22/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0351 - accuracy: 0.9900\n",
      "Epoch 22: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0563 - accuracy: 0.9846 - val_loss: 0.0635 - val_accuracy: 0.9800\n",
      "Epoch 23/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0537 - accuracy: 0.9820\n",
      "Epoch 23: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0595 - accuracy: 0.9836 - val_loss: 0.0596 - val_accuracy: 0.9831\n",
      "Epoch 24/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0412 - accuracy: 0.9860\n",
      "Epoch 24: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0581 - accuracy: 0.9846 - val_loss: 0.0624 - val_accuracy: 0.9808\n",
      "Epoch 25/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0784 - accuracy: 0.9820\n",
      "Epoch 25: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9861 - val_loss: 0.0561 - val_accuracy: 0.9838\n",
      "Epoch 26/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0282 - accuracy: 0.9940\n",
      "Epoch 26: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0562 - accuracy: 0.9856 - val_loss: 0.0564 - val_accuracy: 0.9815\n",
      "Epoch 27/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0686 - accuracy: 0.9800\n",
      "Epoch 27: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0572 - accuracy: 0.9841 - val_loss: 0.0555 - val_accuracy: 0.9815\n",
      "Epoch 28/2000\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0424 - accuracy: 0.9880\n",
      "Epoch 28: val_loss did not improve from 0.05523\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0552 - accuracy: 0.9856 - val_loss: 0.0571 - val_accuracy: 0.9831\n",
      "Epoch 28: early stopping\n"
     ]
    }
   ],
   "source": [
    "model4.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 체크포인트 콜백 정의 (save_format 지정)\n",
    "checkpointer = ModelCheckpoint(filepath='./model/all4/{epoch:02d}-{val_loss:.4f}.h5', \n",
    "                               save_best_only=True, \n",
    "                               verbose=1 )\n",
    "\n",
    "# 얼리 스토핑 콜백 정의\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# 모델 훈련\n",
    "history4 = model4.fit(X_train, y_train,\n",
    "                      epochs=2000,\n",
    "                      batch_size=500,\n",
    "                      verbose=1,\n",
    "                      validation_split=0.25,\n",
    "                      callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "history4_df = pd.DataFrame(data=history4.history)\n",
    "y_loss = history4_df['loss']\n",
    "y_vloss = history4_df['val_loss']\n",
    "history4_df = pd.DataFrame(data=history4.history)\n",
    "x_len = np.arange(len(y_loss))\n",
    "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2 , label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, \"o\", c=\"blue\", markersize=2 , label='Trainset_loss')\n",
    "plt.legend(loc=1)\n",
    "# plt.xlim(0, 10)\n",
    "# plt.ylim(0, 0.2)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldltest1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
