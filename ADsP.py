# 정형 데이터 마이닝은 체계적으로 접근할 수 있는 다양한 주제를 포함합니다. 
# 다음은 정형 데이터 마이닝을 공부하기 위한 목차 예제입니다.

# 1. 데이터 마이닝 개요
#       데이터 마이닝 정의
#       데이터 마이닝의 필요성
#       데이터 마이닝 프로세스
# 2. 데이터 전처리
#       데이터 정제 (결측값 처리, 이상치 처리)
#       데이터 통합
#       데이터 변환 (정규화, 이산화)
#       데이터 축소 (차원 축소, 특징 선택)
# 3. 탐색적 데이터 분석 (EDA)
#       기술 통계
#       데이터 시각화 (히스토그램, 박스 플롯, 산점도)
#       상관 분석


#######################################################################################################
# 4. 분류 알고리즘
#       [의사결정나무]
            # 의사결정나무(Decision Tree)는 
            #       데이터 마이닝과 기계 학습에서 사용되는 강력한 예측 모델링 도구입니다. 
            #       데이터의 특성을 기반으로 하나의 결정에서 다음으로 이어지는 여러 경로를 나무 구조로 표현합니다. 
            #       각 노드(node)는 특정 조건을 나타내며, 각 가지(branch)는 가능한 결과를 나타냅니다.            
            # 노드(Node):
            #     결정 트리의 각 단계를 나타내며, 특정 특성(feature)에 대한 테스트를 나타냅니다.
            # 가지(Branch):
            #     각 노드에서 가능한 다음 상태를 나타냅니다. 예측 변수의 값에 따라 분기됩니다.
            # 루트 노드(Root Node):
            #     결정 트리의 시작 노드로, 모든 데이터 포인트를 포함합니다.
            # 리프 노드(Leaf Node):
            #     최종 결과를 출력하는 노드입니다. 더 이상의 분기가 없는 노드입니다.
            # 분기 조건(Splitting Criterion):
            #     각 노드에서 데이터를 분할하는 데 사용되는 기준입니다. 예를 들어, 특정 변수의 값을 기준으로 데이터를 분할할 수 있습니다.
            # 동작 원리
            #     트리 구성: 
            #               결정 트리는 입력 데이터의 특성을 기반으로 하여 트리 구조를 형성합니다. 
            #               각 노드에서는 최적의 분할을 위해 특정 조건(예: 변수의 값이 특정 임계값보다 크거나 작은지)을 고려하여 데이터를 분리합니다.
            #     분할 기준 선택: 
            #               분할 기준은 일반적으로 정보 이득(Information Gain), 지니 계수(Gini Index), 엔트로피(Entropy) 등의 지표를 사용하여 
            #               선택됩니다. 이 지표들은 각 분할 후의 데이터 집합의 순도(purity)를 측정하여 가장 좋은 분할을 결정합니다.
            #     예측: 
            #           새로운 데이터 포인트가 결정 트리를 통과할 때, 각 노드에서 해당하는 조건을 기반으로 
            #           왼쪽이나 오른쪽 가지로 이동하며 최종적으로 리프 노드에 도달하여 예측을 수행합니다.
            
            # 장점과 단점
                # 장점: 
                    # 해석 용이성: 결정 트리는 직관적이며 해석하기 쉽습니다.
                    # 변수의 중요도 평가: 각 특성의 중요도를 평가할 수 있어 특성 선택(feature selection)에 유용합니다.
                    # 범용성: 수치형 데이터와 범주형 데이터 모두에 적용할 수 있습니다.
                # 단점: 
                    # 과적합(Overfitting): 깊은 트리가 생성될 경우 학습 데이터에 너무 맞추어져 일반화 성능이 저하될 수 있습니다. 
                    # 이를 방지하기 위해 가지치기(Pruning)가 필요합니다.
                    # 데이터 불균형 문제: 클래스가 불균형하게 분포된 데이터에서는 예측 성능이 저하될 수 있습니다.
                    # 비선형 관계 표현의 한계: 결정 트리는 각 노드에서 선형 관계를 고려하지 않기 때문에, 
                    # 데이터가 선형적이지 않을 때는 다른 모델이 더 적합할 수 있습니다.            
                # 응용 예시
                    # 의료 진단: 환자의 증상과 검사 결과를 기반으로 질병을 예측하거나 분류합니다.
                    # 금융 분야: 고객의 신용 점수를 기반으로 대출 승인 여부를 결정합니다.
                    # 마케팅: 고객 특성을 기반으로 상품 추천 시스템을 개발하거나 마케팅 전략을 수립합니다.

#       [랜덤 포레스트]
            # 랜덤 포레스트(Random Forest)는 앙상블 학습(Ensemble Learning)의 일종으로, 
            # 여러 개의 결정 트리(Decision Tree)를 사용하여 분류(Classification) 또는 회귀(Regression) 문제를 해결하는 머신러닝 알고리즘입니다. 
            # 랜덤 포레스트는 결정 트리의 과적합 문제를 완화하고, 예측 성능을 향상시키는 데 매우 효과적입니다.
            # 주요 개념
                # 앙상블 학습(Ensemble Learning):
                #     여러 개의 간단한 모델(결정 트리)을 조합하여 하나의 강력한 모델을 만드는 학습 방법론입니다.
                # 결정 트리의 문제점:
                #       단일 결정 트리는 데이터의 작은 변화에도 과적합(Overfitting)되기 쉽습니다. 
                #       랜덤 포레스트는 이러한 문제를 해결하기 위해 여러 결정 트리를 사용합니다.
                # 랜덤 포레스트의 동작 방식:
                # 부트스트랩 샘플링(Bootstrap Sampling): 
                #       원본 데이터에서 무작위로 복원 추출하여 각 결정 트리를 위한 학습 데이터를 생성합니다. 
                #       이로 인해 각 트리는 서로 다른 데이터 부분집합으로 학습됩니다.
                # 랜덤 특성 선택(Random Feature Selection): 
                #       각 노드에서 분할할 후보 특성을 무작위로 선택합니다. 이 과정에서 일부 특성은 전체 특성 집합에서 무작위로 선택됩니다.
                # 예측:
                #       분류 문제의 경우, 각 트리가 예측한 클래스 별 투표를 통해 최종 예측 클래스를 결정합니다.
                #       회귀 문제의 경우, 각 트리의 예측값의 평균을 최종 예측값으로 사용합니다.
            
            
#       [서포트 벡터 머신 (SVM)]
            #    개념 : 초평면, 마진, 서포트 벡터
            #    SVM 분류방식 : 선형SVM, 비선형SVM
            #    SVM 수학적 공식화 : 최적화 문제, 커널 트릭
            # 장점:
                # 높은 분류 성능: 특히 고차원 공간에서 효과적으로 동작합니다.
                # 과적합 방지: 마진을 최대화하기 때문에 일반화 성능이 좋습니다.            
            # 단점:
                # 계산 비용: 대규모 데이터셋에 대해 계산 비용이 많이 들 수 있습니다.
                # 커널 선택: 적절한 커널과 매개변수를 선택하는 것이 중요하며, 이는 데이터에 따라 달라질 수 있습니다. 
                
#       [k-최근접 이웃 (k-NN)]
            # 거리 측정: 알고리즘은 새로운 데이터 포인트와 기존 데이터 포인트들 간의 거리를 측정하여 가장 가까운 k개의 이웃을 찾습니다. 
            # k개의 이웃: 'k'는 사용자가 정해야 하는 하이퍼파라미터로, 
            # 새로운 데이터 포인트를 분류하거나 값을 예측하는 데 사용되는 이웃의 수를 나타냅니다.             
            # 장점:
            #     단순함: 알고리즘이 직관적이고 구현이 간단합니다.
            #     비선형 데이터 처리: 데이터의 분포나 형태에 대한 가정 없이 비선형 관계를 잘 처리할 수 있습니다.
            #     훈련 시간 없음: 훈련 단계가 없고 예측 시점에만 계산이 이루어집니다.
            # 단점:
            #     계산 비용: 예측 시 모든 데이터 포인트와의 거리를 계산해야 하므로, 데이터셋이 클 경우 계산 비용이 많이 듭니다.
            #     메모리 사용량: 모든 훈련 데이터를 저장해야 하므로 메모리 사용량이 큽니다.
            #     k값 선택의 어려움: 적절한 k값을 선택하는 것이 성능에 큰 영향을 미치며, 이를 위해 교차 검증 등의 기법이 필요합니다.
            #     스케일링 필요: 거리 기반 알고리즘이기 때문에, 각 특징(feature)의 스케일이 다를 경우 표준화 또는 정규화가 필요합니다.                
            # k-NN의 활용 예시
            #     이미지 인식: 이미지의 픽셀값을 특징으로 사용하여 비슷한 이미지 분류.
            #     추천 시스템: 유사한 취향의 사용자를 기반으로 영화, 음악 등을 추천.
            #     의료 진단: 환자의 증상 데이터를 사용하여 유사한 사례를 참고하여 질병 분류.    
                             
#       [나이브 베이즈]
            # 나이브 베이즈(Naive Bayes)는 [확률]에 기반한 분류 알고리즘으로, 
            # 주어진 입력 데이터의 특징(feature)들이 독립이라는 가정(즉, 나이브(naive)한 가정) 하에 베이즈 정리를 적용하여 분류를 수행합니다. 
            # 이 알고리즘은 주로 텍스트 분류와 같은 대규모 데이터셋에서 자주 사용됩니다.
            
            
#######################################################################################################           
# 5. 회귀 알고리즘
#       [선형 회귀]
            # 선형 회귀(Linear Regression)는 두 변수 간의 관계를 직선의 형태로 모델링하는 통계적 기법입니다. 
            # 주로 예측 및 회귀 분석에 사용되며, 
            # 주어진 입력 변수(x)와 출력 변수(y) 간의 선형 관계를 찾는 데 목적이 있습니다.
            # 과정: 
                # 데이터 수집 및 준비: 독립 변수와 종속 변수 간의 관계를 나타내는 데이터를 수집합니다.
                # 모델 수립: 수집한 데이터를 기반으로 선형 회귀 모델을 수립합니다.
                # 손실 함수 정의: 일반적으로 평균 제곱 오차(Mean Squared Error, MSE)를 사용하여 손실 함수를 정의합니다
                # 모델 학습: 손실 함수를 최소화하는 b0 와 b1를 찾기 위해 
                # 경사 하강법(Gradient Descent) 또는 최소 제곱법(Ordinary Least Squares, OLS) 등의 방법을 사용합니다.
                # 경사 하강법 : 경사 하강법은 반복적인 최적화 기법으로, 손실 함수의 기울기를 따라 이동하여 최소값을 찾습니다. 
                # 매 반복마다 가중치를 다음과 같이 업데이트합니다.
                
                # 선형 회귀의 장점과 단점
                #     장점: 
                #         해석 용이성: 모델이 직선 형태로 간단하고 해석이 쉽습니다.
                #         효율성: 계산이 빠르고 효율적입니다.
                #         통계적 성질: 좋은 통계적 성질을 가지고 있어 결과 해석에 유리합니다.
                #     단점: 
                #         선형 관계 가정: 독립 변수와 종속 변수 간의 관계가 선형이어야 합니다.
                #         이상치에 민감: 이상치(outlier)에 민감하게 반응합니다.
                #         다중 공선성 문제: 독립 변수들 간의 강한 상관관계가 있는 경우 다중 공선성(multicollinearity) 문제가 발생할 수 있습니다.
                
                
                
#       [다중 선형 회귀]
                # 다중 선형 회귀(Multiple Linear Regression)는 
                # 하나의 종속 변수와 두 개 이상의 독립 변수 간의 선형 관계를 모델링하는 회귀 분석 기법입니다. 
                # 이는 단순 선형 회귀의 확장으로, 여러 독립 변수를 사용하여 종속 변수를 예측할 수 있게 합니다.
                # 평균 제곱 오차(MSE), 평균 절대 오차(MAE) 등의 지표를 사용합니다.
                
                # 최소 제곱법:
                #     최소 제곱법은 오차의 제곱합을 최소화하는 회귀 계수를 찾는 방법입니다. 
                #     손실 함수는 다음과 같습니다.
                
                # 경사 하강법:
                #     경사 하강법은 손실 함수를 최소화하는 방향으로 반복적으로 회귀 계수를 업데이트합니다. 
                #     업데이트 식은 다음과 같습니다.
                
                # 장점:
                #     다수의 변수 고려: 여러 독립 변수를 고려하여 종속 변수의 변동을 설명할 수 있습니다.
                #     예측 성능 향상: 독립 변수의 수가 증가함에 따라 모델의 예측 성능이 향상될 수 있습니다.
                #     해석 가능성: 각 독립 변수가 종속 변수에 미치는 영향을 명확하게 해석할 수 있습니다.
                
                # 단점:
                #     과적합: 독립 변수의 수가 너무 많으면 모델이 과적합(overfitting)될 수 있습니다.
                #     다중 공선성: 독립 변수들 간의 상관관계가 높으면 다중 공선성(multicollinearity) 문제가 발생하여 
                #     회귀 계수의 신뢰성이 낮아질 수 있다.
                #     선형 관계 가정: 독립 변수와 종속 변수 간의 관계가 선형이어야 한다는 가정이 있습니다. 
                #     실제 데이터에서는 이 가정이 항상 성립하지 않을 수 있습니다. 
                
                # 다중 선형 회귀의 활용 예시
                #     부동산 가격 예측: 여러 요인(예: 면적, 방의 수, 위치 등)을 고려하여 부동산 가격을 예측합니다.
                #     마케팅 분석: 광고비, 프로모션 수단, 시장 요인 등을 고려하여 매출액을 예측합니다.
                #     의료 데이터 분석: 여러 환자 정보를 바탕으로 질병의 진행 정도를 예측합니다.
                
                
                
#       [로지스틱 회귀]
            # 로지스틱 회귀(Logistic Regression)는 
            # 통계적인 머신러닝 모델로 주로 이진 분류(binary classification) 문제에 사용됩니다.
            # 이 모델은 선형 회귀를 기반으로 하지만, 
            # 출력을 로지스틱 함수에 통과시켜 확률 값을 생성하고, 이를 기반으로 예측을 수행합니다.                        
            # 로지스틱 회귀는 독립 변수의 선형 결합을 이용하여 사건의 발생 가능성을 예측하는 통계 기법입니다.             
            
            # 이는 독립 변수의 선형 결합으로 
            # 종속 변수를 설명한다는 관점에서 선형 회귀 분석과 유사하지만, 
            # 로지스틱 회귀는 선형 회귀 분석과는 다르게 종속 변수가 범주형 데이터를 대상으로 합니다. 
            # 따라서, 입력 데이터가 주어졌을 때 해당 데이터의 결과가 특정 분류로 나뉘기 때문에 
            # 일종의 분류 (classification) 기법으로도 볼 수 있습니다.
            
            # 로지스틱 회귀의 목적은 
            # 일반적인 회귀 분석의 목표와 동일하게 종속 변수와 독립 변수간의 관계를 구체적인 함수로 나타내어 향후 예측 모델에 사용하는 것입니다. 
            # 이를 위해 로지스틱 회귀는 연속이고 증가함수이며 [0,1]에서 값을 갖는 연결 함수를 사용하여 종속(예측)변수와 연관시킵니다.
            
            
#######################################################################################################           
# 6. 군집화 알고리즘
#       [k-평균]
#       [계층적 군집화]
#       [DBSCAN]

#######################################################################################################
# 7. 연관 규칙 학습
#       [Apriori 알고리즘]
            # Apriori 알고리즘은 연관규칙 학습을 위한 대표적인 알고리즘입니다. 
            # 이 알고리즘은 데이터 내에서 아이템 간의 연관성을 찾아내는 데 사용되며, 
            # 특히 빈발 아이템 집합을 탐색 하여 연관규칙을 생성하는 데 유용합니다.
            # Apriori 알고리즘은 가장 빈번하게 구매하는 구매패턴 대로 추천해주는 알고리즘입니다. 
            # 이 알고리즘은 빈번한 아이템 셋은 하위 아이템셋 또한 빈번할 것이라고 가정하며, 
            # 반대로 빈번하지 않은 아이템 셋은 하위 아이템 셋도 빈번하지 않을 것이라고 가정하여 분석 대상에서 제외합니다.            
            
            # Apriori 알고리즘의 주요 단계는 다음과 같습니다:            
            # 1.최소 지지도 설정
            # 2.개별 품목 중에서 최소 지지도를 넘는 모든 품목 탐색
            # 3.위에서 찾은 개별 품목 만을 이용하여 최소 지지도를 넘는 2가지 품목 집합을 탐색
            # 4.위에서 찾은 품목 집합을 결합하여 최소 지지도를 넘는 3가지 품목 집합을 탐색
            # 5.위의 과정을 반복하여 최소 지지도를 넘는 빈발 품목 집합을 탐색
            
            # 이렇게 Apriori 알고리즘을 통해 얻어진 연관 규칙은 
            # 'A를 구매했을 때, B 또한 구매할 것이다’와 같은 패턴으로 해석될 수 있습니다. 
            # 이러한 정보는 마케팅 전략 수립, 상품 추천 등 다양한 분야에서 활용될 수 있습니다.            
            
            
#       [Eclat 알고리즘]
            # Eclat 알고리즘은 연관규칙 학습을 위한 대표적인 알고리즘 중 하나로, Apriori 알고리즘의 확장 가능한 버전입니다. 
            # 이 알고리즘은 데이터 내에서 아이템 간의 연관성을 찾아내는 데 사용되며, 
            # 빈발 아이템 집합 탐색, 연관규칙 생성 시 유용하다.
            
            # Eclat 알고리즘 특징:
            #     1. 지지도 계산에 많은 시간이 소요되는 것을 획기적 방식으로 개선.
            #     2. 후보빈발항목의 상성은 Apriori-Gen을 사용.
            #     3.지지도 계산의 효율성을 높이기 위해 Vertical Data Format 사용.
            
            # 이 알고리즘은 아이템 집합 간의 연관성을 찾아내는 데 사용되며, 
            # 이를 통해 'A를 구매했을 때, B도 구매할 것이다’와 같은 패턴을 발견할 수 있습니다. 
            # 이러한 정보는 
            #     마케팅 전략 수립, 상품 추천 등 다양한 분야에서 활용될 수 있습니다.
            
#       [연관 규칙 평가 지표 (지지도, 신뢰도, 향상도)]

#######################################################################################################
# 8. 차원 축소
#       [주성분 분석 (PCA)]
            # 주성분 분석 (Principal Component Analysis, PCA)은 
            # 고차원의 데이터를 저차원의 데이터로 환원시키는 기법입니다. 
            # 이 기법은 원 데이터의 분포를 최대한 보존하면서 고차원 공간의 데이터들을 저차원 공간으로 변환합니다. 
            # 이 때, 서로 연관 가능성이 있는 고차원 공간의 표본들을 선형 연관성이 없는 저차원 공간 (주성분)의 표본으로 변환하기 위해 
            # 직교 변환을 사용합니다.
            # PCA는 여러 개의 독립변수들을 잘 설명해줄 수 있는 주된 성분을 추출하는 기법입니다. 
            # 주성분 분석을 통해 전체 변수들의 핵심 특성만 선별하기 때문에, 
            # 독립변수 (차원)의 수를 줄일 수 있습니다. 
            # 이는 흔히 말하는 차원의 저주 (Curse of dimensionality)를 방지하기 위한 방법 입니다.

            # PCA의 원리는 다음과 같습니다:
            # 데이터의 분산을 가장 잘 나타낼 수 있는 축을 찾습니다. 이 축은 주성분이 됩니다.
            # 주성분으로 선정된 축과 직교하는 선이 다음 주성분이 됩니다. 이 선은 주성분과 대조되는 짧은 길이 (낮은 분산)로 생성됩니다.
            # 이 과정을 반복하여 원하는 차원의 수만큼 주성분을 얻습니다.
            # 이렇게 얻은 주성분들은 원래의 고차원 데이터를 저차원으로 표현하는 데 사용됩니다. 
            # 이 과정을 통해 데이터의 차원을 축소하면서도 원래 데이터의 분포를 최대한 유지할 수 있습니다. 
            # 이러한 특성 때문에 PCA는 데이터 시각화, 노이즈 필터링, 특징 추출 등 다양한 분야에서 활용됩니다.


#       [선형 판별 분석 (LDA)]
            # 선형 판별 분석 (Linear Discriminant Analysis, LDA)은 
            # 패턴 인식과 기계 학습에서 객체 간의 차이점을 찾는 데 사용되는 방법이다. 
            # 이는 독립변수들의 측정값에 따라 데이터가 어느 집단에 속할 것인가에 대해 판별하는 분석방법 입니다.
                        
            # LDA는 Classification(분류모델)과 Dimensional Reduction(차원 축소)까지 동시에 사용하는 알고리즘이며, 
            # 입력 데이터 세트를 저차원 공간으로 투영(projection)해 차원을 축소하는 기법 입니다. 
            # 이는 지도학습에서 사용 됩니다.
            # LDA의 목표는 
            # 표본의 두 집단을 가장 잘 분리시키는 선에 정사영 시키는 것입니다. 
            # 이를 위해 분리측도(measure of separation)을 정의하며, 이는 분리가 잘 되었는지를 평가하는 방법 입니다. 
            # 분리측도는 평균차이와 표본분산을 고려하여 정사영된 평균들 사이의 거리를 측정 합니다.
            # 이렇게 얻은 결과는 
            # 데이터의 차원을 축소하면서도 원래 데이터의 분포를 최대한 유지할 수 있습니다. 
            # 이러한 특성 때문에 LDA는 데이터 시각화, 노이즈 필터링, 특징 추출 등 다양한 분야에서 활용 됩니다.            
            
            
            
#       [독립 성분 분석 (ICA)]
            # 독립 성분 분석 (Independent Component Analysis, ICA)은 
            # 다변량의 신호를 통계적으로 독립적인 하부 성분으로 분리하는 계산 방법입니다. 
            # 각 성분은 비 가우스 성 신호로서 서로 통계적 독립을 이루는 성분으로 구성되어 있습니다. 
            # 이는 블라인드 신호를 분리하는 특별한 방법으로, 신호에서 특정 숨겨진 정보를 가져오는 방법입니다.
            # ICA의 개념은 중심극한정리를 정 반대로 생각한 것입니다. 
            # 중심극한정리에 따르면, 비 가우스 성 (Non-Gaussianity)은 성분의 독립성을 측정하는 하나의 방법이며, 
            # 상호 정보량도 신호 간의 독립성을 측정하는 척도가 됩니다.
            # ICA에서는 source들이 서로 독립적이라는 가정을 최대한 만족할 수 있도록하는 행렬을 찾는 것을 목적으로 합니다. 
            # 이를 위해, 아래와 같이 선형 변환 후의 랜덤 변수의 확률밀도함수를 구할 수 있습니다.
            
#######################################################################################################            
# 9. 모델 평가 및 선택
#       교차 검증
#       과적합과 과소적합
#       혼동 행렬
#       ROC 곡선 및 AUC
#       F1 점수, 정밀도, 재현율


#######################################################################################################
# 10. 고급 주제
#       시간 시계열 분석
#       텍스트 마이닝
#       웹 마이닝
#       추천 시스템

#######################################################################################################
# 11. 실제 데이터 마이닝 프로젝트
#       문제 정의
#       데이터 수집 및 전처리
#       모델 선택 및 학습
#       모델 평가 및 튜닝
#       결과 해석 및 보고

#######################################################################################################
# 12. 데이터 마이닝 도구 및 라이브러리
#       Python: 
#           Pandas, 
#           NumPy, 
#           Scikit-learn
#       R: 
#           caret, 
#           dplyr, 
#           ggplot2
# 데이터베이스 및 SQL
# 빅데이터 도구: Hadoop, Spark
 
 